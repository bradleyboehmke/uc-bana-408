# Lesson 2b: Multiple linear regression

```{r setup-2b, include=FALSE}
# clean up saved items that may carry over from previous lessons
rm(list = ls())

knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 3.5,
  fig.width = 6,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

library(kableExtra)
library(here)
img_path <- here("images")
```

In the last lesson we learned how to use one predictor variable to predict a numeric response. However, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (`Gr_Liv_Area`) and the year the house was built (`Year_Built`) are (linearly) related to sale price (`Sale_Price`). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the _multiple linear regression_ (MLR) model and is the focus for this lesson.

## Learning objectives

By the end of this lesson you will know how to:

* Fit, interpret, and assess the performance of a multiple linear regression model.
* Include categorical features in a linear regression model and interpret their results.
* Asses the most influential predictor variables in a linear regression model.

## Prerequisites

This lesson leverages the following packages:

```{r mlr-pkgs, message=FALSE}
# Data wrangling & visualization packages
library(tidyverse)

# Modeling packages
library(tidymodels)
```

We'll also continue working with the `ames` data set:

```{r mlr-ames-train}
# stratified sampling with the rsample package
ames <- AmesHousing::make_ames()

set.seed(123)
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

## Adding additional predictors

In the last lesson we saw how we could use the above ground square footage (`Gr_Liv_Area`) of a house to predict the sale price (`Sale_Price`). 

```{r}
model1 <- linear_reg() %>%
   fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)

tidy(model1)
```
From our model we interpreted the results as that the mean selling price increases by `r round(coef(model1$fit)[2L], digits = 2)` for each additional one square foot of above ground living space.  We also determined that the `Gr_Liv_Area` coefficient is statistically different from zero based on the `p.value`.

And, we saw that our model has a generalization RMSE value of 55942, which means that on average, our model's predicted sales price differs from the actual sale price by \$55,942.

```{r}
model1 %>%
   predict(ames_test) %>%
   bind_cols(ames_test) %>%
   rmse(truth = Sale_Price, estimate = .pred)
```

However, we are only using a single predictor variable to try predict the sale price.  In reality, we likely can use other home attributes to do a better job at predicting sale price. For example, we may wish to understand if above ground square footage (`Gr_Liv_Area`) and the year the house was built (`Year_Built`) are (linearly) related to sale price (`Sale_Price`). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the _multiple linear regression_ (MLR) model.

With two predictors, the MLR model becomes: 

\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2,
\end{equation}

where $x_1$ and $x_2$ are features of interest. In our Ames housing example, $x_1$ can represent `Gr_Liv_Area` and $x_2$ can represent `Year_Built`. 

In R, multiple linear regression models can be fit by separating all the features of interest with a `+`:

```{r lm-model2, linewidth=56}
model2 <- linear_reg() %>%
   fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)

tidy(model2)
```

The LS estimates of the regression coefficients are $\widehat{b}_1 =$ `r round(coef(model2$fit)[2L], digits = 3)` and $\widehat{b}_2 =$ `r round(coef(model2$fit)[3L], digits = 3)` (the estimated intercept is `r round(coef(model2$fit)[1L], digits = 3)`. In other words, every one square foot increase to above ground square footage is associated with an additional `r scales::dollar(coef(model2$fit)[2L])` in mean selling price ___when holding the year the house was built constant___. Likewise, for every year newer a home is there is approximately an increase of `r scales::dollar(coef(model2$fit)[3L])` in selling price ___when holding the above ground square footage constant___.

As our model results show above, the `p.value`s for our two coefficients suggest that both are stastically different than zero. This can also be confirmed by computing our confidence intervals around these coefficient estimates as we did before. 

```{r}
confint(model2$fit)
```


Now, instead of modeling sale price with the  the "best fitting" line that minimizes residuals, we are modeling sale price with the best fitting hyperplane that minimizes the residuals, which is illustrated below.

```{r lm-mls-plane, echo=FALSE, fig.cap="Average home sales price as a function of year built and total square footage."}
library(plotly)
library(reshape2)
# model
model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)
# Setup Axis
axis_x <- seq(min(ames_train$Gr_Liv_Area), max(ames_train$Gr_Liv_Area), by = 50)
axis_y <- seq(min(ames_train$Year_Built), max(ames_train$Year_Built), by = 10)
# Sample points
lm_surface <- expand.grid(Gr_Liv_Area = axis_x, Year_Built = axis_y, KEEP.OUT.ATTRS = F)
lm_surface$Sale_Price <- predict.lm(model2, newdata = lm_surface)
lm_surface <- acast(lm_surface, Year_Built ~ Gr_Liv_Area, value.var = "Sale_Price")
# plot
ames_plot <- plot_ly(ames_train,
                     x = ~ Gr_Liv_Area, 
                     y = ~ Year_Built, 
                     z = ~ Sale_Price,
                     type = "scatter3d", 
                     mode = "markers",
                     marker = list(
                       size = 5,
                       opacity = 0.25
                     ),
                     showlegend = F
                     )
# add surface
ames_plot <- add_trace(p = ames_plot,
                       z = lm_surface,
                       x = axis_x,
                       y = axis_y,
                       type = "surface")
ames_plot
```


### Knowledge check

```{block, type='todo'}
Using the `ames_train` data:

1. Fit a MLR model where `Sale_Price` is a function of `Gr_Liv_Area` and `Garage_Cars`.
2. Interpret the coefficients. Are they both statistically different from zero?
3. Compute and interpret the ***generalization*** RMSE for this model.
4. How does this model compare to the model based on just `Gr_Liv_Area`?
```

## Interactions

You may notice that the fitted plane in the above image is flat; there is no curvature. This is true for all linear models that include only _main effects_ (i.e., terms involving only a single predictor). One way to model curvature is to include _interaction effects_. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. 

Suppose that when people buy older homes they care more about the historical nature and beauty of the home rather than the total square footage.  However, as older historical homes grow larger in size we see a compounding impact to the value of the home. This is known as a synergy effect -- as one feature changes there is a larger or smaller effect of the other feature. 

In linear regression, interactions can be captured via products of features (i.e., $x_1 \times x_2$). A model with two main effects can also include a two-way interaction. For example, to include an interaction between $x_1 =$ `Gr_Liv_Area` and $x_2 =$ `Year_Built`, we introduce an additional product term:

\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1 x_2.
\end{equation}

Note that in R, we use the `:` operator to include an interaction (technically, we could use `*` as well, but `x1 * x2` is shorthand for `x1 + x2 + x1:x2` so is slightly redundant):

```{r lm-model2-w-interaction}
interaction_model <- linear_reg() %>%
   fit(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)

tidy(interaction_model)
```

In this example, we see that the two main effects (`Gr_Liv_Area` & `Year_Built`) are statistically significant and so is the interaction term (`Gr_Liv_Area:Year_Built`). So how do we interpret these results?  

Well, we can say that for every 1 additional square feet in `Gr_Liv_Area`, the `Sale_Price` of a home increases by $b_1 + b_3 \times \text{Year_Built}$ = -728.5084 + 0.4168489 x `Year_Built`. Likewise, for each additional year that a home was built, the `Sale_Price` of a home increases by $b_2 + b_3 \times \text{Gr_Liv_Area}$ = 430.8755 + 0.4168489 x `Gr_Liv_Area`.

Adding an interaction term now makes the change in one variable non-linear because it includes an additional change based on another feature. This non-linearity (or curvature) that interactions capture can be illustrated in the contour plot below. The left plot illustrates a regression model with main effects only. Note how the fitted regression surface is flat (i.e., it does not twist or bend). While the fitted regression surface with interaction is displayed in the right side plot and you can see the curvature of the relationship induced.

```{r lm-mlr-fit, echo=FALSE, fig.width=10, fig.height=4.5, fig.cap="In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane)."}
# Fitted models
fit1 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)
fit2 <- lm(Sale_Price ~ Gr_Liv_Area * Year_Built, data = ames_train)

# Regression plane data
plot_grid <- expand.grid(
  Gr_Liv_Area = seq(from = min(ames_train$Gr_Liv_Area), to = max(ames_train$Gr_Liv_Area), 
                    length = 100), 
  Year_Built = seq(from = min(ames_train$Year_Built), to = max(ames_train$Year_Built), 
                   length = 100)
)
plot_grid$y1 <- predict(fit1, newdata = plot_grid)
plot_grid$y2 <- predict(fit2, newdata = plot_grid)

# Level plots
p1 <- ggplot(plot_grid, aes(x = Gr_Liv_Area, y = Year_Built, 
                            z = y1, fill = y1)) +
  geom_tile() +
  geom_contour(color = "white") +
  viridis::scale_fill_viridis(name = "Predicted\nvalue", option = "inferno") +
  theme_bw() +
  ggtitle("Main effects only")
p2 <- ggplot(plot_grid, aes(x = Gr_Liv_Area, y = Year_Built, 
                            z = y2, fill = y1)) +
  geom_tile() +
  geom_contour(color = "white") +
  viridis::scale_fill_viridis(name = "Predicted\nvalue", option = "inferno") +
  theme_bw() +
  ggtitle("Main effects with two-way interaction")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

```{block, type='note'}
Interaction effects are quite prevalent in predictive modeling. Since linear models are an example of parametric modeling, it is up to the analyst to decide if and when to include interaction effects. This becomes quite tedious and unrealistic for larger data sets.

In later lessons, we'll discuss algorithms that can automatically detect and incorporate interaction effects (albeit in different ways). For now, just realize that adding interactions is possible with MLR models.
```

## Qualitative predictors

In our discussion so far, we have assumed that all variables in our linear regression model are *quantitative*. But in practice, this is not necessarily the case; often some predictors are *qualitative*.

For example, the Credit data set provided by the **ISLR** package records the balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating).

```{r}
credit <- as_tibble(ISLR::Credit)
credit
```


Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. For example, based on the gender, we can create a new variable that takes the form

$$
x_i = \Bigg\{ \genfrac{}{}{0pt}{}{1 \hspace{.5cm}\text{ if }i\text{th person is female}\hspace{.25cm}}{0 \hspace{.5cm}\text{ if }i\text{th person is male}}
$$

and use this variable as a predictor in the regression equation. This results in the model

$$
y_i = b_0 + b_1x_i = \Bigg\{ \genfrac{}{}{0pt}{}{b_0 + b_1 \hspace{.5cm}\text{ if }i\text{th person is female}\hspace{.3cm}}{b_0 \hspace{1.5cm}\text{ if }i\text{th person is male}}
$$
Now $b_0$ can be interpreted as the average credit card balance among males, $b_0 + b_1$ as the average credit card balance among females, and $b_1$ as the average difference in credit card balance between females and males.  We can produce this model in R using the same syntax as we saw earlier:

```{r}
qual_model <- linear_reg() %>%
   fit(Balance ~ Gender, data = credit)

tidy(qual_model)
```

The results above suggest that males are estimated to carry \$509.80 in credit card debt where females carry \$509.80 - $19.73 = \$490.07.

The decision to code males as 0 and females as 1 is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients.  If we want to change the reference variable (the variable coded as 0) we can change the factor levels. 

```{r}
credit$Gender <- factor(credit$Gender, levels = c("Female", " Male"))

qual_model <- linear_reg() %>%
   fit(Balance ~ Gender, data = credit)

tidy(qual_model)
```

A similar process ensues for qualitative predictor categories with more than two levels.  For instance, if we go back to our Ames housing data we'll see that there is a `Neighborhood` variable. In our data there are 28 different neighborhoods.

```{r}
ames_train %>%
   count(Neighborhood)
```

Most people are aware that different neighborhoods can generate significantly different home prices than other neighborhoods. In this data we can visualize this by looking at the distribution of `Sale_Price` across neighborhoods. We see that the Stone Brook neighborhood has the highest average sale price whereas Meadow Village has the lowest. This could be for many reasons (i.e. age of the neighborhood, amenities provided by the neighborhood, proximity to undesirable things such as manufacturing plants).

```{r, fig.height=8, fig.width=8}
ggplot(ames_train, aes(fct_reorder(Neighborhood, Sale_Price), Sale_Price)) +
   geom_boxplot() +
   xlab(NULL) +
   scale_y_continuous("Sale Price", labels = scales::dollar) +
   coord_flip()
```

So, naturally, we can assume there is some relationship between `Neighborhood` and `Sale_Price`. We can assess this relationship by running the following model.  

Based on the results we see that the reference Neighborhood (which is North Ames based on `levels(ames_train$Neighborhood)`) has an average `Sale_Price` of \$143,516.75 (based on the intercept).  Whereas College Creek has a `Sale_Price` of \$143,516.75 + \$57,006.75 = \$200,523.50. The `p.value` for College Creek is very small suggesting that this difference between North Ames and College Creek is statistically significant.

However, look at the results for the Sawyer neighborhood. The coefficient suggests that the average `Sale_Price` for the Sawyer neighborhood is \$143,516.75 - \$4,591.68 = \$148,108.40. However, the `p.value` is 0.43 which suggests that there is no statistical difference between the reference neighborhood (North Ames) and Sawyer.

```{r}
neighborhood_model <- linear_reg() %>%
   fit(Sale_Price ~ Neighborhood, data = ames_train)

tidy(neighborhood_model)
```

### Knowledge check

```{block, type='todo'}
The Ames housing data has an `Overall_Qual` variable that measures the overall quality of a home (Very Poor, Poor, ..., Excellent, Very Excellent).

1. Plot the relationship between `Sale_Price` and the `Overall_Qual` variable. Does there look to be a relationship between the quality of a home and its sale price?
2. Model this relationship with a simple linear regression model.
3. Interpret the coefficients.
```


## Including many predictors

In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with _p_ distinct predictors is

\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_p x_p,
\end{equation}

where $x_i$ for $i = 1, 2, \dots, p$ are the predictors of interest. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. 

The code below creates a model where we use all features in our data set as main effects (i.e., no interaction terms) to predict `Sale_Price`. 

```{block, type='note'}
However, note that we remove a few variables first. This is because these variables introduce some new problems that require us to do some feature engineering steps. We'll discuss this in a future lesson but for now we'll just put these feature variables to the side.
```

```{r mlr-model3}
# remove some trouble variables
trbl_vars <- c("MS_SubClass", "Condition_2", "Exterior_1st", 
               "Exterior_2nd", "Misc_Feature")
ames_train <- ames_train %>%
   select(-trbl_vars)

# include all possible main effects
model3 <- linear_reg() %>%
   fit(Sale_Price ~ ., data = ames_train) 

# print estimated coefficients in a tidy data frame
tidy(model3)  
```

You'll notice that our model's results includes the intercept plus 248 predictor variable coefficients. However, our `ames_train` data only includes 75 predictor variables after removing those 5 troublesome variables! What gives?

```{r}
ames_train %>%
   select(-Sale_Price) %>%
   dim()
```

The reason is that 41 of our predictor variables are qualitative and many of these include several levels. So the dummy encoding procedure discussed in the last section causes us to have many more coefficients than initial predictor variables.

```{r}
ames_train %>%
   select_if(is.factor) %>%
   colnames()
```

If we wanted to assess which features have a relationship we could easily filter our model results to find which coefficients have `p.values` less than 0.05. In this model we see that 67 (68 minus the intercept) features have a statistical relationship with `Sale_Price`.

```{r}
tidy(model3) %>%
   filter(p.value < 0.05)
```

How does our model with all available predictors perform? We can compute the generalization error to assess.

```{r, error=TRUE}
model3 %>%
   predict(ames_test) %>%
   bind_cols(ames_test) %>%
   rmse(truth = Sale_Price, estimate = .pred)
```

Not too shabby! Using all the predictor variables in our model has drastically reduced our test RMSE!

## Feature importance

Ok, so we found a linear regression model that performs pretty good compared to the other linear regression models we trained. Our next goal is often to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the lesson, this constant rate of change is provided by the coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables?

Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the t-statistic for each model parameter used. Rather than search through each of the variables to compare their t-statistic values, we can use `vip::vip()` to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). The plot below illustrates the top 20 most influential variables. We see that the top 4 most important variables have to do with roofing material followed by the total square footage on the second floor.  

```{r lm-vip, fig.height=4.5}
# plot top 10 influential features
model3 %>%
   vip::vip(num_features = 20)
```

This is basically saying that our model finds that these are the most influential features in our data set that have the largest impact on the predicted outcome. If we order our data based on the t-statistic we see similar results. 

```{r lm-best-model-t-stat}
tidy(model3) %>%
   arrange(desc(statistic))
```

## Exercises

```{block, type='todo'}
Using the Boston housing data set where the response feature is the median value of homes within a census tract (`cmedv`):

1. Split the data into 70-30 training-test sets.
2. Train an MLR model that includes all the predictor variables.
3. Assess and interpret the coefficients. Are all predictor variables statistically significant? Explain why or why not.
4. What is the generalization error of this model?
5. Which features are most influential in this model and which features are not?
```
