# Lesson 3a: Resampling

```{r setup-3a, include=FALSE}
# clean up saved items that may carry over from previous lessons
rm(list = ls())

knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 3.5,
  fig.width = 6,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

library(kableExtra)
library(here)
img_path <- here("images")
```

The last several lessons gave you a good introduction to building predictive models using the tidymodels construct. And as we trained our models we evaluated their performance on the test set, which we called the _generalization error_. However, the approach we've taken thus far to evaluate the generalization error can have some pitfalls. This lesson is going to go deeper into the idea of model evaluation and we’ll discuss how to incorporate ___resampling___ procedures to give you a more robust assessment of model performance. 

## Learning objectives

By the end of this lesson you will be able to:

1. Explain the reasoning for resampling procedures and when/why we should incorporate them into our ML workflow.
2. Implement _k_-fold cross-validation procedures for more robust model evaluation.

## Prerequisites

This lesson leverages the following packages and data. Recall in the last lesson we removed 5 variables since they cause some problems.  In the next lesson we'll learn how to take care of these issues but for now we'll just remove them as we did before.

```{r}
library(tidymodels)

ames <- AmesHousing::make_ames()

# remove some trouble variables
trbl_vars <- c("MS_SubClass", "Condition_2", "Exterior_1st", 
               "Exterior_2nd", "Misc_Feature")
ames <- ames %>%
   select(-trbl_vars)
```

Let's go ahead and create our train-test split:

```{r 3a-train-test-split}
# create train/test split
set.seed(123)  # for reproducibility
split  <- initial_split(ames, prop = 0.7)
ames_train  <- training(split)
ames_test   <- testing(split)
```

## Resampling & cross-validation

In the previous lessons we split our data into training and testing sets and we assessed the performance of our model on the test set. Recall that the last model we trained and evaluated had a 

```{r}
mlr <- linear_reg() %>%
   fit(Sale_Price ~ ., data = ames_train) 

# generalization error based on test set
mlr %>%
   predict(ames_test) %>%
   bind_cols(ames_test) %>%
   rmse(truth = Sale_Price, estimate = .pred)
```




Unfortunately, there are a few pitfalls to this approach:

1. If our dataset is small, a single test set may not provide realistic expectations of our model’s performance on unseen data.
2. A single test set does not provide us any insight on variability of our model’s performance.
3. Using our test set to drive our model building process can bias our results via data leakage. Basically, the more we use the test data to assess various model performances, the less likely the test data is behaving like true, unseen data.

```{block, type='warning'}
It is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process.

```

___Resampling methods___ provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts of the training data.

```{r 2b-resampling, echo=FALSE, out.width='90%', out.height='90%', fig.cap="Illustration of resampling. [@tidymodels]"}
knitr::include_graphics(here(img_path, "resampling.svg"))
```

```{block, type='note'}
This allows us to train and validate our model entirely on the training data and not touch the test data until we have selected a final “optimal” model.
```

The two most commonly used resampling methods include **k-fold cross-validation** and **bootstrap sampling**.

## K-fold cross-validation

Cross-validation consists of repeating the procedure such that the training and testing sets are different each time. Generalization performance metrics are collected for each repetition and then aggregated. As a result we can get an estimate of the variability of the model’s generalization performance.

k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into *k* groups (aka folds) of approximately equal size.

```{r 2b-cv-diagram, echo=FALSE, fig.cap="Illustration of k-fold sampling across a data sets index.", out.width='90%', out.height='90%'}
knitr::include_graphics(here(img_path, "cross_validation_diagram.png"))
```

The model is fit on $k-1$ folds and then the remaining fold is used to compute model performance.  This procedure is repeated _k_ times; each time, a different fold is treated as the validation set. Consequently, with _k_-fold CV, every observation in the training data will be held out one time to be included in the assessment/validation set. This process results in _k_ estimates of the generalization error (say $\epsilon_1, \epsilon_2, \dots, \epsilon_k$). Thus, the _k_-fold CV estimate is computed by averaging the _k_ test errors, providing us with an approximation of the error we might expect on unseen data.

```{r 2b-modeling-process-cv-diagram, echo=FALSE, out.width='90%', out.height='90%', fig.cap="Illustration of a 5-fold cross validation procedure."}
knitr::include_graphics(here(img_path, "cv.png"))
```

```{block, type='tip'}
In practice, one typically uses k=5 or k=10. There is no formal rule as to the size of k; however, as k gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease.
```

To implement k-fold CV we first make a resampling object. In this example we create a 10-fold resampling object.

```{r}
kfolds <- vfold_cv(train, v = 10)
```

We can now create our random forest model object and create a workflow object as we did in the previous lesson. To fit our model across our 10-folds we just use `fit_resamples()`.

```{r 2b-rf-cv, eval=FALSE}
# create our random forest model object
rf_mod <- rand_forest() %>%
   set_mode('regression')

# add model object and our formula spec to a workflow object
rf_wflow <- workflow() %>% 
   add_model(rf_mod) %>%
   add_formula(Sale_Price ~ .)

# fit our model across the 10-fold CV
rf_fit_cv <- rf_wflow %>%
   fit_resamples(kfolds)
```

We can then get our average 10-fold cross validation error with `collect_metrics()`:

```{r 2b-overall-cv-results, eval=FALSE}
collect_metrics(rf_fit_cv)
```

If we want to see the model evaluation metric (i.e. RMSE) for each fold we just need to unnest the `rf_fit_cv` object. 

```{block, type='tip'}
We have not discussed nested data frames but you can read about them [here](https://tidyr.tidyverse.org/articles/nest.html) 
```

```{r 2b-cv-fold-results, eval=FALSE}
rf_fit_cv %>% 
   unnest(.metrics) %>%
   filter(.metric == 'rmse')
```

## Bootstrapping


## Alternative methods


## Exercises
