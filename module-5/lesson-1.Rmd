# Lesson 5a: Hyperparameter Tuning


```{r setup-5a, include=FALSE}
# clean up saved items that may carry over from previous lessons
rm(list = ls())

knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 3.5,
  fig.width = 6,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

library(kableExtra)
library(here)
img_path <- here("images")

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
```

We learned in the last lesson that hyperparameters (aka tuning parameters) are parameters that we can use to control the complexity of machine learning algorithms. The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we often go through iterations of testing out different values to determine which hyperparameter settings provide the optimal result. As we add more hyperparameters, this becomes quite tedious to do manually so in this lesson we'll learn how we can automate the tuning process to find the optimal (or near-optimal) settings for hyperparameters.

## Learning objectives

By the end of this module you will:

- Be able to explain the two components that make up prediction errors.
- Understand why hyperparameter tuning is an essential part of the machine learning process.
- Apply efficient and effective hyperparameter tuning with Tidymodels.

## Prerequisites

```{r, message=FALSE}
# Helper packages
library(tidyverse) # for data wrangling & plotting

# Modeling packages
library(tidymodels)

# Model interpretability packages
library(vip)      # variable importance
```

```{r}
# Stratified sampling with the rsample package
ames <- AmesHousing::make_ames()
set.seed(123)  # for reproducibility
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

## Bias-variance tradeoff

```{block, type='video'}
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&playerId=kaltura_player&entry_id=1_qye5wf7l&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_3rohkebe" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Bias-Variance tradeoff"></iframe>
```


Prediction errors can be decomposed into two important subcomponents: error due to "bias" and error due to "variance". There is often a tradeoff between a model's ability to minimize bias and variance. Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models.

### Bias

_Bias_ is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model's predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data. Figure \@ref(fig:modeling-process-bias-model) illustrates an example where the polynomial model does not capture the underlying structure well.  Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships. 

```{block, type='note'}
We can think of models with high bias as ***underfitting*** to the true patterns and relationships in our data. This is often because high bias models either oversimplify these relationships or are constrained in a way that cannot adequately form to the true, complex relationship that exists. These models tend to lead to high error on training and test data.
```

We also need to think of bias-variance in relation to resampling.  Models with high bias are rarely affected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated below:

```{r modeling-process-bias-model, echo=FALSE, fig.height=4, fig.width=10, fig.cap="A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left).  Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).", warning=FALSE, message=FALSE}
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)
# Single model fit
bias_model <- lm(y ~ I(x^3), data = df)
df$predictions <- predict(bias_model, df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = "dodgerblue") +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("Single biased model fit")
# Bootstrapped model fit
bootstrap_n <- 25
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  set.seed(i)  # for reproducibility
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  fit <- lm(y ~ I(x^3), data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}
p2 <- ggplot(bootstrap_results, aes(x, predictions, color = model)) +
  geom_line(show.legend = FALSE, size = .5) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("25 biased models fit to bootstrap samples")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

### Variance

On the other hand, error due to _variance_ is defined as the variability of a model prediction for a given data point. Many models (e.g., _k_-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to.  However, these models offer their own problems as they run the risk of overfitting to the training data.  Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.

```{block, type='note'}
Models with high variance can be very adaptable and will conform very well to the patterns and relationships in the training data. In fact, these models will try to overfit to patterns and relationships in the training data so much that they are overly personalized to the training data and will not generalize well to data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data.
```


```{r modeling-process-variance-model, echo=FALSE, fig.height=4, fig.width=10, fig.cap="A high variance _k_-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left).  Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right).", warning=FALSE, message=FALSE}
library(caret)

# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)
# Single model fit
variance_model <- knnreg(y ~ x, k = 3, data = df)
df$predictions <- predict(variance_model, df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = "dodgerblue") +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("Single high variance model fit")
# Bootstrapped model fit
bootstrap_n <- 25
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  set.seed(i)  # for reproducibility
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  fit <- knnreg(y ~ x, k = 3, data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}
p2 <- ggplot(bootstrap_results, aes(x, predictions, color = model)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  ggtitle("25 high variance models fit to bootstrap samples")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk.  Moreover, many algorithms that are capable of achieving high generalization performance have lots of _hyperparameters_ that control the level of model complexity (i.e., the tradeoff between bias and variance).

### Balancing the tradeoff

We can think of bias and variance as two model attributes competing with one another. If our model is too simple and cannot conform to the relationships in our data then it is underfitting and will not generalize well. If our model is too flexible and overly conforms to the training data then it will also not generalize well. So our objective is to find a model with good balance that does not overfit nor underfit to the training data. This is a model that will generalize well.

```{r balancing-bias-variance1, echo=FALSE, out.width="80%", out.height="80%", fig.cap="Our objective is to find a model with good balance that does not overfit nor underfit to the training data. This is a model that will generalize well."}
knitr::include_graphics(here::here(img_path, "bias-variance-tradeoff1.png"))
```

At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more hyperparameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls.

```{r balancing-bias-variance2, echo=FALSE, out.width="50%", out.height="50%", fig.cap="As more and more hyperparameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. The sweet spot for any model is the level of complexity that minimizes bias while keeping variance constrained."}
knitr::include_graphics(here::here(img_path, "bias-variance-tradeoff.png"))
```

Understanding bias and variance is critical for understanding the behavior of prediction models, but in general what you really care about is overall error, not the specific decomposition. The sweet spot for any model is the level of complexity that minimizes bias while keeping variance constrained.  To find this we need an effective and efficient hyperparameter tuning process.

## Hyperparameter tuning

```{block, type='video'}
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&playerId=kaltura_player&entry_id=1_tvnmy5by&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_3s80j7ua" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Hyperparameters"></iframe>
```

Hyperparameters are the "knobs to twiddle" to control the complexity of machine learning algorithms and, therefore, the bias-variance trade-off.  Not all algorithms have hyperparameters; however, as the complexity of our models increase (therefore the ability to capture and conform to more complex relationships in our data) we tend to see an increase in the number of hyperparameters. 

The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we need a method of identifying the optimal setting.  For example, in the high variance example in the previous section, we illustrated a high variance _k_-nearest neighbor model.  _k_-nearest neighbor models have a single hyperparameter (_k_) that determines the predicted value to be made based on the _k_ nearest observations in the training data to the one being predicted.  If _k_ is small (e.g., $k=3$), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted.  This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data.  As _k_ gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging often helps to reduce variance!). The figure below illustrates this point.  Smaller _k_ values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal _k_ value might exist somewhere between 20--50, but how do we know which value of _k_ to use?

```{r modeling-process-knn-options, fig.width=10, fig.height=6, echo=FALSE, fig.cap="_k_-nearest neighbor model with differing values for _k_.", warning=FALSE, message=FALSE}
k_results <- NULL
k <- c(2, 5, 10, 20, 50, 150)

# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

# Fit many different models
for(i in seq_along(k)) {
  df_sim <- df
  fit <- knnreg(y ~ x, k = k[i], data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("k = ", str_pad(k[i], 3, pad = " "))
  k_results <- rbind(k_results, df_sim)
}
ggplot() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  geom_line(data = k_results, aes(x, predictions), color = "dodgerblue", size = 1.5) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  facet_wrap(~ model)
```

One way to perform hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy (as measured using _k_-fold CV, for instance). However, this can be very tedious work depending on the number of hyperparameters. An alternative approach is to perform a ***grid search***. A grid search is an automated approach to searching across many combinations of hyperparameter values.  

For the simple example above, a grid search would predefine a candidate set of values for _k_ (e.g., $k = 1, 2, \dots, j$) and perform a resampling method (e.g., _k_-fold CV) to estimate which _k_ value generalizes the best to unseen data.  The plots in the below examples illustrate the results from a grid search to assess $k = 3, 5, \dots, 150$ using repeated 10-fold CV. The error rate displayed represents the average error for each value of _k_ across all the repeated CV folds. On average, $k=46$ was the optimal hyperparameter value to minimize error (in this case, RMSE which will be discussed shortly) on unseen data.  

```{r modeling-process-knn-tune, fig.height=3, fig.width=7, echo=FALSE, fig.cap="Results from a grid search for a _k_-nearest neighbor model assessing values for _k_ ranging from 3-25.  We see high error values due to high model variance when _k_ is small and we also see high errors values due to high model bias when _k_ is large.  The optimal model is found at _k_ = 46."}
cv <- trainControl(method = "repeatedcv", number = 5, repeats = 10, returnResamp = "all")
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))
knn_fit <- train(x ~ y, data = df, method = "knn", trControl = cv, tuneGrid = hyper_grid)
ggplot() +
  geom_line(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = filter(knn_fit$results, k == as.numeric(knn_fit$bestTune)),
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 2) +
  scale_y_continuous("Error (RMSE)")
```

Throughout this course you'll be exposed to different approaches to performing grid searches. In the above example, we used a _full cartesian grid search_, which assesses every hyperparameter value manually defined.  However, as models get more complex and offer more hyperparameters, this approach can become computationally burdensome and requires you to define the optimal hyperparameter grid settings to explore.  Additional approaches we'll illustrate include _random grid searches_ [@bergstra2012random] which explores randomly selected hyperparameter values from a range of possible values, _early stopping_ which allows you to stop a grid search once reduction in the error stops marginally improving, _adaptive resampling_ via futility analysis [@kuhn2014futility] which adaptively resamples candidate hyperparameter values based on approximately optimal performance, and more.

## Implementation

```{block, type='video'}
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&playerId=kaltura_player&entry_id=1_oy6e3m47&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_neqdrim7" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Implementing hyperparameter tuning"></iframe>
```

Recall our regularized regression model from the last lesson. With that model there are two two main hyperparameters that we need to tune:

* `mixture`: the type of regularization (ridge, lasso, elastic net) we want to apply and,
* `penalty`: the strength of the regularization parameter ($\lambda$).

Initially, we set `mixture = 0` (Ridge model) and the strength of our regularization to `penalty = 1000`.

```{r}
# Step 1: create regularized model object
reg_mod <- linear_reg(mixture = 0, penalty = 1000) %>%
   set_engine("glmnet")

# Step 2: create model & preprocessing recipe
model_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
   step_normalize(all_numeric_predictors()) %>%
   step_dummy(all_nominal_predictors())

# Step 3. create resampling object
set.seed(123)
kfolds <- vfold_cv(ames_train, v = 5, strata = Sale_Price)

# Step 4: fit model workflow
reg_fit <- workflow() %>%
   add_recipe(model_recipe) %>%
   add_model(reg_mod) %>%
   fit_resamples(kfolds)

# Step 5: assess results
reg_fit %>%
   collect_metrics() %>%
   filter(.metric == 'rmse')
```

We then manually iterated through different values of `mixture` and `penalty` to try find the optimal setting. Which is less than efficient. 

### Tuning

Rather than specify set values for `mixture` and `penalty`, let's instead build our model in a way that uses placeholders for values.  We can do this using the `tune()` function:

```{r}
reg_mod <- linear_reg(mixture = tune(), penalty = tune()) %>%
   set_engine("glmnet")
```

We can create a regular grid of values to try using some convenience functions for each hyperparameter:

```{r}
reg_grid <- grid_regular(mixture(), penalty(), levels = 5)
```

The function `grid_regular()` is from the [dials](https://dials.tidymodels.org/) package. It chooses sensible values to try for each hyperparameter; here, we asked for 5 values each. Since we have two to tune, `grid_regular()` returns $5 \times 5 = 25$ different possible tuning combinations to try in a data frame.

```{r}
reg_grid
```

Now that we have our model with hyperparameter value placeholders and a grid of hyperparameter values to assess we can create a workflow object as we've done in the past and use `tune_grid()` to train our 25 models using our k-fold cross validation resamples.

```{r}
# tune
tuning_results <- workflow() %>%
   add_recipe(model_recipe) %>%
   add_model(reg_mod) %>%
   tune_grid(resamples = kfolds, grid = reg_grid)

# assess results
tuning_results %>%
   collect_metrics() %>%
   filter(.metric == "rmse")
```


We can assess our best models with `show_best()`, which by default will show the top 5 performing models based on the desired metric. In this case we see that our top 5 models use the Ridge regularization (`mixture = 0`) and across all the penalties we get the same cross-validation RMSE (31,373).

```{r}
tuning_results %>%
   show_best(metric = "rmse")
```

We can also use the `select_best()` function to pull out the single set of hyperparameter values for our best regularization model:

```{r}
tuning_results %>%
   select_best("rmse")
```

### More tuning

Based on the above results, we may wish to do another iteration and adjust the hyperparameter values to assess. Since all our best models were Ridge models we may want to set our model to use a Ridge penalty but then just tune the strength of the penalty. 

Here, we create another hyperparameter grid but we specify the range we want to search through. Note that `penalty` automatically applies a log transformation so by saying `range = c(0, 5)` I am actually saying to search between 1 - 100,000.

```{r}
reg_mod <- linear_reg(mixture = 0, penalty = tune()) %>%
   set_engine("glmnet")

reg_grid <- grid_regular(penalty(range = c(0, 5)), levels = 10)
```

Now we can search again and we see that by using slightly higher `penalty` values we improve our performance.

```{r}
tuning_results <- workflow() %>%
   add_recipe(model_recipe) %>%
   add_model(reg_mod) %>%
   tune_grid(resamples = kfolds, grid = reg_grid)

tuning_results %>%
   show_best(metric = "rmse")
```


### Finalizing our model

We can update (or “finalize”) our workflow object with the values from `select_best()`. This now creates a final model workflow with the optimal hyperparameter values.

```{r}
best_hyperparameters <- select_best(tuning_results, metric = "rmse")

final_wf <- workflow() %>%
   add_recipe(model_recipe) %>%
   add_model(reg_mod) %>% 
   finalize_workflow(best_hyperparameters)

final_wf
```

We can then use this final workflow to do further assessments. For example, if we want to train this workflow on the entire data set and look at which predictors are most influential we can:

```{r}
final_wf %>%
   fit(data = ames_train) %>% 
   extract_fit_parsnip() %>% 
   vip()
```


## Exercises

```{block, type='todo'}
Using the same `kernlab::spam` data we saw in the [section 12.10](https://bradleyboehmke.github.io/uc-bana-4080/lesson-4b-regularized-regression.html#classification-problems-1)...

1. Split the data into 70-30 training-test sets.
2. Apply a regularized classification model (`type` is our response variable) but use the `tune()` and `grid_regular()` approach we saw in this lesson to automatically tune the `mixture` and `penalty` hyperparameters. Use a 5-fold cross-validation procedure.
3. Which hyperparameter values maximize the AUC (`roc_auc`) metric?
4. Retrain a final model with these optimal hyperparameters and identify the top 10 most influential predictors.
```
