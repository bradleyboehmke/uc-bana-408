<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Lesson 2b: Model evaluation &amp; selection | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Lesson 2b: Model evaluation &amp; selection | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Lesson 2b: Model evaluation &amp; selection | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lesson-2a-feature-engineering.html"/>
<link rel="next" href="computing-environment.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.9/grViz.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing</a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes</a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers</a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing</a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition</a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Feature engineering</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>5.3</b> Create a recipe</a></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#feature-filtering"><i class="fa fa-check"></i><b>5.4</b> Feature filtering</a></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>5.5</b> Numeric features</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#skewness"><i class="fa fa-check"></i><b>5.5.1</b> Skewness</a></li>
<li class="chapter" data-level="5.5.2" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#standardization"><i class="fa fa-check"></i><b>5.5.2</b> Standardization</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>5.6</b> Categorical features</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>5.6.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>5.6.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>5.6.3</b> Lumping</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>5.7</b> Fit a model with a recipe</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-feature-engineering.html"><a href="lesson-2a-feature-engineering.html#exercises-2"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Model evaluation &amp; selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#resampling-cross-validation"><i class="fa fa-check"></i><b>6.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.4</b> K-fold cross-validation</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.5</b> Hyperparameter tuning</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#bias"><i class="fa fa-check"></i><b>6.5.1</b> Bias</a></li>
<li class="chapter" data-level="6.5.2" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#variance"><i class="fa fa-check"></i><b>6.5.2</b> Variance</a></li>
<li class="chapter" data-level="6.5.3" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#hyperparameters"><i class="fa fa-check"></i><b>6.5.3</b> Hyperparameters</a></li>
<li class="chapter" data-level="6.5.4" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#full-cartesian-grid-search"><i class="fa fa-check"></i><b>6.5.4</b> Full cartesian grid search</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#finalizing-our-model"><i class="fa fa-check"></i><b>6.6</b> Finalizing our model</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#exercises-3"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-model-evaluation-selection.html"><a href="lesson-2b-model-evaluation-selection.html#additional-resources"><i class="fa fa-check"></i><b>6.8</b> Additional resources</a></li>
</ul></li>
<li class="part"><span><b>III Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-2b-model-evaluation-selection" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Lesson 2b: Model evaluation &amp; selection<a href="lesson-2b-model-evaluation-selection.html#lesson-2b-model-evaluation-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The last couple of lessons gave you a good introduction to building predictive models using the tidymodels construct. This lesson is going to go deeper into the idea of model evaluation &amp; selection. We’ll discuss how to incorporate cross-validation procedures to give you a more robust assessment of model performance. We’ll also discuss the concept of hyperparameter tuning, the bias-variance tradeoff, and how to implement a tuning strategy to find a model the maximizes generalizability.</p>
<div id="learning-objectives-6" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Learning objectives<a href="lesson-2b-model-evaluation-selection.html#learning-objectives-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this lesson you will be able to:</p>
<ol style="list-style-type: decimal">
<li>Perform cross-validation procedures for more robust model performance assessment.</li>
<li>Execute hyperparameter tuning to find optimal model parameter settings.</li>
</ol>
</div>
<div id="prerequisites-2" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Prerequisites<a href="lesson-2b-model-evaluation-selection.html#prerequisites-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this lesson we’ll use several packages provided via <strong>tidymodels</strong> and we’ll use the <code>ames</code> housing data. However, for this module we’ll use the Ames housing data provided by the <strong>AmesHousing</strong> package.</p>
<div class="note">
<p>
Take a minute to check out the Ames data from
<code>AmesHousing::make_ames()</code>. It is very similar to the Ames
data provided in the CSV. Note that the various quality/condition
variables (i.e. <code>Overall_Qual</code>) are already encoded as
ordinal factors.
</p>
</div>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="lesson-2b-model-evaluation-selection.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb45-2"><a href="lesson-2b-model-evaluation-selection.html#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="lesson-2b-model-evaluation-selection.html#cb45-3" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span></code></pre></div>
<p>Let’s go ahead and create our train-test split:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="lesson-2b-model-evaluation-selection.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create train/test split</span></span>
<span id="cb46-2"><a href="lesson-2b-model-evaluation-selection.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb46-3"><a href="lesson-2b-model-evaluation-selection.html#cb46-3" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb46-4"><a href="lesson-2b-model-evaluation-selection.html#cb46-4" aria-hidden="true" tabindex="-1"></a>train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb46-5"><a href="lesson-2b-model-evaluation-selection.html#cb46-5" aria-hidden="true" tabindex="-1"></a>test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="resampling-cross-validation" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Resampling &amp; cross-validation<a href="lesson-2b-model-evaluation-selection.html#resampling-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous lessons we split our data into training and testing sets and we assessed the performance of our model on the test set. Unfortunately, there are a few pitfalls to this approach:</p>
<ol style="list-style-type: decimal">
<li>If our dataset is small, a single test set may not provide realistic expectations of our model’s performance on unseen data.</li>
<li>A single test set does not provide us any insight on variability of our model’s performance.</li>
<li>Using our test set to drive our model building process can bias our results via data leakage.</li>
</ol>
<p>Resampling methods provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts of the training data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2b-resampling"></span>
<img src="images/resampling.svg" alt="Illustration of resampling." width="90%" height="90%" />
<p class="caption">
Figure 6.1: Illustration of resampling.
</p>
</div>
<div class="note">
<p>
This allows us to train and validate our model entirely on the
training data and not touch the test data until we have selected a final
“optimal” model.
</p>
</div>
<p>The two most commonly used resampling methods include <strong>k-fold cross-validation</strong> and <strong>bootstrap sampling</strong>. This lesson focuses on using k-fold cross-validation.</p>
</div>
<div id="k-fold-cross-validation" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> K-fold cross-validation<a href="lesson-2b-model-evaluation-selection.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cross-validation consists of repeating the procedure such that the training and testing sets are different each time. Generalization performance metrics are collected for each repetition and then aggregated. As a result we can get an estimate of the variability of the model’s generalization performance.</p>
<p>k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into <em>k</em> groups (aka folds) of approximately equal size.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2b-cv-diagram"></span>
<img src="images/cross_validation_diagram.png" alt="Illustration of k-fold sampling across a data sets index." width="90%" height="90%" />
<p class="caption">
Figure 6.2: Illustration of k-fold sampling across a data sets index.
</p>
</div>
<p>The model is fit on <span class="math inline">\(k-1\)</span> folds and then the remaining fold is used to compute model performance. This procedure is repeated <em>k</em> times; each time, a different fold is treated as the validation set. Consequently, with <em>k</em>-fold CV, every observation in the training data will be held out one time to be included in the assessment/validation set. This process results in <em>k</em> estimates of the generalization error (say <span class="math inline">\(\epsilon_1, \epsilon_2, \dots, \epsilon_k\)</span>). Thus, the <em>k</em>-fold CV estimate is computed by averaging the <em>k</em> test errors, providing us with an approximation of the error we might expect on unseen data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2b-modeling-process-cv-diagram"></span>
<img src="images/cv.png" alt="Illustration of a 5-fold cross validation procedure." width="90%" height="90%" />
<p class="caption">
Figure 6.3: Illustration of a 5-fold cross validation procedure.
</p>
</div>
<div class="tip">
<p>
In practice, one typically uses k=5 or k=10. There is no formal rule
as to the size of k; however, as k gets larger, the difference between
the estimated performance and the true performance to be seen on the
test set will decrease.
</p>
</div>
<p>To implement k-fold CV we first make a resampling object. In this example we create a 10-fold resampling object.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="lesson-2b-model-evaluation-selection.html#cb47-1" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(train, <span class="at">v =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>We can now create our random forest model object and create a workflow object as we did in the previous lesson. To fit our model across our 10-folds we just use <code>fit_resamples()</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="lesson-2b-model-evaluation-selection.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create our random forest model object</span></span>
<span id="cb48-2"><a href="lesson-2b-model-evaluation-selection.html#cb48-2" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>() <span class="sc">%&gt;%</span></span>
<span id="cb48-3"><a href="lesson-2b-model-evaluation-selection.html#cb48-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_mode</span>(<span class="st">&#39;regression&#39;</span>)</span>
<span id="cb48-4"><a href="lesson-2b-model-evaluation-selection.html#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="lesson-2b-model-evaluation-selection.html#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># add model object and our formula spec to a workflow object</span></span>
<span id="cb48-6"><a href="lesson-2b-model-evaluation-selection.html#cb48-6" aria-hidden="true" tabindex="-1"></a>rf_wflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb48-7"><a href="lesson-2b-model-evaluation-selection.html#cb48-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(rf_mod) <span class="sc">%&gt;%</span></span>
<span id="cb48-8"><a href="lesson-2b-model-evaluation-selection.html#cb48-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_formula</span>(Sale_Price <span class="sc">~</span> .)</span>
<span id="cb48-9"><a href="lesson-2b-model-evaluation-selection.html#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="lesson-2b-model-evaluation-selection.html#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit our model across the 10-fold CV</span></span>
<span id="cb48-11"><a href="lesson-2b-model-evaluation-selection.html#cb48-11" aria-hidden="true" tabindex="-1"></a>rf_fit_cv <span class="ot">&lt;-</span> rf_wflow <span class="sc">%&gt;%</span></span>
<span id="cb48-12"><a href="lesson-2b-model-evaluation-selection.html#cb48-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds)</span></code></pre></div>
<p>We can then get our average 10-fold cross validation error with <code>collect_metrics()</code>:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="lesson-2b-model-evaluation-selection.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(rf_fit_cv)</span>
<span id="cb49-2"><a href="lesson-2b-model-evaluation-selection.html#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb49-3"><a href="lesson-2b-model-evaluation-selection.html#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n   std_err .config             </span></span>
<span id="cb49-4"><a href="lesson-2b-model-evaluation-selection.html#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb49-5"><a href="lesson-2b-model-evaluation-selection.html#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   25978.       10 898.      Preprocessor1_Model1</span></span>
<span id="cb49-6"><a href="lesson-2b-model-evaluation-selection.html#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.902    10   0.00787 Preprocessor1_Model1</span></span></code></pre></div>
<p>If we want to see the model evaluation metric (i.e. RMSE) for each fold we just need to unnest the <code>rf_fit_cv</code> object.</p>
<div class="tip">
<p>
We have not discussed nested data frames but you can read about them
<a href="https://tidyr.tidyverse.org/articles/nest.html">here</a>
</p>
</div>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="lesson-2b-model-evaluation-selection.html#cb50-1" aria-hidden="true" tabindex="-1"></a>rf_fit_cv <span class="sc">%&gt;%</span> </span>
<span id="cb50-2"><a href="lesson-2b-model-evaluation-selection.html#cb50-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">unnest</span>(.metrics) <span class="sc">%&gt;%</span></span>
<span id="cb50-3"><a href="lesson-2b-model-evaluation-selection.html#cb50-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb50-4"><a href="lesson-2b-model-evaluation-selection.html#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 10 × 7</span></span>
<span id="cb50-5"><a href="lesson-2b-model-evaluation-selection.html#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="do">##    splits             id     .metric .estimator .estimate .config          .notes  </span></span>
<span id="cb50-6"><a href="lesson-2b-model-evaluation-selection.html#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="do">##    &lt;list&gt;             &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  </span></span>
<span id="cb50-7"><a href="lesson-2b-model-evaluation-selection.html#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  1 &lt;split [1845/206]&gt; Fold01 rmse    standard      26210. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-8"><a href="lesson-2b-model-evaluation-selection.html#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  2 &lt;split [1846/205]&gt; Fold02 rmse    standard      22089. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-9"><a href="lesson-2b-model-evaluation-selection.html#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  3 &lt;split [1846/205]&gt; Fold03 rmse    standard      30512. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-10"><a href="lesson-2b-model-evaluation-selection.html#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  4 &lt;split [1846/205]&gt; Fold04 rmse    standard      26106. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-11"><a href="lesson-2b-model-evaluation-selection.html#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  5 &lt;split [1846/205]&gt; Fold05 rmse    standard      25095. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-12"><a href="lesson-2b-model-evaluation-selection.html#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  6 &lt;split [1846/205]&gt; Fold06 rmse    standard      24337. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-13"><a href="lesson-2b-model-evaluation-selection.html#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  7 &lt;split [1846/205]&gt; Fold07 rmse    standard      23586. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-14"><a href="lesson-2b-model-evaluation-selection.html#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  8 &lt;split [1846/205]&gt; Fold08 rmse    standard      30650. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-15"><a href="lesson-2b-model-evaluation-selection.html#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  9 &lt;split [1846/205]&gt; Fold09 rmse    standard      27256. Preprocessor1_M… &lt;tibble&gt;</span></span>
<span id="cb50-16"><a href="lesson-2b-model-evaluation-selection.html#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 &lt;split [1846/205]&gt; Fold10 rmse    standard      23935. Preprocessor1_M… &lt;tibble&gt;</span></span></code></pre></div>
</div>
<div id="hyperparameter-tuning" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Hyperparameter tuning<a href="lesson-2b-model-evaluation-selection.html#hyperparameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Say you have the below relationship between a response variable and some predictor variable <code>x</code> (gray dots). Given two different models fit to this relationship (blue line), which model do you prefer?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2b-bias-variance-comparison"></span>
<img src="images/bias-variance-comparison.png" alt="Between model A and B, which do you think is better?" width="90%" height="90%" />
<p class="caption">
Figure 6.4: Between model A and B, which do you think is better?
</p>
</div>
<p>The image above illustrates the fact that prediction errors can be decomposed into two main subcomponents we care about:</p>
<ul>
<li>error due to “bias”</li>
<li>error due to “variance”</li>
</ul>
<p>Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models.</p>
<div id="bias" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Bias<a href="lesson-2b-model-evaluation-selection.html#bias" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Error due to <strong><em>bias</em></strong> is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model’s predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data.</p>
<p>The left image below illustrates an example where a polynomial model does not capture the underlying relationship well. Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships.</p>
<p>We also need to think of bias-variance in relation to resampling. Models with high bias are rarely affected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated by the right plot below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modeling-process-bias-model"></span>
<img src="_main_files/figure-html/modeling-process-bias-model-1.png" alt="A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left).  Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right)." width="960" />
<p class="caption">
Figure 6.5: A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left). Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).
</p>
</div>
</div>
<div id="variance" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Variance<a href="lesson-2b-model-evaluation-selection.html#variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Error due to <strong><em>variance</em></strong> is the variability of a model prediction for a given data point.</p>
<p>Many models (e.g., k-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data. Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modeling-process-variance-model"></span>
<img src="_main_files/figure-html/modeling-process-variance-model-1.png" alt="A high variance _k_-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left).  Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right)." width="960" />
<p class="caption">
Figure 6.6: A high variance <em>k</em>-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left). Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right).
</p>
</div>
<p>Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. Moreover, many algorithms that are capable of achieving high generalization performance have lots of hyperparameters that control the level of model complexity (i.e., the tradeoff between bias and variance).</p>
<div class="note">
<p>
Many high performing models (i.e. random forests, gradient boosting
machines, deep learning) are very flexible in the patterns they can
conform to due to the many hyperparameters they have. However, this also
means they are prone to overfitting (aka can have high variance
error).
</p>
</div>
</div>
<div id="hyperparameters" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> Hyperparameters<a href="lesson-2b-model-evaluation-selection.html#hyperparameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong><em>Hyperparameters</em></strong> (aka tuning parameters) are the “knobs to twiddle” to control the complexity of machine learning algorithms and, therefore, the <strong><em>bias-variance trade-off</em></strong>. Not all algorithms have hyperparameters (e.g., ordinary least squares8); however, most have at least one or more.</p>
<p>Some models have very few hyperparameters. For example, <em>k</em>-nearest neighbor models have a single hyperparameter (<em>k</em>) that determines the predicted value to be made based on the k nearest observations in the training data to the one being predicted. If k is small (e.g., <span class="math inline">\(k = 3\)</span>), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted. This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data. As <em>k</em> gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging often helps to reduce variance!).</p>
<p>The plot below illustrates this point. Smaller <em>k</em> values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal <em>k</em> value might exist somewhere between 20–50, but how do we know which value of k to use?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modeling-process-knn-options"></span>
<img src="_main_files/figure-html/modeling-process-knn-options-1.png" alt="_k_-nearest neighbor model with differing values for _k_." width="960" />
<p class="caption">
Figure 6.7: <em>k</em>-nearest neighbor model with differing values for <em>k</em>.
</p>
</div>
<div class="tip">
<p>
Some algorithms such as ordinary least squares don’t have any
hyperparameters. Some algorithms such as <em>k</em>-nearest neighbor
have one or two hyperparameters. And some algorithms such as gradient
boosted machines (GBMs) and deep learning models have many.
</p>
</div>
<p><strong><em>Hyperparameter tuning</em></strong> is the process of screening hyperparameter values (or combinations of hyperparameter values) to find a model that balances bias &amp; variance so that the model generalizes well to unseen data.</p>
<p>Let’s illustrate by using decision trees. One of the key hyperparameters in decision trees is the depth of the tree.</p>
<div class="tip">
<p>
This lesson does not dig into the decision tree algorithm but if you
want to better understand the hyperparameters for decision trees you can
read about them <a
href="https://bradleyboehmke.github.io/HOML/DT.html#how-deep">here</a>
</p>
</div>
<p>Say we wanted to assess what happens when we grow the decision tree 5 levels deep. We could do this manually:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="lesson-2b-model-evaluation-selection.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create our decision tree model object</span></span>
<span id="cb51-2"><a href="lesson-2b-model-evaluation-selection.html#cb51-2" aria-hidden="true" tabindex="-1"></a>dt_mod <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>(<span class="at">tree_depth =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb51-3"><a href="lesson-2b-model-evaluation-selection.html#cb51-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_mode</span>(<span class="st">&#39;regression&#39;</span>)</span>
<span id="cb51-4"><a href="lesson-2b-model-evaluation-selection.html#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="lesson-2b-model-evaluation-selection.html#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co"># add model object and our formula spec to a workflow object</span></span>
<span id="cb51-6"><a href="lesson-2b-model-evaluation-selection.html#cb51-6" aria-hidden="true" tabindex="-1"></a>dt_wflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb51-7"><a href="lesson-2b-model-evaluation-selection.html#cb51-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(dt_mod) <span class="sc">%&gt;%</span></span>
<span id="cb51-8"><a href="lesson-2b-model-evaluation-selection.html#cb51-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_formula</span>(Sale_Price <span class="sc">~</span> .)</span>
<span id="cb51-9"><a href="lesson-2b-model-evaluation-selection.html#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="lesson-2b-model-evaluation-selection.html#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit our model across the 10-fold CV</span></span>
<span id="cb51-11"><a href="lesson-2b-model-evaluation-selection.html#cb51-11" aria-hidden="true" tabindex="-1"></a>dt_fit_cv <span class="ot">&lt;-</span> dt_wflow <span class="sc">%&gt;%</span></span>
<span id="cb51-12"><a href="lesson-2b-model-evaluation-selection.html#cb51-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds)</span>
<span id="cb51-13"><a href="lesson-2b-model-evaluation-selection.html#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="lesson-2b-model-evaluation-selection.html#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co"># assess results</span></span>
<span id="cb51-15"><a href="lesson-2b-model-evaluation-selection.html#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(dt_fit_cv)</span>
<span id="cb51-16"><a href="lesson-2b-model-evaluation-selection.html#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb51-17"><a href="lesson-2b-model-evaluation-selection.html#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n   std_err .config             </span></span>
<span id="cb51-18"><a href="lesson-2b-model-evaluation-selection.html#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb51-19"><a href="lesson-2b-model-evaluation-selection.html#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   39175.       10 1382.     Preprocessor1_Model1</span></span>
<span id="cb51-20"><a href="lesson-2b-model-evaluation-selection.html#cb51-20" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.758    10    0.0156 Preprocessor1_Model1</span></span></code></pre></div>
<p>But what if we wanted to assess and compare different <code>tree_depth</code> values. Moreover, decision trees have another key hyperparameter <code>cost_complexity</code>. So what if we wanted to assess a few values of that hyperaparameter in combination with <code>tree_depth</code>? Adjusting these values manually would be painstakingly burdensome.</p>
<div class="note">
<p>
Again, don’t worry if you have no idea what these hyperparameters
mean. Just realize we want to toggle these values to try find an optimal
model.
</p>
</div>
</div>
<div id="full-cartesian-grid-search" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Full cartesian grid search<a href="lesson-2b-model-evaluation-selection.html#full-cartesian-grid-search" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this we could use a <strong><em>full cartesian grid search</em></strong>. A full cartesian grid search takes the values provided for each hyperparameter and assesses every combination.</p>
<p>First, let’s rebuild our decision tree model object; however, this time we’ll create a model specification that identifies which hyperparameters we plan to tune.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="lesson-2b-model-evaluation-selection.html#cb52-1" aria-hidden="true" tabindex="-1"></a>dt_mod <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>(</span>
<span id="cb52-2"><a href="lesson-2b-model-evaluation-selection.html#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(), <span class="co"># &lt;-- these are hyperparameters we want to tune</span></span>
<span id="cb52-3"><a href="lesson-2b-model-evaluation-selection.html#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()       <span class="co"># &lt;-- these are hyperparameters we want to tune</span></span>
<span id="cb52-4"><a href="lesson-2b-model-evaluation-selection.html#cb52-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb52-5"><a href="lesson-2b-model-evaluation-selection.html#cb52-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb52-6"><a href="lesson-2b-model-evaluation-selection.html#cb52-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<div class="note">
<p>
Think of <code>tune()</code> here as a placeholder. After the tuning
process, we will select a single numeric value for each of these
hyperparameters. For now, we specify our parsnip model object and
identify the hyperparameters we will <code>tune()</code>.
</p>
</div>
<p>Next, we create our tuning grid of hyperparameter values we want to assess. The function <code>grid_regular()</code> is from the <strong>dials</strong> package. It chooses sensible values to try for each hyperparameter; here, we asked for 3 of each. Since we have two to tune, <code>grid_regular()</code> returns <span class="math inline">\(3 \times 3 = 9\)</span> different possible tuning combinations to try.</p>
<div class="tip">
<p>
A full cartesian grid search can explode as you add more
hyperparameters and values to assess. When this happens its best to
start using <code>grid_random</code> or
<code>grid_latin_hypercube</code> to reduce combinatorial explosion and
computation time.
</p>
</div>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="lesson-2b-model-evaluation-selection.html#cb53-1" aria-hidden="true" tabindex="-1"></a>dt_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(</span>
<span id="cb53-2"><a href="lesson-2b-model-evaluation-selection.html#cb53-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">cost_complexity</span>(),</span>
<span id="cb53-3"><a href="lesson-2b-model-evaluation-selection.html#cb53-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">tree_depth</span>(),</span>
<span id="cb53-4"><a href="lesson-2b-model-evaluation-selection.html#cb53-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">levels =</span> <span class="dv">3</span></span>
<span id="cb53-5"><a href="lesson-2b-model-evaluation-selection.html#cb53-5" aria-hidden="true" tabindex="-1"></a>   )</span>
<span id="cb53-6"><a href="lesson-2b-model-evaluation-selection.html#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="lesson-2b-model-evaluation-selection.html#cb53-7" aria-hidden="true" tabindex="-1"></a>dt_grid</span>
<span id="cb53-8"><a href="lesson-2b-model-evaluation-selection.html#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 9 × 2</span></span>
<span id="cb53-9"><a href="lesson-2b-model-evaluation-selection.html#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="do">##   cost_complexity tree_depth</span></span>
<span id="cb53-10"><a href="lesson-2b-model-evaluation-selection.html#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             &lt;dbl&gt;      &lt;int&gt;</span></span>
<span id="cb53-11"><a href="lesson-2b-model-evaluation-selection.html#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    0.0000000001          1</span></span>
<span id="cb53-12"><a href="lesson-2b-model-evaluation-selection.html#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    0.00000316            1</span></span>
<span id="cb53-13"><a href="lesson-2b-model-evaluation-selection.html#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 3    0.1                   1</span></span>
<span id="cb53-14"><a href="lesson-2b-model-evaluation-selection.html#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 4    0.0000000001          8</span></span>
<span id="cb53-15"><a href="lesson-2b-model-evaluation-selection.html#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 5    0.00000316            8</span></span>
<span id="cb53-16"><a href="lesson-2b-model-evaluation-selection.html#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 6    0.1                   8</span></span>
<span id="cb53-17"><a href="lesson-2b-model-evaluation-selection.html#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 7    0.0000000001         15</span></span>
<span id="cb53-18"><a href="lesson-2b-model-evaluation-selection.html#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 8    0.00000316           15</span></span>
<span id="cb53-19"><a href="lesson-2b-model-evaluation-selection.html#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 9    0.1                  15</span></span></code></pre></div>
<p>Now that we have our tuning grid and model object defined we can:</p>
<ol style="list-style-type: decimal">
<li>Create our k-fold CV object (5-fold in this example),</li>
<li>Add our model to a workflow object and specify the formula,</li>
<li>Apply <code>tune_grid()</code> to execute our hyperparameter tuning.</li>
</ol>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="lesson-2b-model-evaluation-selection.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5-fold instead of 10-fold to reduce computation time</span></span>
<span id="cb54-2"><a href="lesson-2b-model-evaluation-selection.html#cb54-2" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(train, <span class="at">v =</span> <span class="dv">5</span>)</span>
<span id="cb54-3"><a href="lesson-2b-model-evaluation-selection.html#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="lesson-2b-model-evaluation-selection.html#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># add model object and our formula spec to a workflow object</span></span>
<span id="cb54-5"><a href="lesson-2b-model-evaluation-selection.html#cb54-5" aria-hidden="true" tabindex="-1"></a>dt_wflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb54-6"><a href="lesson-2b-model-evaluation-selection.html#cb54-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(dt_mod) <span class="sc">%&gt;%</span></span>
<span id="cb54-7"><a href="lesson-2b-model-evaluation-selection.html#cb54-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_formula</span>(Sale_Price <span class="sc">~</span> .)</span>
<span id="cb54-8"><a href="lesson-2b-model-evaluation-selection.html#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="lesson-2b-model-evaluation-selection.html#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># fit our model across the 5-fold CV</span></span>
<span id="cb54-10"><a href="lesson-2b-model-evaluation-selection.html#cb54-10" aria-hidden="true" tabindex="-1"></a>dt_grid_search <span class="ot">&lt;-</span> dt_wflow <span class="sc">%&gt;%</span></span>
<span id="cb54-11"><a href="lesson-2b-model-evaluation-selection.html#cb54-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">tune_grid</span>(</span>
<span id="cb54-12"><a href="lesson-2b-model-evaluation-selection.html#cb54-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> kfolds,</span>
<span id="cb54-13"><a href="lesson-2b-model-evaluation-selection.html#cb54-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> dt_grid</span>
<span id="cb54-14"><a href="lesson-2b-model-evaluation-selection.html#cb54-14" aria-hidden="true" tabindex="-1"></a>   )</span></code></pre></div>
<p>We can check out the hyperparameter combinations that resulted in the best model performance with <code>show_best()</code>. Here we look at the top 5 models and we can see that the top 4 all tend to perform very similarly. It appears that deeper trees with smaller cost complexity factor tend to perform best on this data set.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="lesson-2b-model-evaluation-selection.html#cb55-1" aria-hidden="true" tabindex="-1"></a>dt_grid_search <span class="sc">%&gt;%</span></span>
<span id="cb55-2"><a href="lesson-2b-model-evaluation-selection.html#cb55-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb55-3"><a href="lesson-2b-model-evaluation-selection.html#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 5 × 8</span></span>
<span id="cb55-4"><a href="lesson-2b-model-evaluation-selection.html#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   cost_complexity tree_depth .metric .estimator   mean     n std_err .config       </span></span>
<span id="cb55-5"><a href="lesson-2b-model-evaluation-selection.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="do">##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         </span></span>
<span id="cb55-6"><a href="lesson-2b-model-evaluation-selection.html#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    0.0000000001          8 rmse    standard   36638.     5   1515. Preprocessor1…</span></span>
<span id="cb55-7"><a href="lesson-2b-model-evaluation-selection.html#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    0.00000316            8 rmse    standard   36638.     5   1515. Preprocessor1…</span></span>
<span id="cb55-8"><a href="lesson-2b-model-evaluation-selection.html#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 3    0.0000000001         15 rmse    standard   36703.     5   1587. Preprocessor1…</span></span>
<span id="cb55-9"><a href="lesson-2b-model-evaluation-selection.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 4    0.00000316           15 rmse    standard   36703.     5   1587. Preprocessor1…</span></span>
<span id="cb55-10"><a href="lesson-2b-model-evaluation-selection.html#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 5    0.1                   8 rmse    standard   51944.     5   1834. Preprocessor1…</span></span></code></pre></div>
</div>
</div>
<div id="finalizing-our-model" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Finalizing our model<a href="lesson-2b-model-evaluation-selection.html#finalizing-our-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we are satisfied with our results and we want to use the best hyperparameter values for our best decision tree model, we can select it with <code>select_best()</code>:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="lesson-2b-model-evaluation-selection.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select best model based on RMSE metric</span></span>
<span id="cb56-2"><a href="lesson-2b-model-evaluation-selection.html#cb56-2" aria-hidden="true" tabindex="-1"></a>best_tree <span class="ot">&lt;-</span> dt_grid_search <span class="sc">%&gt;%</span></span>
<span id="cb56-3"><a href="lesson-2b-model-evaluation-selection.html#cb56-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">select_best</span>(<span class="at">metric =</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb56-4"><a href="lesson-2b-model-evaluation-selection.html#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="lesson-2b-model-evaluation-selection.html#cb56-5" aria-hidden="true" tabindex="-1"></a>best_tree</span>
<span id="cb56-6"><a href="lesson-2b-model-evaluation-selection.html#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb56-7"><a href="lesson-2b-model-evaluation-selection.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   cost_complexity tree_depth .config             </span></span>
<span id="cb56-8"><a href="lesson-2b-model-evaluation-selection.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="do">##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;               </span></span>
<span id="cb56-9"><a href="lesson-2b-model-evaluation-selection.html#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    0.0000000001          8 Preprocessor1_Model4</span></span></code></pre></div>
<p>We can then update (or “finalize”) our workflow object <code>dt_wflow</code> with the values from <code>select_best()</code>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="lesson-2b-model-evaluation-selection.html#cb57-1" aria-hidden="true" tabindex="-1"></a>final_wflow <span class="ot">&lt;-</span> dt_wflow <span class="sc">%&gt;%</span></span>
<span id="cb57-2"><a href="lesson-2b-model-evaluation-selection.html#cb57-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">finalize_workflow</span>(best_tree)</span></code></pre></div>
<p>Finally, let’s fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function <code>last_fit()</code> with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.</p>
<div class="note">
<p>
We pass the initial train-test split object we created at the
beginning of this lesson to <code>last_fit</code>.
</p>
</div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="lesson-2b-model-evaluation-selection.html#cb58-1" aria-hidden="true" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> final_wflow <span class="sc">%&gt;%</span></span>
<span id="cb58-2"><a href="lesson-2b-model-evaluation-selection.html#cb58-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">last_fit</span>(split)</span>
<span id="cb58-3"><a href="lesson-2b-model-evaluation-selection.html#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="lesson-2b-model-evaluation-selection.html#cb58-4" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb58-5"><a href="lesson-2b-model-evaluation-selection.html#cb58-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb58-6"><a href="lesson-2b-model-evaluation-selection.html#cb58-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">rmse</span>(<span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb58-7"><a href="lesson-2b-model-evaluation-selection.html#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb58-8"><a href="lesson-2b-model-evaluation-selection.html#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb58-9"><a href="lesson-2b-model-evaluation-selection.html#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb58-10"><a href="lesson-2b-model-evaluation-selection.html#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard      34609.</span></span></code></pre></div>
<p>As we can see our test RMSE is less than our CV RMSE. This indicates that we did not overfit during our tuning procedure, which is a good thing.</p>
<p>Perhaps we would also like to understand what variables are important in this final model. We can use the <strong>vip</strong> package to estimate variable importance based on the model’s structure.</p>
<div class="note">
<p>
Don’t worry, we’ll talk about <strong>vip</strong> more later on and
just how this measure of importance is computed.
</p>
</div>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="lesson-2b-model-evaluation-selection.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb59-2"><a href="lesson-2b-model-evaluation-selection.html#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="lesson-2b-model-evaluation-selection.html#cb59-3" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span> </span>
<span id="cb59-4"><a href="lesson-2b-model-evaluation-selection.html#cb59-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb59-5"><a href="lesson-2b-model-evaluation-selection.html#cb59-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">vip</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/2b-vip-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Exercises<a href="lesson-2b-model-evaluation-selection.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Import the dataset blood_transfusion.csv:
</p>
<ol style="list-style-type: decimal">
<li>
The column “Class” contains the target variable. Investigate this
variable. Is this a regression or classification problem?
</li>
<li>
Why is it relevant to add a preprocessing step to standardize the
features? What <code>step_xxx()</code> function would you use to do
so?
</li>
<li>
Perform a k-nearest neighbor model on this data with
<code>neighbors = 10</code>. Be sure to add a preprocessing step to
standardize the features.
</li>
<li>
Perform a 5-fold cross validation with the above model workflow.
What is your average CV score?
</li>
<li>
Now perform hyperparameter tuning to understand the effect of the
parameter <code>neighbors</code> on the model score. Assess 10 values
for <code>neighbors</code> between the range of 1-100. Again, perform a
5-fold cross validation. Which hyperparameter value performed the best
and what was the CV score?
</li>
</ol>
</div>
</div>
<div id="additional-resources" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Additional resources<a href="lesson-2b-model-evaluation-selection.html#additional-resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This module provided a very high-level introduction to predictive modeling with <strong>tidymodels</strong>. To build on this knowledge you can find many great resources at <a href="https://www.tidymodels.org/" class="uri">https://www.tidymodels.org/</a>.</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="lesson-2a-feature-engineering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computing-environment.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-2/lesson-2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
