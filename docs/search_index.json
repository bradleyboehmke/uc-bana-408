[["index.html", "Data Mining with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Mining with R Bradley Boehmke Syllabus This is the primary “textbook” for the Machine Learning section of the UC BANA 4080 Data Mining course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Mining with R! This course provides an intensive, hands-on introduction to data mining and analysis techniques. You will learn the fundamental skills required to extract informative attributes, relationships, and patterns from data sets. You will gain hands-on experience with exploratory data analysis, data visualization, unsupervised learning techniques such as clustering and dimension reduction, and supervised learning techniques such as linear regression, regularized regression, decision trees, random forests, and more! You will also be exposed to some more advanced topics such as ensembling techniques, deep learning, model stacking, and model interpretation. Together, this will provide you with a solid foundation of tools and techniques applied in organizations to aid modern day data-driven decision making. Check out the video in the “Overview of Course” module on Canvas for a quick introduction to this course. Learning Objectives Upon successfully completing this course, you will be able to: Apply data wrangling techniques to manipulate and prepare data for analysis. Use exploratory data analysis and visualization to provide descriptive insights of data. Apply common unsupervised learning algorithms to find common groupings of observations and features in a given dataset. Describe and apply a sound analytic modeling process. Apply, compare, and contrast various predictive modeling techniques. Have the resources and understanding to continue advancing your data mining and analysis capabilities. …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material This course is split into two main sections - Data Wrangling and Machine Learning. The data wrangling section will provide you the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. The primary course material for this section is provided via this free online book. The second section focused on machine learning section will expose you to several algorithms to identify hidden patterns and relationships within your data. The primary course material for this part of the course is provided via this free online book. There will also be recorded lectures and additional supplementary resources provided via Canvas. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “TODO” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. These labs will be provided via the course website on Canvas and the submission of these labs will also be done through the course website on Canvas. Please check Canvas for due dates for these labs. Projects: There will be two projects designed for you to put to work the tools and knowledge that you gain throughout this course. This provides you with multiple benefits. - It will provide you with more experience using data wrangling tools on real life data sets. - It helps you become a self-directed learner. As a data scientist, a large part of your job is to self-direct your learning and interests to find unique and creative ways to find insights in data. - It starts to build your data science portfolio. Establishing a data science portfolio is a great way to show potential employers your ability to work with data. Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description DATA WRANGLING 1 Introduction R fundamentals &amp; the Rstudio IDE Deeper understanding of vectors 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Data structures &amp; importing data 3 Tidy Data and Data Manipulation Data manipulation &amp; summarization Tidy data 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to text &amp; date-time data 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions MACHINE LEARNING 7 Introduction to Applied Modeling Introduction to tidymodels Feature engineering &amp; model evaluation/selection 8 First regression models Ordinary least squares (OLS) OLS cousins 9 First classification models Logistic regression Assessing classification models 10 More regression cousins Regularized regression Multi-adaptive Regression Splines (MARS) 11 Venturing away from linearity K-Nearest neighbor Decision trees 12 Ensembling trees Bagging Random forests 13 Ensembling trees continued Gradient boosting XGBoost and other variants 14 Deep learning Feedforward neural nets A survey of deep learning extensions 15 Unsupervised Learning Clustering. Dimension reductions Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A video demonstrating this topic is available in Canvas. A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-4080/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA4080 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R Brandon Greenwell, Hands-on Machine Learning with R "],["overview.html", "1 Overview 1.1 Learning objectives 1.2 Estimated time requirement 1.3 Tasks", " 1 Overview Before introducing specific machine learning (ML) algorithms, it is important that we have a solid understanding of the overall objective of ML algorithms and the common problems they can address. Consequently, this module provides an introduction to ML and starts to introduce parts of the ML modeling process that you’ll routinely see in future modeling lessons. 1.1 Learning objectives By the end of this module you should be able to: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic predictive model. 1.2 Estimated time requirement The estimated time to go through the module lessons is about 3 hours. 1.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material). "],["lesson-1a-intro-to-machine-learning.html", "2 Lesson 1a: Intro to machine learning 2.1 Learning objectives 2.2 Supervised learning 2.3 Unsupervised learning 2.4 Machine Learning in 2.5 The data sets 2.6 What You’ll Learn Next 2.7 Exercises", " 2 Lesson 1a: Intro to machine learning Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include: Predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge. Segmenting customers based on common attributes or purchasing behavior for targeted marketing. Predicting coupon redemption rates for a given marketing campaign. Predicting customer churn so an organization can perform preventative intervention. And many more! In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision needed during training. 2.1 Learning objectives This lesson will introduce you to some fundamental concepts around ML and this class. By the end of this lesson you will: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Be able to import and explore the data sets we’ll use through various examples. 2.2 Supervised learning A predictive model is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks; using home attributes to predict the sales price; using employee attributes to predict the likelihood of attrition; using patient attributes and symptoms to predict the risk of readmission; using production attributes to predict time to market. Each of these examples has a defined learning task; they each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this course we’ll use various terms interchangeably for \\(X\\): “predictor variable”, “independent variable”, “attribute”, “feature”, “predictor” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the target values. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Most supervised learning problems can be bucketed into one of two categories, regression or classification, which we discuss next. 2.2.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between $80,000 and $755,000). The figure below illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane. Figure 2.1: Average home sales price as a function of year built and total square footage. 2.2.2 Classification problems When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: Did a customer redeem a coupon (coded as yes/no or 1/0)? Did a customer churn (coded as yes/no or 1/0)? Did a customer click on our online ad (coded as yes/no or 1/0)? Classifying customer reviews: Binary: positive vs. negative. Multinomial: extremely negative to extremely positive on a 0–5 Likert scale. Figure 2.2: Classification problem modeling ‘Yes’/‘No’ response based on three features. However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., “yes” or “no”), we often want to predict the probability of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, many of the supervised learning algorithms we cover in this class can be applied to both. These algorithms have become the most popular machine learning applications in recent years. 2.3 Unsupervised learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression). Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised! Despite its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of unsupervised learning models can be used as inputs to downstream supervised learning models. 2.4 Machine Learning in Historically, the R ecosystem provides a wide variety of ML algorithm implementations. This has its benefits; however, this also has drawbacks as it requires the users to learn many different formula interfaces and syntax nuances. More recently, development on a group of packages called Tidymodels has helped to make implementation easier. The tidymodels collection allows you to perform discrete parts of the ML workflow with discrete packages: rsample for data splitting and resampling recipes for data pre-processing and feature engineering parsnip for applying algorithms tune for hyperparameter tuning yardstick for measuring model performance and several others! Throughout this course you’ll be exposed to several of these packages. Go ahead and make sure you have the following packages installed. Just like the tidyverse package, when you install tidymodels you are actually installing several packages that exist in the tidymodels ecosystem as discussed above. # common data wrangling and visualization install.packages(&quot;tidyverse&quot;) install.packages(&quot;vip&quot;) install.packages(&quot;here&quot;) # modeling install.packages(&quot;tidymodels&quot;) packageVersion(&quot;tidymodels&quot;) ## [1] &#39;0.2.0&#39; library(tidymodels) ## ── Attaching packages ──────────────────── tidymodels 0.2.0 ── ## ✔ broom 1.0.0 ✔ rsample 0.1.1 ## ✔ dials 1.0.0 ✔ tibble 3.1.8 ## ✔ dplyr 1.0.9 ✔ tidyr 1.2.0 ## ✔ infer 1.0.2 ✔ tune 0.2.0 ## ✔ modeldata 0.1.1 ✔ workflows 0.2.6 ## ✔ parsnip 1.0.0 ✔ workflowsets 0.2.1 ## ✔ purrr 0.3.4 ✔ yardstick 1.0.0 ## ✔ recipes 0.2.0 ## ── Conflicts ─────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks plotly::filter(), stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ recipes::step() masks stats::step() ## • Dig deeper into tidy modeling with R at https://www.tmwr.org 2.5 The data sets The data sets chosen for this course allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this course is to demonstrate how to implement ML workflows, we make the assumption that you have already spent significant time wrangling, cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this course such as: Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process). Recoding variable names and values so that they are meaningful and more interpretable. Tidying data so that each column is a discrete variable and each row is an individual observation. Recoding, removing, or some other approach to handling missing values. Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. As mentioned above, these data sets are fairly common data sets that provide good benchmarks to compare and illustrate ML workflows. Although some of these data sets are available in R, we will import these data sets from a .csv file to ensure consistency over time. 2.5.1 Boston housing The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978) , it contains 13 attributes to predict the median property value. problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes # data file path library(here) data_path &lt;- here(&quot;data&quot;) # access data boston &lt;- readr::read_csv(here(data_path, &quot;boston.csv&quot;)) # initial dimension dim(boston) ## [1] 506 16 # features dplyr::select(boston, -cmedv) ## # A tibble: 506 × 15 ## lon lat crim zn indus chas nox rm age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.0 42.3 0.00632 18 2.31 0 0.538 6.58 65.2 ## 2 -71.0 42.3 0.0273 0 7.07 0 0.469 6.42 78.9 ## 3 -70.9 42.3 0.0273 0 7.07 0 0.469 7.18 61.1 ## 4 -70.9 42.3 0.0324 0 2.18 0 0.458 7.00 45.8 ## 5 -70.9 42.3 0.0690 0 2.18 0 0.458 7.15 54.2 ## 6 -70.9 42.3 0.0298 0 2.18 0 0.458 6.43 58.7 ## 7 -70.9 42.3 0.0883 12.5 7.87 0 0.524 6.01 66.6 ## 8 -70.9 42.3 0.145 12.5 7.87 0 0.524 6.17 96.1 ## 9 -70.9 42.3 0.211 12.5 7.87 0 0.524 5.63 100 ## 10 -70.9 42.3 0.170 12.5 7.87 0 0.524 6.00 85.9 ## # … with 496 more rows, and 6 more variables: dis &lt;dbl&gt;, ## # rad &lt;dbl&gt;, tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, b &lt;dbl&gt;, lstat &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(boston$cmedv) ## [1] 24.0 21.6 34.7 33.4 36.2 28.7 2.5.2 Pima Indians Diabetes A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and published in smith1988using , it contains 8 attributes to predict the presence of diabetes. problem type: supervised binary classification response variable: diabetes positive or negative response (i.e. “pos”, “neg”) features: 8 observations: 768 objective: use biological attributes to predict the presence of diabetes # access data pima &lt;- readr::read_csv(here(data_path, &quot;pima.csv&quot;)) # initial dimension dim(pima) ## [1] 768 9 # features dplyr::select(pima, -diabetes) ## # A tibble: 768 × 8 ## pregn…¹ glucose press…² triceps insulin mass pedig…³ age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 148 72 35 0 33.6 0.627 50 ## 2 1 85 66 29 0 26.6 0.351 31 ## 3 8 183 64 0 0 23.3 0.672 32 ## 4 1 89 66 23 94 28.1 0.167 21 ## 5 0 137 40 35 168 43.1 2.29 33 ## 6 5 116 74 0 0 25.6 0.201 30 ## 7 3 78 50 32 88 31 0.248 26 ## 8 10 115 0 0 0 35.3 0.134 29 ## 9 2 197 70 45 543 30.5 0.158 53 ## 10 8 125 96 0 0 0 0.232 54 ## # … with 758 more rows, and abbreviated variable names ## # ¹​pregnant, ²​pressure, ³​pedigree ## # ℹ Use `print(n = ...)` to see more rows # response variable head(pima$diabetes) ## [1] &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; 2.5.3 Iris flowers The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper (Fisher 1936) . It is sometimes called Anderson’s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. problem type: supervised multinomial classification response variable: species (i.e. “setosa”, “virginica”, “versicolor”) features: 4 observations: 150 objective: use plant leaf attributes to predict the type of flower # access data iris &lt;- readr::read_csv(here(data_path, &quot;iris.csv&quot;)) # initial dimension dim(iris) ## [1] 150 5 # features dplyr::select(iris, -Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows ## # ℹ Use `print(n = ...)` to see more rows # response variable head(iris$Species) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; 2.5.4 Ames housing The Ames housing data set is an alternative to the Boston housing data set and provides a more comprehensive set of home features to predict sales price. More information can be found in De Cock (2011) . problem type: supervised regression response variable: Sale_Price (i.e., $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home # access data ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) # initial dimension dim(ames) ## [1] 2930 81 # features dplyr::select(ames, -Sale_Price) ## # A tibble: 2,930 × 80 ## MS_SubClass MS_Zo…¹ Lot_F…² Lot_A…³ Street Alley Lot_S…⁴ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_194… Reside… 141 31770 Pave No_A… Slight… ## 2 One_Story_194… Reside… 80 11622 Pave No_A… Regular ## 3 One_Story_194… Reside… 81 14267 Pave No_A… Slight… ## 4 One_Story_194… Reside… 93 11160 Pave No_A… Regular ## 5 Two_Story_194… Reside… 74 13830 Pave No_A… Slight… ## 6 Two_Story_194… Reside… 78 9978 Pave No_A… Slight… ## 7 One_Story_PUD… Reside… 41 4920 Pave No_A… Regular ## 8 One_Story_PUD… Reside… 43 5005 Pave No_A… Slight… ## 9 One_Story_PUD… Reside… 39 5389 Pave No_A… Slight… ## 10 Two_Story_194… Reside… 60 7500 Pave No_A… Regular ## # … with 2,920 more rows, 73 more variables: ## # Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, ## # Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Qual &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(ames$Sale_Price) ## [1] 215000 105000 172000 244000 189900 195500 2.5.5 Attrition The employee attrition data set was originally provided by IBM Watson Analytics Lab and is a fictional data set created by IBM data scientists to explore what employee attributes influence attrition. problem type: supervised binomial classification response variable: Attrition (i.e., “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) # access data attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) # initial dimension dim(attrition) ## [1] 1470 31 # features dplyr::select(attrition, -Attrition) ## # A tibble: 1,470 × 30 ## Age BusinessTra…¹ Daily…² Depar…³ Dista…⁴ Educa…⁵ Educa…⁶ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 41 Travel_Rarely 1102 Sales 1 College Life_S… ## 2 49 Travel_Frequ… 279 Resear… 8 Below_… Life_S… ## 3 37 Travel_Rarely 1373 Resear… 2 College Other ## 4 33 Travel_Frequ… 1392 Resear… 3 Master Life_S… ## 5 27 Travel_Rarely 591 Resear… 2 Below_… Medical ## 6 32 Travel_Frequ… 1005 Resear… 2 College Life_S… ## 7 59 Travel_Rarely 1324 Resear… 3 Bachel… Medical ## 8 30 Travel_Rarely 1358 Resear… 24 Below_… Life_S… ## 9 38 Travel_Frequ… 216 Resear… 23 Bachel… Life_S… ## 10 36 Travel_Rarely 1299 Resear… 27 Bachel… Medical ## # … with 1,460 more rows, 23 more variables: ## # EnvironmentSatisfaction &lt;chr&gt;, Gender &lt;chr&gt;, ## # HourlyRate &lt;dbl&gt;, JobInvolvement &lt;chr&gt;, JobLevel &lt;dbl&gt;, ## # JobRole &lt;chr&gt;, JobSatisfaction &lt;chr&gt;, ## # MaritalStatus &lt;chr&gt;, MonthlyIncome &lt;dbl&gt;, ## # MonthlyRate &lt;dbl&gt;, NumCompaniesWorked &lt;dbl&gt;, ## # OverTime &lt;chr&gt;, PercentSalaryHike &lt;dbl&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(attrition$Attrition) ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 2.5.6 Hitters This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. The idea was to illustrate if and how major league baseball player’s batting performance could predict their salary. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. Note that the data does contain the players name but this should be removed during analysis and is not a valid feature. problem type: supervised regression response variable: Salary features: 19 observations: 322 objective: use baseball player’s batting attributes to predict their salary. # access data hitters &lt;- readr::read_csv(here(data_path, &quot;hitters.csv&quot;)) # initial dimension dim(hitters) ## [1] 322 21 # features dplyr::select(hitters, -Salary, -Player) ## # A tibble: 322 × 19 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 293 66 1 30 29 14 1 293 66 ## 2 315 81 7 24 38 39 14 3449 835 ## 3 479 130 18 66 72 76 3 1624 457 ## 4 496 141 20 65 78 37 11 5628 1575 ## 5 321 87 10 39 42 30 2 396 101 ## 6 594 169 4 74 51 35 11 4408 1133 ## 7 185 37 1 23 8 21 2 214 42 ## 8 298 73 0 24 24 7 3 509 108 ## 9 323 81 6 26 32 8 2 341 86 ## 10 401 92 17 49 66 65 13 5206 1332 ## # … with 312 more rows, and 10 more variables: CHmRun &lt;dbl&gt;, ## # CRuns &lt;dbl&gt;, CRBI &lt;dbl&gt;, CWalks &lt;dbl&gt;, League &lt;chr&gt;, ## # Division &lt;chr&gt;, PutOuts &lt;dbl&gt;, Assists &lt;dbl&gt;, ## # Errors &lt;dbl&gt;, NewLeague &lt;chr&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(hitters$Salary) ## [1] NA 475.0 480.0 500.0 91.5 750.0 2.6 What You’ll Learn Next The lessons that follow are designed to help you understand the individual sub-tasks of an ML project. The focus is to have an intuitive understanding of each discrete sub-task and algorithm. Once you understand when, where, and why these sub-tasks are performed you will be able to transfer this knowledge to other projects. The concepts you will learn include: Provide an overview of the ML modeling process: data splitting model fitting model validation and tuning performance measurement feature engineering Cover common supervised learners: linear regression regularized regression K-nearest neighbors decision trees bagging &amp; random forests gradient boosting Cover common unsupervised learners: K-means clustering Principal component analysis Along the way you’ll learn about: each algorithm’s hyperparameters model interpretation feature importance and more! 2.7 Exercises Identify four real-life applications of supervised and unsupervised problems. Explain what makes these problems supervised versus unsupervised. For each problem identify the target variable (if applicable) and potential features. Identify and contrast a regression problem with a classification problem. What is the target variable in each problem and why would being able to accurately predict this target be beneficial to society? What are potential features and where could you collect this information? What is determining if the problem is a regression or a classification problem? Identify three open source data sets suitable for machine learning (e.g., https://bit.ly/35wKu5c). Explain the type of machine learning models that could be constructed from the data (e.g., supervised versus unsupervised and regression versus classification). What are the dimensions of the data? Is there a code book that explains who collected the data, why it was originally collected, and what each variable represents? If the data set is suitable for supervised learning, which variable(s) could be considered as a useful target? Which variable(s) could be considered as features? Identify examples of misuse of machine learning in society. What was the ethical concern? References "],["lesson-1b-first-model-with-tidymodels.html", "3 Lesson 1b: First model with Tidymodels 3.1 Learning objectives 3.2 Prerequisites 3.3 Data splitting 3.4 Building models 3.5 Making predictions 3.6 Evaluating model performance 3.7 Exercises", " 3 Lesson 1b: First model with Tidymodels Much like exploratory data analysis (EDA), the machine learning (ML) process is very iterative and heuristic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the no free lunch theorem for ML (Wolpert 1996). Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging.1 Approaching ML modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Many books and courses portray the modeling process as a short sprint. A better analogy would be a marathon where many iterations of these steps are repeated before eventually finding the final optimal model. This process is illustrated below. Figure 3.1: General predictive machine learning process. Before introducing specific algorithms, this lesson, along with the next module, introduces concepts that are fundamental to the ML modeling process and that you’ll see briskly covered in future modeling lessons. More specifically, this lesson is designed to get you acquainted with building predictive models using the Tidymodels construct. We’ll focus on the process of splitting our data for improved generalizability, using Tidymodel’s parsnip package for constructing our models, along with yardstick to measure model performance. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. 3.1 Learning objectives By the end of this lesson you will be able to: Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic model. 3.2 Prerequisites For this lesson we’ll primarily use the tidymodels package. library(tidymodels) library(here) The two data sets we’ll use are ames and attrition. data_path &lt;- here(&quot;data&quot;) ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) When performing classification models our response variable needs to be a factor (or sometimes as 0 vs. 1). Consequently, the code chunk below sets the Attrition response variable as a factor rather than as a character. attrition &lt;- attrition %&gt;% dplyr::mutate(Attrition = as.factor(Attrition)) 3.3 Data splitting A major goal of the machine learning process is to find an algorithm \\(f\\left(X\\right)\\) that most accurately predicts future values (\\(\\hat{Y}\\)) based on a set of features (\\(X\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets: Training set: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production). Test set: having chosen a final model, these data are used to estimate an unbiased assessment of the model’s performance, which we refer to as the generalization error. Figure 3.2: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60% (training)–40% (testing), 70%–30%, or 80%–20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind: Spending too much in training (e.g., \\(&gt;80\\%\\)) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting). Sometimes too much spent in testing (\\(&gt;40\\%\\)) won’t allow us to get a good assessment of model parameters. Other factors should also influence the allocation proportions. For example, very large training sets (e.g., \\(n &gt; 100\\texttt{K}\\)) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as \\(p \\geq n\\) (where \\(p\\) represents the number of features), larger samples sizes are often required to identify consistent signals in the features. The two most common ways of splitting data include simple random sampling and stratified sampling. 3.3.1 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the distribution of your response variable (\\(Y\\)). Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this course we’ll often use the seed 123 for reproducibility but the number itself has no special meaning. # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) train &lt;- training(split) test &lt;- testing(split) # dimensions of training data dim(train) ## [1] 2051 81 With sufficient sample size, this sampling approach will typically result in a similar distribution of \\(Y\\) (e.g., Sale_Price in the ames data) between your training and test sets, as illustrated below. train %&gt;% mutate(id = &#39;train&#39;) %&gt;% bind_rows(test %&gt;% mutate(id = &#39;test&#39;)) %&gt;% ggplot(aes(Sale_Price, color = id)) + geom_density() 3.3.2 Stratified sampling If we want to explicitly control the sampling so that our training and test sets have similar \\(Y\\) distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90% of observations with response “Yes” and 10% with response “No”). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will segment \\(Y\\) into quantiles and randomly sample from each. To perform stratified sampling we simply apply the strata argument in initial_split. set.seed(123) split_strat &lt;- initial_split(attrition, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions. # original response distribution table(attrition$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # response distribution for training data table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8394942 0.1605058 # response distribution for test data table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8371041 0.1628959 3.3.3 Knowledge check Import the penguins data from the modeldata package Create a 70-30 stratified train-test split (species is the target variable). What are the response variable proportions for the train and test data sets? 3.4 Building models The R ecosystem provides a wide variety of ML algorithm implementations. This makes many powerful algorithms available at your fingertips. Moreover, there are almost always more than one package to perform each algorithm (e.g., there are over 20 packages for fitting random forests). There are pros and cons to this wide selection; some implementations may be more computationally efficient while others may be more flexible. This also has resulted in some drawbacks as there are inconsistencies in how algorithms allow you to define the formula of interest and how the results and predictions are supplied. Fortunately, the tidymodels ecosystem is simplifying this and, in particular, the Parsnip package provides one common interface to train many different models supplied by other packages. Consequently, we’ll focus on building models the tidymodels way. To create and fit a model with parsnip we follow 3 steps: Create a model type Choose an “engine” Fit our model Let’s illustrate by building a linear regression model. For our first model we will simply use two features from our training data - total square feet of the home (Gr_Liv_Area) and year built (Year_Built) to predict the sale price (Sale_Price). We can use tidy() to get results of our model’s parameter estimates and their statistical properties. Although the summary() function can provide this output, it gives the results back in an unwieldy format. Go ahead, and run summary(lm_ols) to compare the results to what we see below. Many models have a tidy() method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names) lm_ols &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) tidy(lm_ols) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2157423. 69234. -31.2 8.09e-175 ## 2 Gr_Liv_Area 94.4 2.12 44.4 2.54e-302 ## 3 Year_Built 1114. 35.5 31.4 5.30e-177 Now, you may have noticed that I only applied two of the three steps I mentioned previously: Create a model type Choose an “engine” Fit our model The reason is because most model objects (linear_reg() in this example) have a default engine. linear_reg() by default uses lm for ordinary least squares. But we can always change the engine. For example, say I wanted to use keras to perform gradient descent linear regression, then I could change the engine to keras but use the same code workflow. For this code to run successfully on your end you need to have the keras and tensorflow packages installed on your machine. Depending on your current setup this could be an easy process or you could run into problems. If you run into problems don’t fret, this is primarily just to illustrate how we can change engines. lm_sgd &lt;- linear_reg() %&gt;% set_engine(&#39;keras&#39;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) When we talk about ‘engines’ we’re really just referring to packages that provide the desired algorithm. Each model object has different engines available to use and they are all documented. For example check out the help file for linear_reg (?linear_reg) and you’ll see the different engines available (lm, brulee, glm, glmnet, etc.) The beauty of this workflow is that if we want to explore different models we can simply change the model object. For example, say we wanted to run a K-nearest neighbor model. We can just use nearest_neighbor(). In this example we have pretty much the same code as above except we added the line of code set_mode(). This is because most algorithms require you to specify if you are building a regression model or a classification model. When you run this code you’ll probably get an error message saying that “This engine requires some package installs: ‘kknn’.” This just means you need to install.packages(‘kknn’) and then you should be able to successfully run this code. knn &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) You can see all the different model objects available at https://parsnip.tidymodels.org/reference/index.html 3.4.1 Knowledge check If you haven’t already done so, create a 70-30 stratified train-test split on the attrition data (note: Attrition is the response variable). Using the logistic_reg() model object, fit a model using Age, DistanceFromHome, and JobLevel as the features. Now train a K-nearest neighbor model using the ‘kknn’ engine and be sure to set the mode to be a classification model. 3.5 Making predictions We have fit a few different models. Now, if we want to see our predictions we can simply apply predict() and feed it the data set we want to make predictions on. Here, we can see the predictions made on our training data for our ordinary least square linear regression model. lm_ols %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 217657. ## 2 214276. ## 3 223425. ## 4 260324. ## 5 109338. ## 6 195106. ## 7 222217. ## 8 126175. ## 9 98550. ## 10 120811. ## # … with 2,041 more rows ## # ℹ Use `print(n = ...)` to see more rows And here we get the predicted values for our KNN model. knn %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 194967. ## 2 192240 ## 3 174220 ## 4 269760 ## 5 113617. ## 6 173672 ## 7 174820 ## 8 120796 ## 9 114560 ## 10 121346 ## # … with 2,041 more rows ## # ℹ Use `print(n = ...)` to see more rows 3.5.1 Knowledge check Make predictions on the test data using the logistic regression model you built on the attrition data. Now make predictions using the K-nearest neighbor model. 3.6 Evaluating model performance It is important to understand how our model is performing. With ML models, measuring performance means understanding the predictive accuracy – the difference between a predicted value and the actual value. We measure predictive accuracy with loss functions. There are many loss functions to choose from when assessing the performance of a predictive model, each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the “optimal model”. Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric. 3.6.1 Regression models The most common loss functions for regression models include: MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\))2. The squared component results in larger errors having larger penalties. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Let’s compute the RMSE of our OLS regression model. Remember, we want to assess our model’s performance on the test data not the training data since that gives us a better idea of how our model generalizes. To do so, the following: Makes predictions with our test data, Adds the actual Sale_Price values from our test data, Computes the RMSE. lm_ols %&gt;% predict(test) %&gt;% bind_cols(test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 45445. The RMSE value suggests that, on average, our model mispredicts the expected sale price of a home by about $45K. 3.6.2 Classification models There are many loss functions used for classification models. For simplicity we’ll just focus on the overall classification accuracy. I’ll illustrate with the attrition data. Here, we build a logistic regression model that seeks to predict Attrition based on all available features. In R, using a “.” as in Attrition ~ . is a shortcut for saying use all available features to predict Attrition. We then follow the same process as above to make predictions on the test data, add the actual test values for Attrition, and then compute the accuracy rate. logit &lt;- logistic_reg() %&gt;% fit(Attrition ~ ., data = train_strat) logit %&gt;% predict(test_strat) %&gt;% bind_cols(test_strat %&gt;% select(Attrition)) %&gt;% accuracy(truth = Attrition, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.876 3.6.3 Knowledge check Compute the accuracy rate of your logistic regression model for the attrition data. Now compute the accuracy rate of your K-nearest neighbor model. 3.7 Exercises For this exercise we’ll use the Boston housing data set. The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978), it contains 13 attributes to predict the median property value. Data attributes: problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes Modeling tasks: Import the Boston housing data set (boston.csv) and split it into a training set and test set using a 70-30% split. How many observations are in the training set and test set? Compare the distribution of cmedv between the training set and test set. Fit a linear regression model using all available features to predict cmedv and compute the RMSE on the test data. Fit a K-nearest neighbor model that uses all available features to predict cmedv and compute the RMSE on the test data. How do these models compare? References "],["overview-1.html", "4 Overview 4.1 Learning objectives 4.2 Estimated time requirement 4.3 Tasks", " 4 Overview The last couple of lessons gave you a good introduction to building predictive models using the Tidymodels construct. However, before we even try to improve upon performance by changing the algorithm, we can likely improve model performance through two three steps: Making our predictive variables more relevant to modeling with feature engineering. Improving the generalization of our model with resampling procedures. Balancing the bias and variance in our models with hyperparameter tuning. This module introduces these concepts and illustrates how to implement these procedures with Tidymodels. 4.1 Learning objectives By the end of this module you should be able to: Apply feature engineering steps to preprocess numeric and categorical data. Perform resampling and hyperparameter grid search procedures for robust model evaluation and selection. 4.2 Estimated time requirement The estimated time to go through the module lessons is about 3 hours. 4.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-2a-feature-engineering.html", "5 Lesson 2a: Feature engineering 5.1 Learning objectives 5.2 Prerequisites 5.3 Create a recipe 5.4 Feature filtering 5.5 Numeric features 5.6 Categorical features 5.7 Fit a model with a recipe 5.8 Exercises", " 5 Lesson 2a: Feature engineering Data preprocessing and engineering techniques generally refer to the addition, deletion, or transformation of data. In this lesson we introduce you to another tidymodels package, recipes, which is designed to help you preprocess your data before training your model. Recipes are built as a series of preprocessing steps, such as: converting qualitative predictors to indicator variables (also known as dummy variables), transforming data to be on a different scale (e.g., taking the logarithm of a variable), transforming whole groups of predictors together, extracting key features from raw variables (e.g., getting the day of the week out of a date variable), and so on. Although there is an ever-growing number of ways to preprocess your data, we’ll focus on a few common feature engineering steps applied to numeric and categorical data. This will provide you with a foundation of how to perform feature engineering. 5.1 Learning objectives By the end of this lesson you’ll be able to: Explain how to apply feature engineering steps with the recipes package. Filter out low informative features. Normalize and standardize numeric features. Pre-process nominal and ordinal features. Combine multiple feature engineering steps into one recipe and train a model with it. 5.2 Prerequisites For this lesson we’ll use the recipes package with is automatically loaded with tidymodels and we’ll use the ames housing data. library(tidymodels) library(tidyverse) ames_data_path &lt;- here::here(&quot;data&quot;, &quot;ames.csv&quot;) ames &lt;- readr::read_csv(ames_data_path) Let’s go ahead and create our train-test split: # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) train &lt;- training(split) test &lt;- testing(split) 5.3 Create a recipe To get started, let’s create a model recipe that we will build upon and apply in a model downstream. Before training the model, we can use a recipe to create and/or modify predictors and conduct some preprocessing required by the model. ames_recipe &lt;- recipe(Sale_Price ~ ., data = train) Now we can start adding steps onto our recipe using the pipe operator and applying specific feature engineering tasks. The sections that follow provide some discussion as to why we apply each feature engineering step and then we demonstrate how to add it to our recipe. 5.4 Feature filtering In many data analyses and modeling projects we end up with hundreds or even thousands of collected features. From a practical perspective, a model with more features often becomes harder to interpret and is costly to compute. Some models are more resistant to non-informative predictors (e.g., the Lasso and tree-based methods) than others. Although the performance of some of our models are not significantly affected by non-informative predictors, the time to train these models can be negatively impacted as more features are added. Consequently, filtering or reducing features prior to modeling may significantly speed up training time. Zero and near-zero variance variables are low-hanging fruit to eliminate. Zero variance variables, meaning the feature only contains a single unique value, provides no useful information to a model. Some algorithms are unaffected by zero variance features. However, features that have near-zero variance also offer very little, if any, information to a model. Furthermore, they can cause problems during resampling as there is a high probability that a given sample will only contain a single unique value (the dominant value) for that feature. A rule of thumb for detecting near-zero variance features are identifying and removing features with \\(\\leq 5-10\\)% variance. For the Ames data, we do not have any zero variance predictors but there are many features that meet the near-zero threshold. The following shows all features where there is less than 5% variance in distinct values. rowname freqRatio percentUnique zeroVar nzv Street 292.00000 0.0975134 FALSE TRUE Alley 23.60494 0.1462701 FALSE TRUE Utilities 2049.00000 0.1462701 FALSE TRUE Land_Slope 22.18182 0.1462701 FALSE TRUE Land_Contour 21.45349 0.1950268 FALSE TRUE Kitchen_AbvGr 21.25000 0.1950268 FALSE TRUE Pool_QC 680.66667 0.2437835 FALSE TRUE Roof_Matl 144.64286 0.2925402 FALSE TRUE Bsmt_Cond 20.76136 0.2925402 FALSE TRUE Heating 100.90000 0.2925402 FALSE TRUE Misc_Feature 27.40278 0.2925402 FALSE TRUE BsmtFin_Type_2 25.77941 0.3412969 FALSE TRUE Condition_2 202.70000 0.3900536 FALSE TRUE Functional 39.72917 0.3900536 FALSE TRUE Pool_Area 2042.00000 0.4875670 FALSE TRUE Three_season_porch 675.33333 0.9751341 FALSE TRUE Low_Qual_Fin_SF 504.50000 1.4139444 FALSE TRUE Misc_Val 141.00000 1.5114578 FALSE TRUE We can filter out near-zero variance features with step_nzv(). Within step_nzv() we specify which features to apply this to. There are several helper functions to make it easy to apply the step to all_predictors(), all_numeric() variables, all_nominal() variables, and more. See the different variable selectors here. In this example we filter out all predictor variables that have less than 10% variance in values. nzv &lt;- ames_recipe %&gt;% step_nzv(all_predictors(), unique_cut = 10) # threshold as % (10%) The result of the above code is not to actually apply the feature engineering steps but, rather, to create an object that holds the feature engineering logic (or recipe) to be applied later on. 5.5 Numeric features Numeric features can create a host of problems for certain models when their distributions are skewed, contain outliers, or have a wide range in magnitudes. Tree-based models are quite immune to these types of problems in the feature space, but many other models (e.g., GLMs, regularized regression, KNN, support vector machines, neural networks) can be greatly hampered by these issues. Normalizing and standardizing heavily skewed features can help minimize these concerns. 5.5.1 Skewness Parametric models that have distributional assumptions (e.g., GLMs, and regularized models) can benefit from minimizing the skewness of numeric features. When normalizing many variables, it’s best to use the Box-Cox (when feature values are strictly positive) or Yeo-Johnson (when feature values are not strictly positive) procedures as these methods will identify if a transformation is required and what the optimal transformation will be. Non-parametric models are rarely affected by skewed features; however, normalizing features will not have a negative effect on these models’ performance. For example, normalizing features will only shift the optimal split points in tree-based algorithms. Consequently, when in doubt, normalize. We can normalize our numeric predictor variables with step_YeoJohnson(): # Normalize all numeric features X_norm &lt;- ames_recipe %&gt;% step_YeoJohnson(all_predictors(), all_numeric()) 5.5.2 Standardization We must also consider the scale on which the individual features are measured. What are the largest and smallest values across all features and do they span several orders of magnitude? Models that incorporate smooth functions of input features are sensitive to the scale of the inputs. For example, \\(5X+2\\) is a simple linear function of the input X, and the scale of its output depends directly on the scale of the input. Many algorithms use linear functions within their algorithms, some more obvious (e.g., GLMs and regularized regression) than others (e.g., neural networks, support vector machines, and principal components analysis). Other examples include algorithms that use distance measures such as the Euclidean distance (e.g., k nearest neighbor, k-means clustering, and hierarchical clustering). For these models and modeling components, it is often a good idea to standardize the features. Standardizing features includes centering and scaling so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Figure 5.1: Standardizing features allows all features to be compared on a common value scale regardless of their real value differences. We can standardize our numeric features in one of two ways – note that step_normalize() is just a wrapper that combines step_center() and step_scale(). # option 1 std &lt;- ames_recipe %&gt;% step_center(predictors(), all_numeric()) %&gt;% step_scale(predictors(), all_numeric()) # option 2 std &lt;- ames_recipe %&gt;% step_normalize(predictors(), all_numeric()) 5.6 Categorical features Most models require that the predictors take numeric form. There are exceptions; for example, tree-based models naturally handle numeric or categorical features. However, even tree-based models can benefit from pre-processing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features. 5.6.1 One-hot &amp; dummy encoding There are many ways to recode categorical variables as numeric. The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding the left data frame in the below figure results in X being converted into three columns, one for each level. This is called less than full rank encoding . However, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level c has been dropped). This is referred to as dummy encoding. Figure 5.2: Eight observations containing a categorical feature X and the difference in how one-hot and dummy encoding transforms this feature. We can use step_dummy() to add a one-hot or dummy encoding to our recipe: # one-hot encode ohe &lt;- ames_recipe %&gt;% step_dummy(all_nominal(), one_hot = TRUE) # dummy encode de &lt;- ames_recipe %&gt;% step_dummy(all_nominal(), one_hot = FALSE) 5.6.2 Ordinal encoding If a categorical feature is naturally ordered then numerically encoding the feature based on its order is a natural choice (most commonly referred to as ordinal encoding). For example, the various quality features in the Ames housing data are ordinal in nature (ranging from Very_Poor to Very_Excellent). train %&gt;% select(matches(&#39;Qual$|QC$|_Cond$&#39;)) ## # A tibble: 2,051 × 11 ## Overall_Q…¹ Overa…² Exter…³ Exter…⁴ Bsmt_…⁵ Bsmt_…⁶ Heati…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Very_Good Average Good Typical Good Good Excell… ## 2 Above_Aver… Average Good Typical Good Typical Excell… ## 3 Below_Aver… Average Typical Typical Good Typical Good ## 4 Very_Good Average Good Typical Good Typical Excell… ## 5 Above_Aver… Good Typical Typical Typical Good Typical ## 6 Good Average Good Typical Good Typical Excell… ## 7 Above_Aver… Average Typical Typical Good Typical Excell… ## 8 Average Average Typical Typical Typical Typical Typical ## 9 Average Good Typical Typical Fair Typical Typical ## 10 Below_Aver… Average Typical Typical Typical Typical Typical ## # … with 2,041 more rows, 4 more variables: ## # Kitchen_Qual &lt;chr&gt;, Garage_Qual &lt;chr&gt;, Garage_Cond &lt;chr&gt;, ## # Pool_QC &lt;chr&gt;, and abbreviated variable names ## # ¹​Overall_Qual, ²​Overall_Cond, ³​Exter_Qual, ⁴​Exter_Cond, ## # ⁵​Bsmt_Qual, ⁶​Bsmt_Cond, ⁷​Heating_QC ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Ordinal encoding these features provides a natural and intuitive interpretation and can logically be applied to all models. If your features are already ordered factors then you can simply apply step_ordinalscore() to ordinal encode: ord &lt;- ames_recipe %&gt;% step_ordinalscore(matches(&#39;Qual$|QC$|_Cond$&#39;)) However, if we look at our quality features we see they are characters instead of factors and their levels are not ordered. Moreover, some have a unique value that represents that feature doesn’t exist in the house (i.e. No_Basement). train %&gt;% pull(Bsmt_Qual) %&gt;% unique() ## [1] &quot;Good&quot; &quot;Typical&quot; &quot;Fair&quot; &quot;Excellent&quot; ## [5] &quot;No_Basement&quot; &quot;Poor&quot; So in this case we’re going to apply several feature engineering steps to: convert quality features to factors with specified levels, convert any missed levels (i.e. No_Basement, No_Pool) to “None”, convert factor level to integer value (‘Very_Poor’ = 1, ‘Poor’ = 2, …, ‘Excellent’ = 10, ‘Very_Excellent’ = 11) # specify levels in order lvls &lt;- c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Typical&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;) # apply ordinal encoding to quality features ord_lbl &lt;- ames_recipe %&gt;% # 1. convert quality features to factors with specified levels step_string2factor(matches(&#39;Qual$|QC$|_Cond$&#39;), levels = lvls, ordered = TRUE) %&gt;% # 2. convert any missed levels (i.e. No_Basement, No_Pool) to &quot;None&quot; step_unknown(matches(&#39;Qual$|QC$|_Cond$&#39;), new_level = &quot;None&quot;) %&gt;% # 3. convert factor level to integer value step_ordinalscore(matches(&#39;Qual$|QC$|_Cond$&#39;)) Did this work, let’s take a look. If we want to apply a recipe to a data set we can apply: prep: estimate feature engineering parameters based on training data. bake: apply the prepped recipe to new, or the existing (new_data = NULL) data. We can see that our quality variables are now ordinal encoded. baked_recipe &lt;- ord_lbl %&gt;% prep(strings_as_factor = FALSE) %&gt;% bake(new_data = NULL) baked_recipe %&gt;% select(matches(&#39;Qual$|QC$|_Cond$&#39;)) ## # A tibble: 2,051 × 11 ## Overall_Q…¹ Overa…² Exter…³ Exter…⁴ Bsmt_…⁵ Bsmt_…⁶ Heati…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 5 8 6 8 8 10 ## 2 7 5 8 6 8 6 10 ## 3 4 5 6 6 8 6 8 ## 4 9 5 8 6 8 6 10 ## 5 7 8 6 6 6 8 6 ## 6 8 5 8 6 8 6 10 ## 7 7 5 6 6 8 6 10 ## 8 5 5 6 6 6 6 6 ## 9 5 8 6 6 3 6 6 ## 10 4 5 6 6 6 6 6 ## # … with 2,041 more rows, 4 more variables: ## # Kitchen_Qual &lt;dbl&gt;, Garage_Qual &lt;dbl&gt;, Garage_Cond &lt;dbl&gt;, ## # Pool_QC &lt;dbl&gt;, and abbreviated variable names ## # ¹​Overall_Qual, ²​Overall_Cond, ³​Exter_Qual, ⁴​Exter_Cond, ## # ⁵​Bsmt_Qual, ⁶​Bsmt_Cond, ⁷​Heating_QC ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names And if we want to see how the numeric values are mapped to the original data we can. Here, I just focus on the original Overall_Qual values and how they compare to the encoded values. encoded_Overall_Qual &lt;- baked_recipe %&gt;% select(Overall_Qual) %&gt;% rename(encoded_Overall_Qual = Overall_Qual) train %&gt;% select(Overall_Qual) %&gt;% bind_cols(encoded_Overall_Qual) %&gt;% count(Overall_Qual, encoded_Overall_Qual) ## # A tibble: 10 × 3 ## Overall_Qual encoded_Overall_Qual n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Above_Average 7 509 ## 2 Average 5 587 ## 3 Below_Average 4 168 ## 4 Excellent 10 65 ## 5 Fair 3 30 ## 6 Good 8 419 ## 7 Poor 2 9 ## 8 Very_Excellent 11 26 ## 9 Very_Good 9 235 ## 10 Very_Poor 1 3 5.6.3 Lumping Sometimes features will contain levels that have very few observations. For example, there are 28 unique neighborhoods represented in the Ames housing data but several of them only have a few observations. ## # A tibble: 28 × 2 ## Neighborhood n ## &lt;chr&gt; &lt;int&gt; ## 1 Green_Hills 1 ## 2 Landmark 1 ## 3 Greens 6 ## 4 Blueste 8 ## 5 Northpark_Villa 16 ## 6 Veenker 18 ## 7 Briardale 20 ## 8 Bloomington_Heights 22 ## 9 Meadow_Village 26 ## 10 Clear_Creek 29 ## # … with 18 more rows ## # ℹ Use `print(n = ...)` to see more rows Sometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 1% of the training sample into an “other” category. We can use step_other() to do so. However, lumping should be used sparingly as there is often a loss in model performance (Kuhn and Johnson 2013). Tree-based models often perform exceptionally well with high cardinality features and are not as impacted by levels with small representation. The following lumps all neighborhoods that represent less than 1% of observations into an “other” category. # Lump levels for two features rare_encoder &lt;- ames_recipe %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) 5.7 Fit a model with a recipe Let’s combine these feature engineering tasks into one recipe and then train a model with it. final_recipe &lt;- recipe(Sale_Price ~ ., data = train) %&gt;% step_nzv(all_predictors(), unique_cut = 10) %&gt;% step_YeoJohnson(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_string2factor(matches(&#39;Qual$|QC$|_Cond$&#39;), levels = lvls, ordered = TRUE) %&gt;% step_unknown(matches(&#39;Qual$|QC$|_Cond$&#39;), new_level = &quot;None&quot;) %&gt;% step_integer(matches(&#39;Qual$|QC$|_Cond$&#39;)) %&gt;% step_other(all_nominal_predictors(), threshold = 0.01, other = &quot;other&quot;) Let’s go ahead and build a random forest model using the ranger engine (which is the default). rf_mod &lt;- rand_forest() %&gt;% set_mode(&#39;regression&#39;) We will want to use our recipe across several steps as we train and test our model. We will: Process the recipe using the training set: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors will have zero-variance in the training set, and should be slated for removal, what the mean and scale is in order to standardize the numeric features, etc. Apply the recipe to the training set: We create the final predictor set on the training set. Apply the recipe to the test set: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the feature engineering statistics computed from the training set are applied to the test set. To simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the workflows package from tidymodels to bundle our parsnip model (rf_mod) with our recipe (ames_recipe). rf_wflow &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_recipe(final_recipe) rf_wflow ## ══ Workflow ══════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ────────────────────────────────────────────── ## 7 Recipe Steps ## ## • step_nzv() ## • step_YeoJohnson() ## • step_normalize() ## • step_string2factor() ## • step_unknown() ## • step_integer() ## • step_other() ## ## ── Model ───────────────────────────────────────────────────── ## Random Forest Model Specification (regression) ## ## Computational engine: ranger Now, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors: rf_fit &lt;- rf_wflow %&gt;% fit(data = train) The rf_fit object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions extract_fit_parsnip() and extract_recipe(). rf_fit %&gt;% extract_fit_parsnip() ## parsnip model object ## ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 2051 ## Number of independent variables: 59 ## Mtry: 7 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 711286135 ## R squared (OOB): 0.8895076 Just as in the last lesson, we can make predictions with our fit model object and evaluate the model performance. Here, we compute the RMSE on our test data and we see we have a much better performance than previous models. rf_fit %&gt;% predict(test) %&gt;% bind_cols(test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 26546. This improved performance could be a result of using a model that fits the data better, using more features, or from the feature engineering. Or, most likely, a combination of all the above! 5.8 Exercises Using the ames housing data… Rather than ordinal encode the quality and condition features, one-hot encode these features. What is the difference in the number of features in your training set? Apply this new recipe to the same random forest model and compute the test RMSE. How does the performance differ? Identify three new feature engineering steps that are provided by recipes: Why would these feature engineering steps be applicable to the Ames data? Apply these feature engineering steps along with the same random forest model. How do your results change? References "],["lesson-2b-model-evaluation-selection.html", "6 Lesson 2b: Model evaluation &amp; selection 6.1 Learning objectives 6.2 Prerequisites 6.3 Resampling &amp; cross-validation 6.4 K-fold cross-validation 6.5 Hyperparameter tuning 6.6 Finalizing our model 6.7 Exercises 6.8 Additional resources", " 6 Lesson 2b: Model evaluation &amp; selection The last couple of lessons gave you a good introduction to building predictive models using the tidymodels construct. This lesson is going to go deeper into the idea of model evaluation &amp; selection. We’ll discuss how to incorporate cross-validation procedures to give you a more robust assessment of model performance. We’ll also discuss the concept of hyperparameter tuning, the bias-variance tradeoff, and how to implement a tuning strategy to find a model the maximizes generalizability. 6.1 Learning objectives By the end of this lesson you will be able to: Perform cross-validation procedures for more robust model performance assessment. Execute hyperparameter tuning to find optimal model parameter settings. 6.2 Prerequisites For this lesson we’ll use several packages provided via tidymodels and we’ll use the ames housing data. However, for this module we’ll use the Ames housing data provided by the AmesHousing package. Take a minute to check out the Ames data from AmesHousing::make_ames(). It is very similar to the Ames data provided in the CSV. Note that the various quality/condition variables (i.e. Overall_Qual) are already encoded as ordinal factors. library(tidymodels) ames &lt;- AmesHousing::make_ames() Let’s go ahead and create our train-test split: # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) train &lt;- training(split) test &lt;- testing(split) 6.3 Resampling &amp; cross-validation In the previous lessons we split our data into training and testing sets and we assessed the performance of our model on the test set. Unfortunately, there are a few pitfalls to this approach: If our dataset is small, a single test set may not provide realistic expectations of our model’s performance on unseen data. A single test set does not provide us any insight on variability of our model’s performance. Using our test set to drive our model building process can bias our results via data leakage. Resampling methods provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts of the training data. Figure 6.1: Illustration of resampling. This allows us to train and validate our model entirely on the training data and not touch the test data until we have selected a final “optimal” model. The two most commonly used resampling methods include k-fold cross-validation and bootstrap sampling. This lesson focuses on using k-fold cross-validation. 6.4 K-fold cross-validation Cross-validation consists of repeating the procedure such that the training and testing sets are different each time. Generalization performance metrics are collected for each repetition and then aggregated. As a result we can get an estimate of the variability of the model’s generalization performance. k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. Figure 6.2: Illustration of k-fold sampling across a data sets index. The model is fit on \\(k-1\\) folds and then the remaining fold is used to compute model performance. This procedure is repeated k times; each time, a different fold is treated as the validation set. Consequently, with k-fold CV, every observation in the training data will be held out one time to be included in the assessment/validation set. This process results in k estimates of the generalization error (say \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging the k test errors, providing us with an approximation of the error we might expect on unseen data. Figure 6.3: Illustration of a 5-fold cross validation procedure. In practice, one typically uses k=5 or k=10. There is no formal rule as to the size of k; however, as k gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease. To implement k-fold CV we first make a resampling object. In this example we create a 10-fold resampling object. kfolds &lt;- vfold_cv(train, v = 10) We can now create our random forest model object and create a workflow object as we did in the previous lesson. To fit our model across our 10-folds we just use fit_resamples(). # create our random forest model object rf_mod &lt;- rand_forest() %&gt;% set_mode(&#39;regression&#39;) # add model object and our formula spec to a workflow object rf_wflow &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_formula(Sale_Price ~ .) # fit our model across the 10-fold CV rf_fit_cv &lt;- rf_wflow %&gt;% fit_resamples(kfolds) We can then get our average 10-fold cross validation error with collect_metrics(): collect_metrics(rf_fit_cv) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 25978. 10 898. Preprocessor1_… ## 2 rsq standard 0.902 10 0.00787 Preprocessor1_… If we want to see the model evaluation metric (i.e. RMSE) for each fold we just need to unnest the rf_fit_cv object. We have not discussed nested data frames but you can read about them here rf_fit_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 10 × 7 ## splits id .metric .estima…¹ .esti…² .config ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &lt;split [1845/206]&gt; Fold01 rmse standard 26210. Prepro… ## 2 &lt;split [1846/205]&gt; Fold02 rmse standard 22089. Prepro… ## 3 &lt;split [1846/205]&gt; Fold03 rmse standard 30512. Prepro… ## 4 &lt;split [1846/205]&gt; Fold04 rmse standard 26106. Prepro… ## 5 &lt;split [1846/205]&gt; Fold05 rmse standard 25095. Prepro… ## 6 &lt;split [1846/205]&gt; Fold06 rmse standard 24337. Prepro… ## 7 &lt;split [1846/205]&gt; Fold07 rmse standard 23586. Prepro… ## 8 &lt;split [1846/205]&gt; Fold08 rmse standard 30650. Prepro… ## 9 &lt;split [1846/205]&gt; Fold09 rmse standard 27256. Prepro… ## 10 &lt;split [1846/205]&gt; Fold10 rmse standard 23935. Prepro… ## # … with 1 more variable: .notes &lt;list&gt;, and abbreviated ## # variable names ¹​.estimator, ²​.estimate ## # ℹ Use `colnames()` to see all variable names 6.5 Hyperparameter tuning Say you have the below relationship between a response variable and some predictor variable x (gray dots). Given two different models fit to this relationship (blue line), which model do you prefer? Figure 6.4: Between model A and B, which do you think is better? The image above illustrates the fact that prediction errors can be decomposed into two main subcomponents we care about: error due to “bias” error due to “variance” Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models. 6.5.1 Bias Error due to bias is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model’s predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data. The left image below illustrates an example where a polynomial model does not capture the underlying relationship well. Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships. We also need to think of bias-variance in relation to resampling. Models with high bias are rarely affected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated by the right plot below. Figure 6.5: A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left). Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right). 6.5.2 Variance Error due to variance is the variability of a model prediction for a given data point. Many models (e.g., k-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data. Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data. Figure 6.6: A high variance k-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left). Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right). Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. Moreover, many algorithms that are capable of achieving high generalization performance have lots of hyperparameters that control the level of model complexity (i.e., the tradeoff between bias and variance). Many high performing models (i.e. random forests, gradient boosting machines, deep learning) are very flexible in the patterns they can conform to due to the many hyperparameters they have. However, this also means they are prone to overfitting (aka can have high variance error). 6.5.3 Hyperparameters Hyperparameters (aka tuning parameters) are the “knobs to twiddle” to control the complexity of machine learning algorithms and, therefore, the bias-variance trade-off. Not all algorithms have hyperparameters (e.g., ordinary least squares8); however, most have at least one or more. Some models have very few hyperparameters. For example, k-nearest neighbor models have a single hyperparameter (k) that determines the predicted value to be made based on the k nearest observations in the training data to the one being predicted. If k is small (e.g., \\(k = 3\\)), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted. This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data. As k gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging often helps to reduce variance!). The plot below illustrates this point. Smaller k values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal k value might exist somewhere between 20–50, but how do we know which value of k to use? Figure 6.7: k-nearest neighbor model with differing values for k. Some algorithms such as ordinary least squares don’t have any hyperparameters. Some algorithms such as k-nearest neighbor have one or two hyperparameters. And some algorithms such as gradient boosted machines (GBMs) and deep learning models have many. Hyperparameter tuning is the process of screening hyperparameter values (or combinations of hyperparameter values) to find a model that balances bias &amp; variance so that the model generalizes well to unseen data. Let’s illustrate by using decision trees. One of the key hyperparameters in decision trees is the depth of the tree. This lesson does not dig into the decision tree algorithm but if you want to better understand the hyperparameters for decision trees you can read about them here Say we wanted to assess what happens when we grow the decision tree 5 levels deep. We could do this manually: # create our decision tree model object dt_mod &lt;- decision_tree(tree_depth = 5) %&gt;% set_mode(&#39;regression&#39;) # add model object and our formula spec to a workflow object dt_wflow &lt;- workflow() %&gt;% add_model(dt_mod) %&gt;% add_formula(Sale_Price ~ .) # fit our model across the 10-fold CV dt_fit_cv &lt;- dt_wflow %&gt;% fit_resamples(kfolds) # assess results collect_metrics(dt_fit_cv) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 39175. 10 1382. Preprocessor1_… ## 2 rsq standard 0.758 10 0.0156 Preprocessor1_… But what if we wanted to assess and compare different tree_depth values. Moreover, decision trees have another key hyperparameter cost_complexity. So what if we wanted to assess a few values of that hyperaparameter in combination with tree_depth? Adjusting these values manually would be painstakingly burdensome. Again, don’t worry if you have no idea what these hyperparameters mean. Just realize we want to toggle these values to try find an optimal model. 6.5.4 Full cartesian grid search For this we could use a full cartesian grid search. A full cartesian grid search takes the values provided for each hyperparameter and assesses every combination. First, let’s rebuild our decision tree model object; however, this time we’ll create a model specification that identifies which hyperparameters we plan to tune. dt_mod &lt;- decision_tree( cost_complexity = tune(), # &lt;-- these are hyperparameters we want to tune tree_depth = tune() # &lt;-- these are hyperparameters we want to tune ) %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;regression&quot;) Think of tune() here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will tune(). Next, we create our tuning grid of hyperparameter values we want to assess. The function grid_regular() is from the dials package. It chooses sensible values to try for each hyperparameter; here, we asked for 3 of each. Since we have two to tune, grid_regular() returns \\(3 \\times 3 = 9\\) different possible tuning combinations to try. A full cartesian grid search can explode as you add more hyperparameters and values to assess. When this happens its best to start using grid_random or grid_latin_hypercube to reduce combinatorial explosion and computation time. dt_grid &lt;- grid_regular( cost_complexity(), tree_depth(), levels = 3 ) dt_grid ## # A tibble: 9 × 2 ## cost_complexity tree_depth ## &lt;dbl&gt; &lt;int&gt; ## 1 0.0000000001 1 ## 2 0.00000316 1 ## 3 0.1 1 ## 4 0.0000000001 8 ## 5 0.00000316 8 ## 6 0.1 8 ## 7 0.0000000001 15 ## 8 0.00000316 15 ## 9 0.1 15 Now that we have our tuning grid and model object defined we can: Create our k-fold CV object (5-fold in this example), Add our model to a workflow object and specify the formula, Apply tune_grid() to execute our hyperparameter tuning. # 5-fold instead of 10-fold to reduce computation time kfolds &lt;- vfold_cv(train, v = 5) # add model object and our formula spec to a workflow object dt_wflow &lt;- workflow() %&gt;% add_model(dt_mod) %&gt;% add_formula(Sale_Price ~ .) # fit our model across the 5-fold CV dt_grid_search &lt;- dt_wflow %&gt;% tune_grid( resamples = kfolds, grid = dt_grid ) We can check out the hyperparameter combinations that resulted in the best model performance with show_best(). Here we look at the top 5 models and we can see that the top 4 all tend to perform very similarly. It appears that deeper trees with smaller cost complexity factor tend to perform best on this data set. dt_grid_search %&gt;% show_best(metric = &#39;rmse&#39;) ## # A tibble: 5 × 8 ## cost_complexity tree_…¹ .metric .esti…² mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 8 rmse standa… 36638. 5 1515. ## 2 0.00000316 8 rmse standa… 36638. 5 1515. ## 3 0.0000000001 15 rmse standa… 36703. 5 1587. ## 4 0.00000316 15 rmse standa… 36703. 5 1587. ## 5 0.1 8 rmse standa… 51944. 5 1834. ## # … with 1 more variable: .config &lt;chr&gt;, and abbreviated ## # variable names ¹​tree_depth, ²​.estimator ## # ℹ Use `colnames()` to see all variable names 6.6 Finalizing our model If we are satisfied with our results and we want to use the best hyperparameter values for our best decision tree model, we can select it with select_best(): # select best model based on RMSE metric best_tree &lt;- dt_grid_search %&gt;% select_best(metric = &#39;rmse&#39;) best_tree ## # A tibble: 1 × 3 ## cost_complexity tree_depth .config ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.0000000001 8 Preprocessor1_Model4 We can then update (or “finalize”) our workflow object dt_wflow with the values from select_best(). final_wflow &lt;- dt_wflow %&gt;% finalize_workflow(best_tree) Finally, let’s fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data. We pass the initial train-test split object we created at the beginning of this lesson to last_fit. final_fit &lt;- final_wflow %&gt;% last_fit(split) final_fit %&gt;% collect_predictions() %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 34609. As we can see our test RMSE is less than our CV RMSE. This indicates that we did not overfit during our tuning procedure, which is a good thing. Perhaps we would also like to understand what variables are important in this final model. We can use the vip package to estimate variable importance based on the model’s structure. Don’t worry, we’ll talk about vip more later on and just how this measure of importance is computed. library(vip) final_fit %&gt;% extract_fit_parsnip() %&gt;% vip() 6.7 Exercises Import the dataset blood_transfusion.csv: The column “Class” contains the target variable. Investigate this variable. Is this a regression or classification problem? Why is it relevant to add a preprocessing step to standardize the features? What step_xxx() function would you use to do so? Perform a k-nearest neighbor model on this data with neighbors = 10. Be sure to add a preprocessing step to standardize the features. Perform a 5-fold cross validation with the above model workflow. What is your average CV score? Now perform hyperparameter tuning to understand the effect of the parameter neighbors on the model score. Assess 10 values for neighbors between the range of 1-100. Again, perform a 5-fold cross validation. Which hyperparameter value performed the best and what was the CV score? 6.8 Additional resources This module provided a very high-level introduction to predictive modeling with tidymodels. To build on this knowledge you can find many great resources at https://www.tidymodels.org/. "],["computing-environment.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ───────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.4 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-08-18 ## rstudio 2022.07.1+554 Spotted Wakerobin (desktop) ## pandoc 2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown) ## ## ─ Packages ───────────────────────────────────────────────── ## package * version date (UTC) lib source ## broom * 1.0.0 2022-07-01 [1] CRAN (R 4.2.0) ## caret * 6.0-92 2022-04-19 [1] CRAN (R 4.2.0) ## DiagrammeR * 1.0.9 2022-03-05 [1] CRAN (R 4.2.0) ## dials * 1.0.0 2022-06-14 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## infer * 1.0.2 2022-06-10 [1] CRAN (R 4.2.0) ## kableExtra * 1.3.4 2021-02-20 [1] CRAN (R 4.2.0) ## lattice * 0.20-45 2021-09-22 [1] CRAN (R 4.2.0) ## modeldata * 0.1.1 2021-07-14 [1] CRAN (R 4.2.0) ## parsnip * 1.0.0 2022-06-16 [1] CRAN (R 4.2.0) ## plotly * 4.10.0 2021-10-09 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## ranger * 0.14.1 2022-06-18 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## recipes * 0.2.0 2022-02-18 [1] CRAN (R 4.2.0) ## rpart * 4.1.16 2022-01-24 [1] CRAN (R 4.2.0) ## rsample * 0.1.1 2021-11-08 [1] CRAN (R 4.2.0) ## scales * 1.2.0 2022-04-13 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.8 2022-07-22 [1] CRAN (R 4.2.0) ## tidymodels * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.2 2022-07-18 [1] CRAN (R 4.2.0) ## tune * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## vip * 0.3.2 2020-12-17 [1] CRAN (R 4.2.0) ## workflows * 0.2.6 2022-03-18 [1] CRAN (R 4.2.0) ## workflowsets * 0.2.1 2022-03-15 [1] CRAN (R 4.2.0) ## yardstick * 1.0.0 2022-06-06 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ─ Python configuration ───────────────────────────────────── ## python: /Users/b294776/Library/r-miniconda/envs/r-reticulate/bin/python ## libpython: /Users/b294776/Library/r-miniconda/envs/r-reticulate/lib/libpython3.8.dylib ## pythonhome: /Users/b294776/Library/r-miniconda/envs/r-reticulate:/Users/b294776/Library/r-miniconda/envs/r-reticulate ## version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:47) [Clang 12.0.1 ] ## numpy: /Users/b294776/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/numpy ## numpy_version: 1.22.4 ## ## ──────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
