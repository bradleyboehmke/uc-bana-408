[["index.html", "Data Mining with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Mining with R Bradley Boehmke Syllabus This is the primary “textbook” for the Machine Learning section of the UC BANA 4080 Data Mining course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Mining with R! This course provides an intensive, hands-on introduction to data mining and analysis techniques. You will learn the fundamental skills required to extract informative attributes, relationships, and patterns from data sets. You will gain hands-on experience with exploratory data analysis, data visualization, unsupervised learning techniques such as clustering and dimension reduction, and supervised learning techniques such as linear regression, regularized regression, decision trees, random forests, and more! You will also be exposed to some more advanced topics such as ensembling techniques, deep learning, model stacking, and model interpretation. Together, this will provide you with a solid foundation of tools and techniques applied in organizations to aid modern day data-driven decision making. Learning Objectives Upon successfully completing this course, you will be able to: Apply data wrangling techniques to manipulate and prepare data for analysis. Use exploratory data analysis and visualization to provide descriptive insights of data. Apply common unsupervised learning algorithms to find common groupings of observations and features in a given dataset. Describe and apply a sound analytic modeling process. Apply, compare, and contrast various predictive modeling techniques. Have the resources and understanding to continue advancing your data mining and analysis capabilities. …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material This course is split into two main sections - Data Wrangling and Machine Learning. The data wrangling section will provide you the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. The primary course material for this section is provided via this free online book. The second section focused on machine learning section will expose you to several algorithms to identify hidden patterns and relationships within your data. The primary course material for this part of the course is provided via this free online book. There will also be recorded lectures and additional supplementary resources provided via Canvas. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “TODO” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. These labs will be provided via the course website on Canvas and the submission of these labs will also be done through the course website on Canvas. Please check Canvas for due dates for these labs. Projects: There will be two projects designed for you to put to work the tools and knowledge that you gain throughout this course. This provides you with multiple benefits. - It will provide you with more experience using data wrangling tools on real life data sets. - It helps you become a self-directed learner. As a data scientist, a large part of your job is to self-direct your learning and interests to find unique and creative ways to find insights in data. - It starts to build your data science portfolio. Establishing a data science portfolio is a great way to show potential employers your ability to work with data. Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description DATA WRANGLING 1 Introduction R fundamentals &amp; the Rstudio IDE Deeper understanding of vectors 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Data structures &amp; importing data 3 Tidy Data and Data Manipulation Data manipulation &amp; summarization Tidy data 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to text &amp; date-time data 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions 7 Mid-term Project MACHINE LEARNING 8 Introduction to Applied Modeling Introduction to machine learning First model with Tidymodels 9 First Regression Models Simple linear regression Multiple linear regression 10 More Modeling Processes Feature engineering Resampling 11 Classification &amp; Regularization Logistic regression Regularized regression 12 Hyperparameter Tuning &amp; Non-linearity Hyperparameter tuning Multivariate adaptive regression splines 13 Tree-based Models Decision trees Bagging Random forests 14 Unsupervised learning Clustering Dimension reduction 15 Final Project Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A video demonstrating this topic is available in Canvas. A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-4080/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA4080 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R Brandon Greenwell, Hands-on Machine Learning with R "],["overview.html", "1 Overview 1.1 Learning objectives 1.2 Estimated time requirement 1.3 Tasks", " 1 Overview Before introducing specific machine learning (ML) algorithms, it is important that we have a solid understanding of the overall objective of ML algorithms and the common problems they can address. Consequently, this module provides an introduction to ML and starts to introduce parts of the ML modeling process that you’ll routinely see in future modeling lessons. 1.1 Learning objectives By the end of this module you should be able to: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic predictive model. 1.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 3 hours Reading + videos: 4 hours 1.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material). "],["lesson-1a-intro-to-machine-learning.html", "2 Lesson 1a: Intro to machine learning 2.1 Learning objectives 2.2 Supervised learning 2.3 Unsupervised learning 2.4 Machine Learning in 2.5 The data sets 2.6 What You’ll Learn Next 2.7 Exercises", " 2 Lesson 1a: Intro to machine learning Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include: Predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge. Segmenting customers based on common attributes or purchasing behavior for targeted marketing. Predicting coupon redemption rates for a given marketing campaign. Predicting customer churn so an organization can perform preventative intervention. And many more! In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision needed during training. 2.1 Learning objectives This lesson will introduce you to some fundamental concepts around ML and this class. By the end of this lesson you will: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Be able to import and explore the data sets we’ll use through various examples. 2.2 Supervised learning A predictive model is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks; using home attributes to predict the sales price; using employee attributes to predict the likelihood of attrition; using patient attributes and symptoms to predict the risk of readmission; using production attributes to predict time to market. Each of these examples has a defined learning task; they each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this course we’ll use various terms interchangeably for \\(X\\): “predictor variable”, “independent variable”, “attribute”, “feature”, “predictor” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the target values. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Most supervised learning problems can be bucketed into one of two categories, regression or classification, which we discuss next. 2.2.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between $80,000 and $755,000). The figure below illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane. Figure 2.1: Average home sales price as a function of year built and total square footage. 2.2.2 Classification problems When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: Did a customer redeem a coupon (coded as yes/no or 1/0)? Did a customer churn (coded as yes/no or 1/0)? Did a customer click on our online ad (coded as yes/no or 1/0)? Classifying customer reviews: Binary: positive vs. negative. Multinomial: extremely negative to extremely positive on a 0–5 Likert scale. Figure 2.2: Classification problem modeling ‘Yes’/‘No’ response based on three features. However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., “yes” or “no”), we often want to predict the probability of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, many of the supervised learning algorithms we cover in this class can be applied to both. These algorithms have become the most popular machine learning applications in recent years. 2.2.3 Knowledge check Identify the features, response variable, and the type of supervised model required for the following tasks: There is an online retailer that wants to predict whether you will click on a certain featured product given your demographics, the current products in your online basket, and the time since your previous purchase. A bank wants to use a customers historical data such as the number of loans they’ve had, the time it took to payoff those loans, previous loan defaults, the number of new loans within the past two years, along with the customers income and level of education to determine if they should issue a new loan for a car. If the bank above does issue a new loan, they want to use the same information to determine the interest rate of the new loan issued. To better plan incoming and outgoing flights, an airline wants to use flight information such as scheduled flight time, day/month of year, number of passengers, airport departing from, airport arriving to, distance to travel, and weather warnings to determine if a flight will be delayed. What if the above airline wants to use the same information to predict the number of minutes a flight will arrive late or early? 2.3 Unsupervised learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression). Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised! Despite its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of unsupervised learning models can be used as inputs to downstream supervised learning models. 2.3.1 Knowledge check Identify the type of unsupervised model required for the following tasks: Say you have a YouTube channel. You may have a lot of data about the subscribers of your channel. What if you want to use that data to detect groups of similar subscribers? Say you’d like to group Ohio counties together based on the demographics of their residents. A retailer has collected hundreds of attributes about all their customers; however, many of those features are highly correlated. They’d like to reduce the number of features down by combining all those highly correlated features into groups. 2.4 Machine Learning in Historically, the R ecosystem provides a wide variety of ML algorithm implementations. This has its benefits; however, this also has drawbacks as it requires the users to learn many different formula interfaces and syntax nuances. More recently, development on a group of packages called Tidymodels has helped to make implementation easier. The tidymodels collection allows you to perform discrete parts of the ML workflow with discrete packages: rsample for data splitting and resampling recipes for data pre-processing and feature engineering parsnip for applying algorithms tune for hyperparameter tuning yardstick for measuring model performance and several others! Throughout this course you’ll be exposed to several of these packages. Go ahead and make sure you have the following packages installed. Just like the tidyverse package, when you install tidymodels you are actually installing several packages that exist in the tidymodels ecosystem as discussed above. # common data wrangling and visualization install.packages(&quot;tidyverse&quot;) install.packages(&quot;vip&quot;) install.packages(&quot;here&quot;) # modeling install.packages(&quot;tidymodels&quot;) packageVersion(&quot;tidymodels&quot;) ## [1] &#39;1.2.0&#39; library(tidymodels) ## ── Attaching packages ───────────────────────────── tidymodels 1.2.0 ── ## ✔ broom 1.0.6 ✔ rsample 1.2.1 ## ✔ dials 1.2.1 ✔ tibble 3.2.1 ## ✔ dplyr 1.1.4 ✔ tidyr 1.3.1 ## ✔ infer 1.0.7 ✔ tune 1.2.1 ## ✔ modeldata 1.4.0 ✔ workflows 1.1.4 ## ✔ parsnip 1.2.1 ✔ workflowsets 1.1.0 ## ✔ purrr 1.0.2 ✔ yardstick 1.3.1 ## ✔ recipes 1.1.0 ## ── Conflicts ──────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks plotly::filter(), stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ recipes::step() masks stats::step() ## • Dig deeper into tidy modeling with R at https://www.tmwr.org 2.4.1 Knowledge check Check out the Tidymodels website: https://www.tidymodels.org/. Identify which packages can be used for: Efficiently splitting your data Optimizing hyperparameters Measuring the effectiveness of your model Working with correlation matrices 2.5 The data sets The data sets chosen for this course allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this course is to demonstrate how to implement ML workflows, we make the assumption that you have already spent significant time wrangling, cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this course such as: Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process). Recoding variable names and values so that they are meaningful and more interpretable. Tidying data so that each column is a discrete variable and each row is an individual observation. Recoding, removing, or some other approach to handling missing values. Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. As mentioned above, these data sets are fairly common data sets that provide good benchmarks to compare and illustrate ML workflows. Although some of these data sets are available in R, we will import these data sets from a .csv file to ensure consistency over time. 2.5.1 Boston housing The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978) , it contains 13 attributes to predict the median property value. problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes # data file path library(here) data_path &lt;- here(&quot;data&quot;) # access data boston &lt;- readr::read_csv(here(data_path, &quot;boston.csv&quot;)) # initial dimension dim(boston) ## [1] 506 16 # features dplyr::select(boston, -cmedv) ## # A tibble: 506 × 15 ## lon lat crim zn indus chas nox rm age dis rad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.0 42.3 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 ## 2 -71.0 42.3 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 ## 3 -70.9 42.3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 ## 4 -70.9 42.3 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 ## 5 -70.9 42.3 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 ## 6 -70.9 42.3 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 ## 7 -70.9 42.3 0.0883 12.5 7.87 0 0.524 6.01 66.6 5.56 5 ## 8 -70.9 42.3 0.145 12.5 7.87 0 0.524 6.17 96.1 5.95 5 ## 9 -70.9 42.3 0.211 12.5 7.87 0 0.524 5.63 100 6.08 5 ## 10 -70.9 42.3 0.170 12.5 7.87 0 0.524 6.00 85.9 6.59 5 ## # ℹ 496 more rows ## # ℹ 4 more variables: tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, b &lt;dbl&gt;, lstat &lt;dbl&gt; # response variable head(boston$cmedv) ## [1] 24.0 21.6 34.7 33.4 36.2 28.7 2.5.2 Pima Indians Diabetes A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and published in smith1988using , it contains 8 attributes to predict the presence of diabetes. problem type: supervised binary classification response variable: diabetes positive or negative response (i.e. “pos”, “neg”) features: 8 observations: 768 objective: use biological attributes to predict the presence of diabetes # access data pima &lt;- readr::read_csv(here(data_path, &quot;pima.csv&quot;)) # initial dimension dim(pima) ## [1] 768 9 # features dplyr::select(pima, -diabetes) ## # A tibble: 768 × 8 ## pregnant glucose pressure triceps insulin mass pedigree age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 148 72 35 0 33.6 0.627 50 ## 2 1 85 66 29 0 26.6 0.351 31 ## 3 8 183 64 0 0 23.3 0.672 32 ## 4 1 89 66 23 94 28.1 0.167 21 ## 5 0 137 40 35 168 43.1 2.29 33 ## 6 5 116 74 0 0 25.6 0.201 30 ## 7 3 78 50 32 88 31 0.248 26 ## 8 10 115 0 0 0 35.3 0.134 29 ## 9 2 197 70 45 543 30.5 0.158 53 ## 10 8 125 96 0 0 0 0.232 54 ## # ℹ 758 more rows # response variable head(pima$diabetes) ## [1] &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; 2.5.3 Iris flowers The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper (R. A. Fisher 1936) . It is sometimes called Anderson’s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. problem type: supervised multinomial classification response variable: species (i.e. “setosa”, “virginica”, “versicolor”) features: 4 observations: 150 objective: use plant leaf attributes to predict the type of flower # access data iris &lt;- readr::read_csv(here(data_path, &quot;iris.csv&quot;)) # initial dimension dim(iris) ## [1] 150 5 # features dplyr::select(iris, -Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows # response variable head(iris$Species) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; 2.5.4 Ames housing The Ames housing data set is an alternative to the Boston housing data set and provides a more comprehensive set of home features to predict sales price. More information can be found in De Cock (2011) . problem type: supervised regression response variable: Sale_Price (i.e., $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home # access data ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) # initial dimension dim(ames) ## [1] 2930 81 # features dplyr::select(ames, -Sale_Price) ## # A tibble: 2,930 × 80 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_19… Resident… 141 31770 Pave No_A… Slightly… ## 2 One_Story_19… Resident… 80 11622 Pave No_A… Regular ## 3 One_Story_19… Resident… 81 14267 Pave No_A… Slightly… ## 4 One_Story_19… Resident… 93 11160 Pave No_A… Regular ## 5 Two_Story_19… Resident… 74 13830 Pave No_A… Slightly… ## 6 Two_Story_19… Resident… 78 9978 Pave No_A… Slightly… ## 7 One_Story_PU… Resident… 41 4920 Pave No_A… Regular ## 8 One_Story_PU… Resident… 43 5005 Pave No_A… Slightly… ## 9 One_Story_PU… Resident… 39 5389 Pave No_A… Slightly… ## 10 Two_Story_19… Resident… 60 7500 Pave No_A… Regular ## # ℹ 2,920 more rows ## # ℹ 73 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, ## # Lot_Config &lt;chr&gt;, Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Qual &lt;chr&gt;, Overall_Cond &lt;chr&gt;, ## # Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, ## # Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, … # response variable head(ames$Sale_Price) ## [1] 215000 105000 172000 244000 189900 195500 2.5.5 Attrition The employee attrition data set was originally provided by IBM Watson Analytics Lab and is a fictional data set created by IBM data scientists to explore what employee attributes influence attrition. problem type: supervised binomial classification response variable: Attrition (i.e., “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) # access data attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) # initial dimension dim(attrition) ## [1] 1470 31 # features dplyr::select(attrition, -Attrition) ## # A tibble: 1,470 × 30 ## Age BusinessTravel DailyRate Department DistanceFromHome Education ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 41 Travel_Rarely 1102 Sales 1 College ## 2 49 Travel_Freque… 279 Research_… 8 Below_Co… ## 3 37 Travel_Rarely 1373 Research_… 2 College ## 4 33 Travel_Freque… 1392 Research_… 3 Master ## 5 27 Travel_Rarely 591 Research_… 2 Below_Co… ## 6 32 Travel_Freque… 1005 Research_… 2 College ## 7 59 Travel_Rarely 1324 Research_… 3 Bachelor ## 8 30 Travel_Rarely 1358 Research_… 24 Below_Co… ## 9 38 Travel_Freque… 216 Research_… 23 Bachelor ## 10 36 Travel_Rarely 1299 Research_… 27 Bachelor ## # ℹ 1,460 more rows ## # ℹ 24 more variables: EducationField &lt;chr&gt;, ## # EnvironmentSatisfaction &lt;chr&gt;, Gender &lt;chr&gt;, HourlyRate &lt;dbl&gt;, ## # JobInvolvement &lt;chr&gt;, JobLevel &lt;dbl&gt;, JobRole &lt;chr&gt;, ## # JobSatisfaction &lt;chr&gt;, MaritalStatus &lt;chr&gt;, MonthlyIncome &lt;dbl&gt;, ## # MonthlyRate &lt;dbl&gt;, NumCompaniesWorked &lt;dbl&gt;, OverTime &lt;chr&gt;, ## # PercentSalaryHike &lt;dbl&gt;, PerformanceRating &lt;chr&gt;, … # response variable head(attrition$Attrition) ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 2.5.6 Hitters This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. The idea was to illustrate if and how major league baseball player’s batting performance could predict their salary. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. Note that the data does contain the players name but this should be removed during analysis and is not a valid feature. problem type: supervised regression response variable: Salary features: 19 observations: 322 objective: use baseball player’s batting attributes to predict their salary. # access data hitters &lt;- readr::read_csv(here(data_path, &quot;hitters.csv&quot;)) # initial dimension dim(hitters) ## [1] 322 21 # features dplyr::select(hitters, -Salary, -Player) ## # A tibble: 322 × 19 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 293 66 1 30 29 14 1 293 66 1 30 ## 2 315 81 7 24 38 39 14 3449 835 69 321 ## 3 479 130 18 66 72 76 3 1624 457 63 224 ## 4 496 141 20 65 78 37 11 5628 1575 225 828 ## 5 321 87 10 39 42 30 2 396 101 12 48 ## 6 594 169 4 74 51 35 11 4408 1133 19 501 ## 7 185 37 1 23 8 21 2 214 42 1 30 ## 8 298 73 0 24 24 7 3 509 108 0 41 ## 9 323 81 6 26 32 8 2 341 86 6 32 ## 10 401 92 17 49 66 65 13 5206 1332 253 784 ## # ℹ 312 more rows ## # ℹ 8 more variables: CRBI &lt;dbl&gt;, CWalks &lt;dbl&gt;, League &lt;chr&gt;, ## # Division &lt;chr&gt;, PutOuts &lt;dbl&gt;, Assists &lt;dbl&gt;, Errors &lt;dbl&gt;, ## # NewLeague &lt;chr&gt; # response variable head(hitters$Salary) ## [1] NA 475.0 480.0 500.0 91.5 750.0 2.6 What You’ll Learn Next The lessons that follow are designed to help you understand the individual sub-tasks of an ML project. The focus is to have an intuitive understanding of each discrete sub-task and algorithm. Once you understand when, where, and why these sub-tasks are performed you will be able to transfer this knowledge to other projects. The concepts you will learn include: Provide an overview of the ML modeling process: data splitting model fitting model validation and tuning performance measurement feature engineering Cover common supervised learners: linear regression regularized regression K-nearest neighbors decision trees bagging &amp; random forests gradient boosting Cover common unsupervised learners: K-means clustering Principal component analysis Along the way you’ll learn about: each algorithm’s hyperparameters model interpretation feature importance and more! 2.7 Exercises Identify four real-life applications of supervised and unsupervised problems. Explain what makes these problems supervised versus unsupervised. For each problem identify the target variable (if applicable) and potential features. Identify and contrast a regression problem with a classification problem. What is the target variable in each problem and why would being able to accurately predict this target be beneficial to society? What are potential features and where could you collect this information? What is determining if the problem is a regression or a classification problem? Identify three open source data sets suitable for machine learning (e.g., https://bit.ly/35wKu5c). Explain the type of machine learning models that could be constructed from the data (e.g., supervised versus unsupervised and regression versus classification). What are the dimensions of the data? Is there a code book that explains who collected the data, why it was originally collected, and what each variable represents? If the data set is suitable for supervised learning, which variable(s) could be considered as a useful target? Which variable(s) could be considered as features? Identify examples of misuse of machine learning in society. What was the ethical concern? References De Cock, Dean. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3). Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. Harrison Jr, David, and Daniel L Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” Journal of Environmental Economics and Management 5 (1): 81–102. "],["lesson-1b-first-model-with-tidymodels.html", "3 Lesson 1b: First model with Tidymodels 3.1 Learning objectives 3.2 Prerequisites 3.3 Data splitting 3.4 Building models 3.5 Making predictions 3.6 Evaluating model performance 3.7 Exercises", " 3 Lesson 1b: First model with Tidymodels Much like exploratory data analysis (EDA), the machine learning (ML) process is very iterative and heuristic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the no free lunch theorem for ML (Wolpert 1996). Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging.1 Approaching ML modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Many books and courses portray the modeling process as a short sprint. A better analogy would be a marathon where many iterations of these steps are repeated before eventually finding the final optimal model. This process is illustrated below. Figure 3.1: General predictive machine learning process. Before introducing specific algorithms, this lesson introduces concepts that are fundamental to the ML modeling process and that you’ll see briskly covered in future modeling lessons. More specifically, this lesson is designed to get you acquainted with building predictive models using the Tidymodels construct. We’ll focus on the process of splitting our data for improved generalizability, using Tidymodel’s parsnip package for constructing our models, along with yardstick to measure model performance. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. 3.1 Learning objectives By the end of this lesson you will be able to: Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic model. 3.2 Prerequisites For this lesson we’ll primarily use the tidymodels package. library(tidymodels) library(here) The two data sets we’ll use are ames and attrition. data_path &lt;- here(&quot;data&quot;) ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) When performing classification models our response variable needs to be a factor (or sometimes as 0 vs. 1). Consequently, the code chunk below sets the Attrition response variable as a factor rather than as a character. attrition &lt;- attrition %&gt;% dplyr::mutate(Attrition = as.factor(Attrition)) 3.3 Data splitting A major goal of the machine learning process is to find an algorithm \\(f\\left(X\\right)\\) that most accurately predicts future values (\\(\\hat{Y}\\)) based on a set of features (\\(X\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets: Training set: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production). Test set: having chosen a final model, these data are used to estimate an unbiased assessment of the model’s performance, which we refer to as the generalization error. Figure 3.2: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60% (training)–40% (testing), 70%–30%, or 80%–20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind: Spending too much in training (e.g., \\(&gt;80\\%\\)) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting). Sometimes too much spent in testing (\\(&gt;40\\%\\)) won’t allow us to get a good assessment of model parameters. Other factors should also influence the allocation proportions. For example, very large training sets (e.g., \\(n &gt; 100\\texttt{K}\\)) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as \\(p \\geq n\\) (where \\(p\\) represents the number of features), larger samples sizes are often required to identify consistent signals in the features. The two most common ways of splitting data include simple random sampling and stratified sampling. 3.3.1 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the distribution of your response variable (\\(Y\\)). Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this course we’ll often use the seed 123 for reproducibility but the number itself has no special meaning. # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) train &lt;- training(split) test &lt;- testing(split) # dimensions of training data dim(train) ## [1] 2051 81 With sufficient sample size, this sampling approach will typically result in a similar distribution of \\(Y\\) (e.g., Sale_Price in the ames data) between your training and test sets, as illustrated below. train %&gt;% mutate(id = &#39;train&#39;) %&gt;% bind_rows(test %&gt;% mutate(id = &#39;test&#39;)) %&gt;% ggplot(aes(Sale_Price, color = id)) + geom_density() 3.3.2 Stratified sampling If we want to explicitly control the sampling so that our training and test sets have similar \\(Y\\) distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90% of observations with response “Yes” and 10% with response “No”). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will segment \\(Y\\) into quantiles and randomly sample from each. To perform stratified sampling we simply apply the strata argument in initial_split. set.seed(123) split_strat &lt;- initial_split(attrition, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions. # original response distribution table(attrition$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # response distribution for training data table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8394942 0.1605058 # response distribution for test data table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8371041 0.1628959 3.3.3 Knowledge check Import the penguins data from the modeldata package Create a 70-30 stratified train-test split (species is the target variable). What are the response variable proportions for the train and test data sets? 3.4 Building models The R ecosystem provides a wide variety of ML algorithm implementations. This makes many powerful algorithms available at your fingertips. Moreover, there are almost always more than one package to perform each algorithm (e.g., there are over 20 packages for fitting random forests). There are pros and cons to this wide selection; some implementations may be more computationally efficient while others may be more flexible. This also has resulted in some drawbacks as there are inconsistencies in how algorithms allow you to define the formula of interest and how the results and predictions are supplied. Fortunately, the tidymodels ecosystem is simplifying this and, in particular, the Parsnip package provides one common interface to train many different models supplied by other packages. Consequently, we’ll focus on building models the tidymodels way. To create and fit a model with parsnip we follow 3 steps: Create a model type Choose an “engine” Fit our model Let’s illustrate by building a linear regression model. For our first model we will simply use two features from our training data - total square feet of the home (Gr_Liv_Area) and year built (Year_Built) to predict the sale price (Sale_Price). We can use tidy() to get results of our model’s parameter estimates and their statistical properties. Although the summary() function can provide this output, it gives the results back in an unwieldy format. Go ahead, and run summary(lm_ols) to compare the results to what we see below. Many models have a tidy() method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names) lm_ols &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) tidy(lm_ols) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2157423. 69234. -31.2 8.09e-175 ## 2 Gr_Liv_Area 94.4 2.12 44.4 2.54e-302 ## 3 Year_Built 1114. 35.5 31.4 5.30e-177 Now, you may have noticed that I only applied two of the three steps I mentioned previously: Create a model type Choose an “engine” Fit our model The reason is because most model objects (linear_reg() in this example) have a default engine. linear_reg() by default uses lm for ordinary least squares. But we can always change the engine. For example, say I wanted to use keras to perform gradient descent linear regression, then I could change the engine to keras but use the same code workflow. For this code to run successfully on your end you need to have the keras and tensorflow packages installed on your machine. Depending on your current setup this could be an easy process or you could run into problems. If you run into problems don’t fret, this is primarily just to illustrate how we can change engines. lm_sgd &lt;- linear_reg() %&gt;% set_engine(&#39;keras&#39;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) ## Epoch 1/20 ## 65/65 - 1s - loss: 39844552704.0000 - 1s/epoch - 18ms/step ## Epoch 2/20 ## 65/65 - 0s - loss: 39657136128.0000 - 207ms/epoch - 3ms/step ## Epoch 3/20 ## 65/65 - 0s - loss: 39486091264.0000 - 176ms/epoch - 3ms/step ## Epoch 4/20 ## 65/65 - 0s - loss: 39329832960.0000 - 182ms/epoch - 3ms/step ## Epoch 5/20 ## 65/65 - 0s - loss: 39185645568.0000 - 184ms/epoch - 3ms/step ## Epoch 6/20 ## 65/65 - 0s - loss: 39054196736.0000 - 185ms/epoch - 3ms/step ## Epoch 7/20 ## 65/65 - 0s - loss: 38933123072.0000 - 183ms/epoch - 3ms/step ## Epoch 8/20 ## 65/65 - 0s - loss: 38821081088.0000 - 181ms/epoch - 3ms/step ## Epoch 9/20 ## 65/65 - 0s - loss: 38716805120.0000 - 183ms/epoch - 3ms/step ## Epoch 10/20 ## 65/65 - 0s - loss: 38618251264.0000 - 187ms/epoch - 3ms/step ## Epoch 11/20 ## 65/65 - 0s - loss: 38522990592.0000 - 185ms/epoch - 3ms/step ## Epoch 12/20 ## 65/65 - 0s - loss: 38426714112.0000 - 184ms/epoch - 3ms/step ## Epoch 13/20 ## 65/65 - 0s - loss: 38330507264.0000 - 185ms/epoch - 3ms/step ## Epoch 14/20 ## 65/65 - 0s - loss: 38228709376.0000 - 176ms/epoch - 3ms/step ## Epoch 15/20 ## 65/65 - 0s - loss: 38117527552.0000 - 195ms/epoch - 3ms/step ## Epoch 16/20 ## 65/65 - 0s - loss: 37992321024.0000 - 183ms/epoch - 3ms/step ## Epoch 17/20 ## 65/65 - 0s - loss: 37848707072.0000 - 178ms/epoch - 3ms/step ## Epoch 18/20 ## 65/65 - 0s - loss: 37681577984.0000 - 181ms/epoch - 3ms/step ## Epoch 19/20 ## 65/65 - 0s - loss: 37488734208.0000 - 181ms/epoch - 3ms/step ## Epoch 20/20 ## 65/65 - 0s - loss: 37266784256.0000 - 181ms/epoch - 3ms/step When we talk about ‘engines’ we’re really just referring to packages that provide the desired algorithm. Each model object has different engines available to use and they are all documented. For example check out the help file for linear_reg (?linear_reg) and you’ll see the different engines available (lm, brulee, glm, glmnet, etc.) The beauty of this workflow is that if we want to explore different models we can simply change the model object. For example, say we wanted to run a K-nearest neighbor model. We can just use nearest_neighbor(). In this example we have pretty much the same code as above except we added the line of code set_mode(). This is because most algorithms require you to specify if you are building a regression model or a classification model. When you run this code you’ll probably get an error message saying that “This engine requires some package installs: ‘kknn’.” This just means you need to install.packages(‘kknn’) and then you should be able to successfully run this code. knn &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) You can see all the different model objects available at https://parsnip.tidymodels.org/reference/index.html 3.4.1 Knowledge check If you haven’t already done so, create a 70-30 stratified train-test split on the attrition data (note: Attrition is the response variable). Using the logistic_reg() model object, fit a model using Age, DistanceFromHome, and JobLevel as the features. Now train a K-nearest neighbor model using the ‘kknn’ engine and be sure to set the mode to be a classification model. 3.5 Making predictions We have fit a few different models. Now, if we want to see our predictions we can simply apply predict() and feed it the data set we want to make predictions on. Here, we can see the predictions made on our training data for our ordinary least square linear regression model. lm_ols %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 217657. ## 2 214276. ## 3 223425. ## 4 260324. ## 5 109338. ## 6 195106. ## 7 222217. ## 8 126175. ## 9 98550. ## 10 120811. ## # ℹ 2,041 more rows And here we get the predicted values for our KNN model. knn %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 194967. ## 2 192240 ## 3 174220 ## 4 269760 ## 5 113617. ## 6 173672 ## 7 174820 ## 8 120796 ## 9 114560 ## 10 121346 ## # ℹ 2,041 more rows 3.5.1 Knowledge check Make predictions on the test data using the logistic regression model you built on the attrition data. Now make predictions using the K-nearest neighbor model. 3.6 Evaluating model performance It is important to understand how our model is performing. With ML models, measuring performance means understanding the predictive accuracy – the difference between a predicted value and the actual value. We measure predictive accuracy with loss functions. There are many loss functions to choose from when assessing the performance of a predictive model, each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the “optimal model”. Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric. 3.6.1 Regression models The most common loss functions for regression models include: MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\))2. The squared component results in larger errors having larger penalties. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Let’s compute the RMSE of our OLS regression model. Remember, we want to assess our model’s performance on the test data not the training data since that gives us a better idea of how our model generalizes. To do so, the following: Makes predictions with our test data, Adds the actual Sale_Price values from our test data, Computes the RMSE. lm_ols %&gt;% predict(test) %&gt;% bind_cols(test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 45445. The RMSE value suggests that, on average, our model mispredicts the expected sale price of a home by about $45K. 3.6.2 Classification models There are many loss functions used for classification models. For simplicity we’ll just focus on the overall classification accuracy. I’ll illustrate with the attrition data. Here, we build a logistic regression model that seeks to predict Attrition based on all available features. In R, using a “.” as in Attrition ~ . is a shortcut for saying use all available features to predict Attrition. We then follow the same process as above to make predictions on the test data, add the actual test values for Attrition, and then compute the accuracy rate. logit &lt;- logistic_reg() %&gt;% fit(Attrition ~ ., data = train_strat) logit %&gt;% predict(test_strat) %&gt;% bind_cols(test_strat %&gt;% select(Attrition)) %&gt;% accuracy(truth = Attrition, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.876 3.6.3 Knowledge check Compute the accuracy rate of your logistic regression model for the attrition data. Now compute the accuracy rate of your K-nearest neighbor model. 3.7 Exercises For this exercise we’ll use the Boston housing data set. The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978), it contains 13 attributes to predict the median property value. Data attributes: problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes Modeling tasks: Import the Boston housing data set (boston.csv) and split it into a training set and test set using a 70-30% split. How many observations are in the training set and test set? Compare the distribution of cmedv between the training set and test set. Fit a linear regression model using all available features to predict cmedv and compute the RMSE on the test data. Fit a K-nearest neighbor model that uses all available features to predict cmedv and compute the RMSE on the test data. How do these models compare? References Harrison Jr, David, and Daniel L Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” Journal of Environmental Economics and Management 5 (1): 81–102. Wolpert, David H. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90. See https://www.fatml.org/resources/relevant-scholarship for many discussions regarding implications of poorly applied and interpreted ML.↩︎ This deviates slightly from the usual definition of MSE in ordinary linear regression, where we divide by \\(n-p\\) (to adjust for bias) as opposed to \\(n\\).↩︎ "],["overview-1.html", "4 Overview 4.1 Learning objectives 4.2 Estimated time requirement 4.3 Tasks", " 4 Overview In the last module we discussed the basics of fitting a model. Part of this process included creating a model type and we illustrated how to apply linear regression, K-nearest neighbor, and logistic regression models. Yet, we didn’t really discuss these algorithms and what they are doing under the hood. We’ll turn our attention to that now and we’ll start by looking at a fundamental algorithm – linear regression. 4.1 Learning objectives By the end of this module you should be able to: Explain how a linear regression model characterizes the data it is applied to. Fit, interpret, and assess the performance of simple and multiple linear regression models. 4.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 3 hours Reading + videos: 4 hours 4.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-2a-simple-linear-regression.html", "5 Lesson 2a: Simple linear regression 5.1 Learning objectives 5.2 Prerequisites 5.3 Correlation 5.4 Simple linear regression 5.5 Making predictions 5.6 Assessing model accuracy 5.7 Exercises 5.8 Other resources", " 5 Lesson 2a: Simple linear regression Linear regression, a staple of classical statistical modeling, is one of the simplest algorithms for doing supervised learning. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later modules, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good starting point for more advanced approaches because many of the more sophisticated statistical learning approaches can be seen as generalizations to or extensions of ordinary linear regression. Consequently, it is important to have a good understanding of linear regression before studying more complex learning methods. This lesson introduces simple linear regression with an emphasis on prediction, rather than explanation. Modeling for explanation: When you want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(x\\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables. Modeling for prediction: When you want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\). Check out Lipovetsky (2020) for a great introduction to linear regression for explanation. 5.1 Learning objectives By the end of this lesson you will know how to: Fit a simple linear regression model. Interpret results of a simple linear regression model. Assess the performance of a simple linear regression model. 5.2 Prerequisites This lesson leverages the following packages: # Data wrangling &amp; visualization packages library(tidyverse) # Modeling packages library(tidymodels) We’ll also continue working with the ames data set: Take a minute to check out the Ames data from AmesHousing::make_ames(). It is very similar to the Ames data provided in the CSV. Note that the various quality/condition variables (i.e. Overall_Qual) are already encoded as ordinal factors. # stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 5.3 Correlation Correlation is a single-number statistic that measures the extent that two variables are related (“co-related”) to one another. For example, say we want to understand the relationship between the total above ground living space of a home (Gr_Liv_Area) and the home’s sale price (Sale_Price). Looking at the following scatter plot we can see that some relationship does exist. It appears that as Gr_Liv_Area increases the Sale_Price of a home increases as well. ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) Correlation allows us to quantify this relationship. We can compute the correlation with the following: ames_train %&gt;% summarize(correlation = cor(Gr_Liv_Area, Sale_Price)) ## # A tibble: 1 × 1 ## correlation ## &lt;dbl&gt; ## 1 0.708 The value of the correlation coefficient varies between +1 and -1. In our example, the correlation coefficient is 0.71. When the value of the correlation coefficient lies around ±1, then it is said to be a perfect degree of association between the two variables (near +1 implies a strong positive association and near -1 implies a strong negative association). As the correlation coefficient nears 0, the relationship between the two variables weakens with a near 0 value implying no association between the two variables. So, in our case we could say we have a moderate positive correlation between Gr_Liv_Area and Sale_Price. Let’s look at another relationship. In the following we look at the relationship between the unfinished basement square footage of homes (Bsmt_Unf_SF) and the Sale_Price. ggplot(ames_train, aes(Bsmt_Unf_SF, Sale_Price)) + geom_point(size = 1.5, alpha = .25) In this example, we don’t see much of a relationship. Basically, as Bsmt_Unf_SF gets larger or smaller, we really don’t see a strong pattern with Sale_Price. If we look at the correlation for this relationship, we see that the correlation coefficient is much closer to zero than to 1. This confirms our visual assessment that there does not seem to be much of a relationship between these two variables. ames_train %&gt;% summarize(correlation = cor(Bsmt_Unf_SF, Sale_Price)) ## # A tibble: 1 × 1 ## correlation ## &lt;dbl&gt; ## 1 0.186 5.3.1 Knowledge check Interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the 80s-style video game called, “Guess the Correlation”, at http://guessthecorrelation.com/ Using the ames_train data, visualize the relationship between Year_Built and Sale_Price. Guess what the correlation is between these two variables? Now compute the correlation between these two variables. Although a useful measure, correlation can be hard to imagine exactly what the association is between two variables based on this single statistic. Moreover, its important to realize that correlation assumes a linear relationship between two variables. For example, let’s check out the anscombe data, which is a built-in data set provided in R. If we look at each x and y relationship visually, we can see significant differences: p1 &lt;- qplot(x = x1, y = y1, data = anscombe) p2 &lt;- qplot(x = x2, y = y2, data = anscombe) p3 &lt;- qplot(x = x3, y = y3, data = anscombe) p4 &lt;- qplot(x = x4, y = y4, data = anscombe) gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2) However, if we compute the correlation between each of these relationships we see that they all have nearly equal correlation coefficients! Never take a correlation coefficient at face value! You should always compare the visual relationship with the computed correlation value. anscombe %&gt;% summarize( corr_x1_y1 = cor(x1, y1), corr_x2_y2 = cor(x2, y2), corr_x3_y3 = cor(x3, y3), corr_x4_y4 = cor(x4, y4) ) ## corr_x1_y1 corr_x2_y2 corr_x3_y3 corr_x4_y4 ## 1 0.8164205 0.8162365 0.8162867 0.8165214 There are actually several different ways to measure correlation. The most common, and the one we’ve been using here, is Pearson’s correlation. Alternative methods allow us to loosen some assumptions such as assuming a linear relationship. You can read more at http://uc-r.github.io/correlations. 5.4 Simple linear regression As discussed in the last section, correlation is often used to quantify the strength of the linear association between two continuous variables. However, this statistic alone does not provide us with a lot of actionable insights. But we can build on the concept of correlation to provide us with more useful information. In this section, we seek to fully characterize the linear relationship we measured with correlation using a method called simple linear regression (SLR). 5.4.1 Best fit line Let’s go back to our plot illustrating the relationship between Gr_Liv_Area and Sale_Price. We can characterize this relationship with a linear line that we consider is the “best-fitting” line (we’ll define “best-fitting” in a little bit). We do this by adding overplotting with geom_smooth(method = \"lm\", se = FALSE) ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) The line in the above plot is called a “regression line.” The regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable Sale_Price and the explanatory variable Gr_Liv_Area. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.71 suggesting that there is a positive relationship between these two variables. 5.4.2 Estimating “best fit” You may recall from secondary/high school algebra that the equation of a line is \\(y = a + b \\times x\\). Often, in formulas like these we illustrate multiplication without the \\(\\times\\) symbol like the following \\(y = a + bx\\). The equation of our line is defined by two coefficients \\(a\\) and \\(b\\). The intercept coefficient \\(a\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(b\\) for \\(x\\) is the increase in \\(y\\) for every increase of one in \\(x\\). This is also called the “rise over run.” However, when defining a regression line like the regression line in the previous plot, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 x\\). The intercept coefficient is \\(b_0\\), so \\(b_0\\) is the value of \\(\\widehat{y}\\) when \\(x = 0\\). The slope coefficient for \\(x\\) is \\(b_1\\), i.e., the increase in \\(\\widehat{y}\\) for every increase of one unit in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a “fitted value,” or the value of \\(y\\) on the regression line for a given \\(x\\) value. So what are the coefficients of our best fit line that characterizes the relationship between Gr_Liv_Area and Sale_Price? We can get that by fitting an SLR model where Sale_Price is our response variable and Gr_Liv_Area is our single predictor variable. Once our model is fit we can extract our fitted model results with tidy(): model1 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area, data = ames_train) tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 The estimated coefficients from our model are \\(b_0 =\\) 15938.17 and \\(b_1 =\\) 109.67. To interpret, we estimate that the mean selling price increases by 109.67 for each additional one square foot of above ground living space. With these coefficients, we can look at our scatter plot again (this time with the x &amp; y axes formatted) and compare the characterization of our linear line with the coefficients. This simple description of the relationship between the sale price and square footage using a single number (i.e., the slope) is what makes linear regression such an intuitive and popular modeling tool. ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::dollar) This is great but you may still be asking how we are estimating the coefficients? Ideally, we want estimates of \\(b_0\\) and \\(b_1\\) that give us the “best fitting” line. But what is meant by “best fitting”? The most common approach is to use the method of least squares (LS) estimation; this form of linear regression is often referred to as ordinary least squares (OLS) regression. There are multiple ways to measure “best fitting”, but the LS criterion finds the “best fitting” line by minimizing the residual sum of squares (RSS). Before we define RSS, let’s first define what a residual is. Let’s look at a single home. This home has 3,608 square feet of living space and sold for $475,000. In other words, \\(x = 3608\\) and \\(y = 475000\\). ## # A tibble: 1 × 2 ## Gr_Liv_Area Sale_Price ## &lt;int&gt; &lt;int&gt; ## 1 3608 475000 Based on our linear regression model (or the intercept and slope we identified from our model) our best fit line estimates that this house’s sale price is \\[\\widehat{y} = b_0 + b_1 \\times x = 15938.1733 + 109.6675 \\times 3608 = 411618.5\\] We can visualize this in our plot where we have the actual Sale_Price (orange) and the estimated Sale_Price based on our fitted line. The difference between these two values (\\(y - \\widehat{y} = 475000 - 411618.5 = 63381.5\\)) is what we call our residual. It is considered the error for this observation, which we can visualize with the red line. Now, if we look across all our data points you will see that each one has a residual associated with it. In the right plot, the vertical lines represent the individual residuals/errors associated with each observation. Figure 5.1: The least squares fit from regressing sale price on living space for the the Ames housing data. Left: Fitted regression line. Right: Fitted regression line with vertical grey bars representing the residuals. The OLS criterion identifies the “best fitting” line that minimizes the sum of squares of these residuals. Mathematically, this is computed by taking the sum of the squared residuals (or as stated before the residual sum of squares –&gt; “RSS”). \\[\\begin{equation} RSS = \\sum_{i=1}^n\\left(y_i - \\widehat{y_i}\\right)^2 \\end{equation}\\] where \\(y_i\\) and \\(\\widehat{y_i}\\) just mean the actual and predicted response values for the ith observation. 5.4.3 Inference Let’s go back to our model1 results that show the \\(b_0\\) and \\(b_1\\) coefficient values: tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 Note that we call these coefficient values “estimates.” Due to various reasons we should always assume that there is some variability in our estimated coefficient values. The variability of an estimate is often measured by its standard error (SE). When we fit our linear regression model the SE for each coefficient was computed for us and are displayed in the column labeled std.error in the output from tidy(). From this, we can also derive simple \\(t\\)-tests to understand if the individual coefficients are statistically significant from zero. The t-statistics for such a test are nothing more than the estimated coefficients divided by their corresponding estimated standard errors (i.e., in the output from tidy(), t value (aka statistic) = estimate / std.error). The reported t-statistics measure the number of standard deviations each coefficient is away from 0. Thus, large t-statistics (greater than two in absolute value, say) roughly indicate statistical significance at the \\(\\alpha = 0.05\\) level. The p-values for these tests are also reported by tidy() in the column labeled p.value. This may seem quite complicated but don’t worry, R will do the heavy lifting for us. Just realize we can use these additional statistics provided in our model summary to tell us if the predictor variable (Gr_Liv_Area in our example) has a statistically significant relationship with our response variable. When the p.value for a given coefficient is quite small (i.e. p.value &lt; 0.005), that is a good indication that the estimate for that coefficient is statistically different than zero. For example, the p.value for the Gr_Liv_Area coefficient is 5.17e-311 (basically zero). This means that the estimated coefficient value of 109.6675 is statistically different than zero. Let’s look at this from another perspective. We can compute the 95% confidence intervals for the coefficients in our SLR example. confint(model1$fit, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 8384.213 23492.1336 ## Gr_Liv_Area 104.920 114.4149 To interpret, we estimate with 95% confidence that the mean selling price increases between 104.92 and 114.41 for each additional one square foot of above ground living space. We can also conclude that the slope \\(b_1\\) is significantly different from zero (or any other pre-specified value not included in the interval) at the \\(\\alpha = 0.05\\) level (\\(\\alpha = 0.05\\) because we just take 1 - confidence level we are computing so \\(1 - 0.95 = 0.05\\)). 5.4.4 Knowledge check Let’s revisit the relationship between Year_Built and Sale_Price. Using the ames_train data: Visualize the relationship between these two variables. Compute their correlation. Create a simple linear regression model where Sale_Price is a function of Year_Built. Interpret the coefficient for Year_Built. What is the 95% confidence interval for this coefficient and can we confidently say it is statistically different than zero? 5.5 Making predictions We’ve created a simple linear regression model to describe the relationship between Gr_Liv_Area and Sale_Price. As we saw in the last module, we can make predictions with this model. model1 %&gt;% predict(ames_train) ## # A tibble: 2,049 × 1 ## .pred ## &lt;dbl&gt; ## 1 135695. ## 2 135695. ## 3 107620. ## 4 98408. ## 5 126922. ## 6 224526. ## 7 114639. ## 8 129992. ## 9 205444. ## 10 132515. ## # ℹ 2,039 more rows And we can always add these predictions back to our training data if we want to look at how the predicted values differ from the actual values. model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% select(Gr_Liv_Area, Sale_Price, .pred) ## # A tibble: 2,049 × 3 ## Gr_Liv_Area Sale_Price .pred ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1092 105500 135695. ## 2 1092 88000 135695. ## 3 836 120000 107620. ## 4 752 125000 98408. ## 5 1012 67500 126922. ## 6 1902 112000 224526. ## 7 900 122000 114639. ## 8 1040 127000 129992. ## 9 1728 84900 205444. ## 10 1063 128000 132515. ## # ℹ 2,039 more rows 5.6 Assessing model accuracy This allows us to assess the accuracy of our model. Recall from the last module that for regression models we often use mean squared error (MSE) and root mean squared error (RMSE) to quantify the accuracy of our model. These two values are directly correlated to the RSS we discussed above, which determines the best fit line. Let’s illustrate. 5.6.1 Training data accuracy Recall that the residuals are the differences between the actual \\(y\\) and the estimated \\(\\widehat{y}\\) based on the best fit line. residuals &lt;- model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% select(Gr_Liv_Area, Sale_Price, .pred) %&gt;% mutate(residual = Sale_Price - .pred) head(residuals, 5) ## # A tibble: 5 × 4 ## Gr_Liv_Area Sale_Price .pred residual ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1092 105500 135695. -30195. ## 2 1092 88000 135695. -47695. ## 3 836 120000 107620. 12380. ## 4 752 125000 98408. 26592. ## 5 1012 67500 126922. -59422. The RSS squares these values and then sums them. residuals %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize(sum_of_squared_residuals = sum(squared_residuals)) ## # A tibble: 1 × 1 ## sum_of_squared_residuals ## &lt;dbl&gt; ## 1 6.60e12 Why do we square the residuals? So that both positive and negative deviations of the same amount are treated equally. While taking the absolute value of the residuals would also treat both positive and negative deviations of the same amount equally, squaring the residuals is used for reasons related to calculus: taking derivatives and minimizing functions. If you’d like to learn more we suggest you consult one of the textbooks referenced at the end of the lesson. However, when expressing the performance of a model we rarely state the RSS. Instead it is more common to state the average of the squared error, or the MSE as discussed here. Unfortunately, both the RSS and MSE are not very intuitive because the units the metrics are expressed in do have much meaning. So, we usually use the RMSE metric, which simply takes the square root of the MSE metric so that your error metric is in the same units as your response variable. We can manually compute this with the following, which tells us that on average, our linear regression model mispredicts the expected sale price of a home by about $56,760. residuals %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize( MSE = mean(squared_residuals), RMSE = sqrt(MSE) ) ## # A tibble: 1 × 2 ## MSE RMSE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3221722037. 56760. We could also compute this using the rmse() function we saw in the last module: model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56760. 5.6.2 Test data accuracy Recall that a major goal of the machine learning process is to find a model that most accurately predicts future values based on a set of features. In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. In the last module we called this our generalization error. So, ultimately, we want to understand how well our model will generalize to unseen data. To do this we need to compute the RMSE of our model on our test set. Here, we see that our test RMSE is right around the same as our training data. As we’ll see in later modules, this is not always the case. model1 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 55942. 5.6.3 Knowledge check Let’s revisit the simple linear regression model you created earlier with Year_Built and Sale_Price. Using this model, make predictions using the test data. What is the predicted value for the first home in the test data? Compute the generalization RMSE for this model. Interpret the generalization RMSE. How does this model compare to the model based on Gr_Liv_Area? 5.7 Exercises Using the Boston housing data set where the response feature is the median value of homes within a census tract (cmedv): Split the data into 70-30 training-test sets. Using the training data, pick a single feature variable and… Visualize the relationship between that feature and cmedv. Compute the correlation between that feature and cmedv. Create a simple linear regression model with cmedv as a function of that feature variable. Interpret the feature’s coefficient. What is the model’s generalization error? Now pick another feature variable and repeat the process in #2. 5.8 Other resources Some execellent resources to go deeper into linear regression: Kutner et al. (2005) for an excellent and comprehensive overview of linear regression. Faraway (2016b) for a thorough discussion of linear regression in R. Lipovetsky (2020) for a great introduction to linear regression for explanation rather than prediction. References ———. 2016b. Linear Models with r. Chapman; Hall/CRC. Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li. 2005. Applied Linear Statistical Models. 5th ed. McGraw Hill. Lipovetsky, Stan. 2020. Taylor &amp; Francis. "],["lesson-2b-multiple-linear-regression.html", "6 Lesson 2b: Multiple linear regression 6.1 Learning objectives 6.2 Prerequisites 6.3 Adding additional predictors 6.4 Interactions 6.5 Qualitative predictors 6.6 Including many predictors 6.7 Feature importance 6.8 Exercises", " 6 Lesson 2b: Multiple linear regression In the last lesson we learned how to use one predictor variable to predict a numeric response. However, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (Gr_Liv_Area) and the year the house was built (Year_Built) are (linearly) related to sale price (Sale_Price). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the multiple linear regression (MLR) model and is the focus for this lesson. 6.1 Learning objectives By the end of this lesson you will know how to: Fit, interpret, and assess the performance of a multiple linear regression model. Include categorical features in a linear regression model and interpret their results. Asses the most influential predictor variables in a linear regression model. 6.2 Prerequisites This lesson leverages the following packages: # Data wrangling &amp; visualization packages library(tidyverse) # Modeling packages library(tidymodels) We’ll also continue working with the ames data set: # stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 6.3 Adding additional predictors In the last lesson we saw how we could use the above ground square footage (Gr_Liv_Area) of a house to predict the sale price (Sale_Price). model1 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area, data = ames_train) tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 From our model we interpreted the results as that the mean selling price increases by 109.67 for each additional one square foot of above ground living space. We also determined that the Gr_Liv_Area coefficient is statistically different from zero based on the p.value. And, we saw that our model has a generalization RMSE value of 55942, which means that on average, our model’s predicted sales price differs from the actual sale price by $55,942. model1 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 55942. However, we are only using a single predictor variable to try predict the sale price. In reality, we likely can use other home attributes to do a better job at predicting sale price. For example, we may wish to understand if above ground square footage (Gr_Liv_Area) and the year the house was built (Year_Built) are (linearly) related to sale price (Sale_Price). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the multiple linear regression (MLR) model. With two predictors, the MLR model becomes: \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2, \\end{equation}\\] where \\(x_1\\) and \\(x_2\\) are features of interest. In our Ames housing example, \\(x_1\\) can represent Gr_Liv_Area and \\(x_2\\) can represent Year_Built. In R, multiple linear regression models can be fit by separating all the features of interest with a +: model2 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train) tidy(model2) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2102905. 69441. -30.3 9.17e-167 ## 2 Gr_Liv_Area 93.8 2.07 45.3 1.17e-310 ## 3 Year_Built 1087. 35.6 30.5 3.78e-169 The LS estimates of the regression coefficients are \\(\\widehat{b}_1 =\\) 93.828 and \\(\\widehat{b}_2 =\\) 1086.852 (the estimated intercept is -2.1029046^{6}. In other words, every one square foot increase to above ground square footage is associated with an additional $93.83 in mean selling price when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of $1,086.85 in selling price when holding the above ground square footage constant. As our model results show above, the p.values for our two coefficients suggest that both are stastically different than zero. This can also be confirmed by computing our confidence intervals around these coefficient estimates as we did before. confint(model2$fit) ## 2.5 % 97.5 % ## (Intercept) -2.239086e+06 -1.966723e+06 ## Gr_Liv_Area 8.976367e+01 9.789288e+01 ## Year_Built 1.017072e+03 1.156632e+03 Now, instead of modeling sale price with the the “best fitting” line that minimizes residuals, we are modeling sale price with the best fitting hyperplane that minimizes the residuals, which is illustrated below. Figure 6.1: Average home sales price as a function of year built and total square footage. 6.3.1 Knowledge check Using the ames_train data: Fit a MLR model where Sale_Price is a function of Gr_Liv_Area and Garage_Cars. Interpret the coefficients. Are they both statistically different from zero? Compute and interpret the generalization RMSE for this model. How does this model compare to the model based on just Gr_Liv_Area? 6.4 Interactions You may notice that the fitted plane in the above image is flat; there is no curvature. This is true for all linear models that include only main effects (i.e., terms involving only a single predictor). One way to model curvature is to include interaction effects. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. Suppose that when people buy older homes they care more about the historical nature and beauty of the home rather than the total square footage. However, as older historical homes grow larger in size we see a compounding impact to the value of the home. This is known as a synergy effect – as one feature changes there is a larger or smaller effect of the other feature. In linear regression, interactions can be captured via products of features (i.e., \\(x_1 \\times x_2\\)). A model with two main effects can also include a two-way interaction. For example, to include an interaction between \\(x_1 =\\) Gr_Liv_Area and \\(x_2 =\\) Year_Built, we introduce an additional product term: \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1 x_2. \\end{equation}\\] Note that in R, we use the : operator to include an interaction (technically, we could use * as well, but x1 * x2 is shorthand for x1 + x2 + x1:x2 so is slightly redundant): interaction_model &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train) tidy(interaction_model) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -810498. 210537. -3.85 1.22e- 4 ## 2 Gr_Liv_Area -729. 127. -5.75 1.01e- 8 ## 3 Year_Built 431. 107. 4.03 5.83e- 5 ## 4 Gr_Liv_Area:Year_Built 0.417 0.0642 6.49 1.04e-10 In this example, we see that the two main effects (Gr_Liv_Area &amp; Year_Built) are statistically significant and so is the interaction term (Gr_Liv_Area:Year_Built). So how do we interpret these results? Well, we can say that for every 1 additional square feet in Gr_Liv_Area, the Sale_Price of a home increases by \\(b_1 + b_3 \\times \\text{Year_Built}\\) = -728.5084 + 0.4168489 x Year_Built. Likewise, for each additional year that a home was built, the Sale_Price of a home increases by \\(b_2 + b_3 \\times \\text{Gr_Liv_Area}\\) = 430.8755 + 0.4168489 x Gr_Liv_Area. Adding an interaction term now makes the change in one variable non-linear because it includes an additional change based on another feature. This non-linearity (or curvature) that interactions capture can be illustrated in the contour plot below. The left plot illustrates a regression model with main effects only. Note how the fitted regression surface is flat (i.e., it does not twist or bend). While the fitted regression surface with interaction is displayed in the right side plot and you can see the curvature of the relationship induced. Figure 6.2: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The ‘best-fit’ plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane). Interaction effects are quite prevalent in predictive modeling. Since linear models are an example of parametric modeling, it is up to the analyst to decide if and when to include interaction effects. This becomes quite tedious and unrealistic for larger data sets. In later lessons, we’ll discuss algorithms that can automatically detect and incorporate interaction effects (albeit in different ways). For now, just realize that adding interactions is possible with MLR models. 6.5 Qualitative predictors In our discussion so far, we have assumed that all variables in our linear regression model are quantitative. But in practice, this is not necessarily the case; often some predictors are qualitative. For example, the Credit data set provided by the ISLR package records the balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating). credit &lt;- as_tibble(ISLR::Credit) credit ## # A tibble: 400 × 12 ## ID Income Limit Rating Cards Age Education Gender Student ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 14.9 3606 283 2 34 11 &quot; Male&quot; No ## 2 2 106. 6645 483 3 82 15 &quot;Female&quot; Yes ## 3 3 105. 7075 514 4 71 11 &quot; Male&quot; No ## 4 4 149. 9504 681 3 36 11 &quot;Female&quot; No ## 5 5 55.9 4897 357 2 68 16 &quot; Male&quot; No ## 6 6 80.2 8047 569 4 77 10 &quot; Male&quot; No ## 7 7 21.0 3388 259 2 37 12 &quot;Female&quot; No ## 8 8 71.4 7114 512 2 87 9 &quot; Male&quot; No ## 9 9 15.1 3300 266 5 66 13 &quot;Female&quot; No ## 10 10 71.1 6819 491 3 41 19 &quot;Female&quot; Yes ## # ℹ 390 more rows ## # ℹ 3 more variables: Married &lt;fct&gt;, Ethnicity &lt;fct&gt;, Balance &lt;int&gt; Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. For example, based on the gender, we can create a new variable that takes the form \\[ x_i = \\Bigg\\{ \\genfrac{}{}{0pt}{}{1 \\hspace{.5cm}\\text{ if }i\\text{th person is female}\\hspace{.25cm}}{0 \\hspace{.5cm}\\text{ if }i\\text{th person is male}} \\] and use this variable as a predictor in the regression equation. This results in the model \\[ y_i = b_0 + b_1x_i = \\Bigg\\{ \\genfrac{}{}{0pt}{}{b_0 + b_1 \\hspace{.5cm}\\text{ if }i\\text{th person is female}\\hspace{.3cm}}{b_0 \\hspace{1.5cm}\\text{ if }i\\text{th person is male}} \\] Now \\(b_0\\) can be interpreted as the average credit card balance among males, \\(b_0 + b_1\\) as the average credit card balance among females, and \\(b_1\\) as the average difference in credit card balance between females and males. We can produce this model in R using the same syntax as we saw earlier: qual_model &lt;- linear_reg() %&gt;% fit(Balance ~ Gender, data = credit) tidy(qual_model) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 510. 33.1 15.4 2.91e-42 ## 2 GenderFemale 19.7 46.1 0.429 6.69e- 1 The results above suggest that males are estimated to carry $509.80 in credit card debt where females carry $509.80 + $19.73 = $529.53. The decision to code males as 0 and females as 1 is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we want to change the reference variable (the variable coded as 0) we can change the factor levels. credit$Gender &lt;- factor(credit$Gender, levels = c(&quot;Female&quot;, &quot; Male&quot;)) qual_model &lt;- linear_reg() %&gt;% fit(Balance ~ Gender, data = credit) tidy(qual_model) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 530. 32.0 16.6 3.31e-47 ## 2 Gender Male -19.7 46.1 -0.429 6.69e- 1 A similar process ensues for qualitative predictor categories with more than two levels. For instance, if we go back to our Ames housing data we’ll see that there is a Neighborhood variable. In our data there are 28 different neighborhoods. ames_train %&gt;% count(Neighborhood) ## # A tibble: 28 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 North_Ames 306 ## 2 College_Creek 199 ## 3 Old_Town 164 ## 4 Edwards 131 ## 5 Somerset 122 ## 6 Northridge_Heights 116 ## 7 Gilbert 111 ## 8 Sawyer 108 ## 9 Northwest_Ames 82 ## 10 Sawyer_West 94 ## # ℹ 18 more rows Most people are aware that different neighborhoods can generate significantly different home prices than other neighborhoods. In this data we can visualize this by looking at the distribution of Sale_Price across neighborhoods. We see that the Stone Brook neighborhood has the highest average sale price whereas Meadow Village has the lowest. This could be for many reasons (i.e. age of the neighborhood, amenities provided by the neighborhood, proximity to undesirable things such as manufacturing plants). ggplot(ames_train, aes(fct_reorder(Neighborhood, Sale_Price), Sale_Price)) + geom_boxplot() + xlab(NULL) + scale_y_continuous(&quot;Sale Price&quot;, labels = scales::dollar) + coord_flip() So, naturally, we can assume there is some relationship between Neighborhood and Sale_Price. We can assess this relationship by running the following model. Based on the results we see that the reference Neighborhood (which is North Ames based on levels(ames_train$Neighborhood)) has an average Sale_Price of $143,516.75 (based on the intercept). Whereas College Creek has a Sale_Price of $143,516.75 + $57,006.75 = $200,523.50. The p.value for College Creek is very small suggesting that this difference between North Ames and College Creek is statistically significant. However, look at the results for the Sawyer neighborhood. The coefficient suggests that the average Sale_Price for the Sawyer neighborhood is $143,516.75 - $4,591.68 = $148,108.40. However, the p.value is 0.43 which suggests that there is no statistical difference between the reference neighborhood (North Ames) and Sawyer. neighborhood_model &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Neighborhood, data = ames_train) tidy(neighborhood_model) ## # A tibble: 28 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 143517. 2996. 47.9 0 ## 2 NeighborhoodCollege_Creek 57007. 4773. 11.9 8.04e- 32 ## 3 NeighborhoodOld_Town -19609. 5072. -3.87 1.14e- 4 ## 4 NeighborhoodEdwards -10196. 5472. -1.86 6.26e- 2 ## 5 NeighborhoodSomerset 87794. 5612. 15.6 3.68e- 52 ## 6 NeighborhoodNorthridge_Heigh… 177986. 5715. 31.1 2.85e-174 ## 7 NeighborhoodGilbert 44653. 5807. 7.69 2.30e- 14 ## 8 NeighborhoodSawyer -4592. 5866. -0.783 4.34e- 1 ## 9 NeighborhoodNorthwest_Ames 44635. 6518. 6.85 9.87e- 12 ## 10 NeighborhoodSawyer_West 40081. 6181. 6.48 1.11e- 10 ## # ℹ 18 more rows 6.5.1 Knowledge check The Ames housing data has an Overall_Qual variable that measures the overall quality of a home (Very Poor, Poor, …, Excellent, Very Excellent). Plot the relationship between Sale_Price and the Overall_Qual variable. Does there look to be a relationship between the quality of a home and its sale price? Model this relationship with a simple linear regression model. Interpret the coefficients. 6.6 Including many predictors In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with p distinct predictors is \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_p x_p, \\end{equation}\\] where \\(x_i\\) for \\(i = 1, 2, \\dots, p\\) are the predictors of interest. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. The code below creates a model where we use all features in our data set as main effects (i.e., no interaction terms) to predict Sale_Price. However, note that we remove a few variables first. This is because these variables introduce some new problems that require us to do some feature engineering steps. We’ll discuss this in a future lesson but for now we’ll just put these feature variables to the side. # remove some trouble variables trbl_vars &lt;- c(&quot;MS_SubClass&quot;, &quot;Condition_2&quot;, &quot;Exterior_1st&quot;, &quot;Exterior_2nd&quot;, &quot;Misc_Feature&quot;) ames_train &lt;- ames_train %&gt;% select(-trbl_vars) # include all possible main effects model3 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ ., data = ames_train) # print estimated coefficients in a tidy data frame tidy(model3) ## # A tibble: 249 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.76e+7 1.22e+7 -1.44 1.49e-1 ## 2 MS_ZoningResidential_High_Dens… 6.16e+3 8.84e+3 0.697 4.86e-1 ## 3 MS_ZoningResidential_Low_Densi… 3.69e+2 5.64e+3 0.0655 9.48e-1 ## 4 MS_ZoningResidential_Medium_De… 9.51e+2 6.40e+3 0.149 8.82e-1 ## 5 MS_ZoningA_agr -5.26e+4 5.44e+4 -0.966 3.34e-1 ## 6 MS_ZoningC_all -1.55e+4 1.01e+4 -1.53 1.25e-1 ## 7 MS_ZoningI_all -1.98e+4 2.70e+4 -0.736 4.62e-1 ## 8 Lot_Frontage -1.67e+1 2.07e+1 -0.807 4.20e-1 ## 9 Lot_Area 5.89e-1 1.08e-1 5.47 5.24e-8 ## 10 StreetPave 1.52e+3 1.05e+4 0.145 8.85e-1 ## # ℹ 239 more rows You’ll notice that our model’s results includes the intercept plus 248 predictor variable coefficients. However, our ames_train data only includes 75 predictor variables after removing those 5 troublesome variables! What gives? ames_train %&gt;% select(-Sale_Price) %&gt;% dim() ## [1] 2049 75 The reason is that 41 of our predictor variables are qualitative and many of these include several levels. So the dummy encoding procedure discussed in the last section causes us to have many more coefficients than initial predictor variables. ames_train %&gt;% select_if(is.factor) %&gt;% colnames() ## [1] &quot;MS_Zoning&quot; &quot;Street&quot; &quot;Alley&quot; ## [4] &quot;Lot_Shape&quot; &quot;Land_Contour&quot; &quot;Utilities&quot; ## [7] &quot;Lot_Config&quot; &quot;Land_Slope&quot; &quot;Neighborhood&quot; ## [10] &quot;Condition_1&quot; &quot;Bldg_Type&quot; &quot;House_Style&quot; ## [13] &quot;Overall_Qual&quot; &quot;Overall_Cond&quot; &quot;Roof_Style&quot; ## [16] &quot;Roof_Matl&quot; &quot;Mas_Vnr_Type&quot; &quot;Exter_Qual&quot; ## [19] &quot;Exter_Cond&quot; &quot;Foundation&quot; &quot;Bsmt_Qual&quot; ## [22] &quot;Bsmt_Cond&quot; &quot;Bsmt_Exposure&quot; &quot;BsmtFin_Type_1&quot; ## [25] &quot;BsmtFin_Type_2&quot; &quot;Heating&quot; &quot;Heating_QC&quot; ## [28] &quot;Central_Air&quot; &quot;Electrical&quot; &quot;Kitchen_Qual&quot; ## [31] &quot;Functional&quot; &quot;Fireplace_Qu&quot; &quot;Garage_Type&quot; ## [34] &quot;Garage_Finish&quot; &quot;Garage_Qual&quot; &quot;Garage_Cond&quot; ## [37] &quot;Paved_Drive&quot; &quot;Pool_QC&quot; &quot;Fence&quot; ## [40] &quot;Sale_Type&quot; &quot;Sale_Condition&quot; If we wanted to assess which features have a relationship we could easily filter our model results to find which coefficients have p.values less than 0.05. In this model we see that 67 (68 minus the intercept) features have a statistical relationship with Sale_Price. tidy(model3) %&gt;% filter(p.value &lt; 0.05) ## # A tibble: 68 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lot_Area 5.89e-1 0.108 5.47 5.24e-8 ## 2 Lot_ShapeModerately_Irregular 1.03e+4 3759. 2.74 6.21e-3 ## 3 Land_ContourHLS 1.62e+4 4288. 3.78 1.61e-4 ## 4 Land_ContourLvl 1.09e+4 3040. 3.57 3.61e-4 ## 5 Lot_ConfigCulDSac 6.08e+3 2930. 2.07 3.82e-2 ## 6 Lot_ConfigFR2 -8.00e+3 3563. -2.25 2.49e-2 ## 7 Land_SlopeSev -3.42e+4 11287. -3.03 2.48e-3 ## 8 NeighborhoodCollege_Creek 2.66e+4 10478. 2.54 1.13e-2 ## 9 NeighborhoodSomerset 2.31e+4 6180. 3.73 1.95e-4 ## 10 NeighborhoodNorthridge_Heights 3.19e+4 6159. 5.19 2.37e-7 ## # ℹ 58 more rows How does our model with all available predictors perform? We can compute the generalization error to assess. model3 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 22959. Not too shabby! Using all the predictor variables in our model has drastically reduced our test RMSE! 6.7 Feature importance Ok, so we found a linear regression model that performs pretty good compared to the other linear regression models we trained. Our next goal is often to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the lesson, this constant rate of change is provided by the coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables? Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the t-statistic for each model parameter used. Rather than search through each of the variables to compare their t-statistic values, we can use vip::vip() to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). The plot below illustrates the top 20 most influential variables. We see that the top 4 most important variables have to do with roofing material followed by the total square footage on the second floor. # plot top 10 influential features model3 %&gt;% vip::vip(num_features = 20) This is basically saying that our model finds that these are the most influential features in our data set that have the largest impact on the predicted outcome. If we order our data based on the t-statistic we see similar results. tidy(model3) %&gt;% arrange(desc(statistic)) ## # A tibble: 249 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Roof_MatlWdShngl 663237. 43673. 15.2 4.08e-49 ## 2 Roof_MatlCompShg 591744. 41975. 14.1 6.74e-43 ## 3 Roof_MatlWdShake 580841. 43901. 13.2 3.28e-38 ## 4 Roof_MatlTar&amp;Grv 585872. 44342. 13.2 4.08e-38 ## 5 Second_Flr_SF 60.4 4.80 12.6 6.62e-35 ## 6 Roof_MatlRoll 603645. 48760. 12.4 7.56e-34 ## 7 Roof_MatlMembran 642865. 52863. 12.2 9.20e-33 ## 8 Roof_MatlMetal 636083. 52812. 12.0 3.42e-32 ## 9 First_Flr_SF 43.0 4.16 10.3 2.63e-24 ## 10 NeighborhoodGreen_Hills 159547. 19970. 7.99 2.39e-15 ## # ℹ 239 more rows 6.8 Exercises Using the Boston housing data set where the response feature is the median value of homes within a census tract (cmedv): Split the data into 70-30 training-test sets. Train an MLR model that includes all the predictor variables. Assess and interpret the coefficients. Are all predictor variables statistically significant? Explain why or why not. What is the generalization error of this model? Which features are most influential in this model and which features are not? "],["overview-2.html", "7 Overview 7.1 Learning objectives 7.2 Estimated time requirement 7.3 Tasks", " 7 Overview The last several lessons gave you a good introduction to building predictive regression models using the Tidymodels construct. However, we haven’t discussed how to build predictive classification models. The next module will introduce logistic regression, which is very similar to linear regression but for classification problems. But before we create our first classification model we’re going to introduce two new steps into our modeling process – feature engineering to make our feature variables more relevant to modeling and resampling procedures to a more accurate and robust generalization error. 7.1 Learning objectives By the end of this module you should be able to: Make your feature variables more relevant to modeling with feature engineering. Have a more accurate and robust generalization error for your model by using resampling procedures. 7.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 2-3 hours Reading + videos: 3-4 hours 7.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-3a-feature-engineering.html", "8 Lesson 3a: Feature engineering 8.1 Learning objectives 8.2 Prerequisites 8.3 Create a recipe 8.4 Numeric features 8.5 Categorical features 8.6 Fit a model with a recipe 8.7 Exercises", " 8 Lesson 3a: Feature engineering Data preprocessing and engineering techniques generally refer to the addition, deletion, or transformation of data. In this lesson we introduce you to another tidymodels package, recipes, which is designed to help you preprocess your data before training your model. Recipes are built as a series of preprocessing steps, such as: converting qualitative predictors to indicator variables (also known as dummy variables), transforming data to be on a different scale (e.g., taking the logarithm of a variable), transforming whole groups of predictors together, extracting key features from raw variables (e.g., getting the day of the week out of a date variable), and so on. Although there is an ever-growing number of ways to preprocess your data, we’ll focus on a few common feature engineering steps applied to numeric and categorical data. This will provide you with a foundation of how to perform feature engineering. 8.1 Learning objectives By the end of this lesson you’ll be able to: Explain how to apply feature engineering steps with the recipes package. Filter out low informative features. Normalize and standardize numeric features. Pre-process nominal and ordinal features. Combine multiple feature engineering steps into one recipe and train a model with it. 8.2 Prerequisites For this lesson we’ll use the recipes package with is automatically loaded with tidymodels and we’ll use the ames.csv housing data. library(tidymodels) library(tidyverse) ames_data_path &lt;- here::here(&quot;data&quot;, &quot;ames.csv&quot;) ames &lt;- readr::read_csv(ames_data_path) Let’s go ahead and create our train-test split: # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) ames_train &lt;- training(split) ames_test &lt;- testing(split) 8.3 Create a recipe To get started, let’s create a model recipe that we will build upon and apply in a model downstream. Before training the model, we can use a recipe to create and/or modify predictors and conduct some preprocessing required by the model. ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) Similar to the fit() function you’ve already seen, the recipe() function has two primary arguments: A formula: Any variable on the left-hand side of the tilde (~) is considered the response variable (here, aSale_Price). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors. The data: A recipe is associated with the data set used to create the model. This should always be the training set, so data = ames_train here. Now we can start adding feature engineering steps onto our recipe using the pipe operator and applying specific feature engineering tasks. The sections that follow provide some discussion as to why we apply each feature engineering step and then we demonstrate how to add it to our recipe. 8.4 Numeric features Pretty much all algorithms will work out of the box with numeric features. However, some algorithms make some assumptions regarding the distribution of our features. If we look at some of our numeric features, we can see that they span across different ranges: ames_train %&gt;% select_if(is.numeric) ## # A tibble: 2,051 × 35 ## Lot_Frontage Lot_Area Year_Built Year_Remod_Add Mas_Vnr_Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 81 11216 2006 2006 0 ## 2 0 2998 2000 2000 513 ## 3 0 17871 1995 1996 0 ## 4 85 10574 2005 2006 0 ## 5 50 6000 1939 1950 0 ## 6 35 4251 2006 2007 0 ## 7 65 8125 1994 1998 258 ## 8 80 8480 1947 1950 0 ## 9 60 7200 1951 1951 0 ## 10 0 12735 1972 1972 0 ## # ℹ 2,041 more rows ## # ℹ 30 more variables: BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_SF_2 &lt;dbl&gt;, ## # Bsmt_Unf_SF &lt;dbl&gt;, Total_Bsmt_SF &lt;dbl&gt;, First_Flr_SF &lt;dbl&gt;, ## # Second_Flr_SF &lt;dbl&gt;, Low_Qual_Fin_SF &lt;dbl&gt;, Gr_Liv_Area &lt;dbl&gt;, ## # Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;, Full_Bath &lt;dbl&gt;, ## # Half_Bath &lt;dbl&gt;, Bedroom_AbvGr &lt;dbl&gt;, Kitchen_AbvGr &lt;dbl&gt;, ## # TotRms_AbvGrd &lt;dbl&gt;, Fireplaces &lt;dbl&gt;, Garage_Cars &lt;dbl&gt;, … Moreover, sometimes our numeric features are skewed or contain outliers. For example, our Gr_Liv_Area feature is skewed right where most values are centered around 1200-1500 square feet of living space but there several larger than normal homes (i.e. Gr_Liv_Area &gt; 3000). In fact, if we look at our scatter plot we can see that the right tail of Gr_Liv_Area values is where the largest residuals are. p1 &lt;- ggplot(ames_train, aes(Gr_Liv_Area)) + geom_histogram(bins = 100) p2 &lt;- ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) gridExtra::grid.arrange(p1, p2, nrow = 1) We can incorporate feature engineering steps to help address both of the above concerns. 8.4.1 Standardizing We should always consider the scale on which the individual features are measured. What are the largest and smallest values across all features and do they span several orders of magnitude? Models that incorporate smooth functions of input features are sensitive to the scale of the inputs. For example, \\(5X+2\\) is a simple linear function of the input X, and the scale of its output depends directly on the scale of the input. Many algorithms use linear functions within their algorithms, some more obvious (e.g., ordinary least squares and regularized regression) than others (e.g., neural networks, support vector machines, and principal components analysis). Other examples include algorithms that use distance measures such as the Euclidean distance (e.g., k nearest neighbor, k-means clustering, and hierarchical clustering). For these models and modeling components, it is often a good idea to standardize the features. Standardizing features includes centering and scaling so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Whether or not a machine learning model requires normalization of the features depends on the model family. Linear models such as logistic regression generally benefit from scaling the features while other models such as tree-based models (i.e. decision trees, random forests) do not need such preprocessing (but will not suffer from it). Figure 8.1: Standardizing features allows all features to be compared on a common value scale regardless of their real value differences. We can standardize our numeric features in one of two ways – note that step_normalize() is just a wrapper that combines step_center() and step_scale(). # option 1 std &lt;- ames_recipe %&gt;% step_center(all_numeric_predictors()) %&gt;% step_scale(all_numeric_predictors()) # option 2 std &lt;- ames_recipe %&gt;% step_normalize(all_numeric_predictors()) Note how we did not specify an individual variable within our step_xxx() functions. You can certain do that if you wanted to. For example, if you just wanted to standardize the Gr_Liv_Area and Year_Built feature you could with: std &lt;- ames_recipe %&gt;% step_normalize(Gr_Liv_Area, Year_Built) However, instead we used selectors to apply this recipe step to all the numeric features at once using all_nominal_predictors(). The selector functions can be combined to select intersections of variables just like we learned several weeks ago with the dplyr::select() function. The result of the above code is not to actually apply the feature engineering steps but, rather, to create an object that holds the feature engineering logic (or recipe) to be applied later on. 8.4.2 Normalizing Parametric models that have distributional assumptions can benefit from minimizing the skewness of numeric features. For instance, ordinary linear regression models assume that the prediction errors are normally distributed. However, sometimes when certain features or even the response variable has heavy tails (i.e., outliers) or is skewed in one direction or the other, this normality assumption will likely not hold. There are two main approaches to help correct for skewed feature variables: Option 1: normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. ames_recipe %&gt;% step_log(all_numeric_predictors()) However, if your feature(s) have negative values or zeros then a log transformation will produce NaNs and -Infs, respectively (you cannot take the logarithm of a negative number). If the non positive response values are small (say between -0.99 and 0) then you can apply a small offset such as in log1p() which adds 1 to the value prior to applying a log transformation (you can do the same within step_log() by using the offset argument). log(-0.5) ## Warning in log(-0.5): NaNs produced ## [1] NaN log1p(-0.5) ## [1] -0.6931472 If your data consists of values \\(\\leq -1\\), use the Yeo-Johnson transformation mentioned next. Option 2: use a Box Cox transformation. A Box Cox transformation is more flexible than (but also includes as a special case) the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution (Box and Cox 1964; Carroll and Ruppert 1981). At the core of the Box Cox transformation is an exponent, lambda (\\(\\lambda\\)), which varies from -5 to 5. All values of \\(\\lambda\\) are considered and the optimal value for the given data is estimated from the training data; The “optimal value” is the one which results in the best transformation to an approximate normal distribution. The transformation of the response \\(Y\\) has the form: \\[ \\begin{equation} y(\\lambda) = \\begin{cases} \\frac{Y^\\lambda-1}{\\lambda}, &amp; \\text{if}\\ \\lambda \\neq 0 \\\\ \\log\\left(Y\\right), &amp; \\text{if}\\ \\lambda = 0. \\end{cases} \\end{equation} \\] If your response has negative values, the Yeo-Johnson transformation is very similar to the Box-Cox but does not require the input variables to be strictly positive. We can normalize our numeric predictor variables with step_YeoJohnson(): # normalize with Box Cox norm_bc &lt;- ames_recipe %&gt;% step_BoxCox(all_numeric_predictors()) # Normalize with Yeo Johnson norm_yj &lt;- ames_recipe %&gt;% step_YeoJohnson(all_numeric_predictors()) 8.4.3 Knowledge check Using the boston_train data, fill in the blanks to create a recipe that… Models cmedv as a function of all predictors, standardizes all numeric features, Normalizes all all numeric features with a Yeo-Johnson transformation boston_recipe &lt;- recipe(_______ ~ _______, data = boston_train) %&gt;% step_______(___________) %&gt;% step_______(___________) 8.5 Categorical features Most models require that the predictors take numeric form. There are exceptions; for example, some tree-based models naturally handle numeric or categorical features. However, even tree-based models can benefit from pre-processing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features. 8.5.1 One-hot &amp; dummy encoding There are many ways to recode categorical variables as numeric. The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding the left data frame in the below figure results in X being converted into three columns, one for each level. This is called less than full rank encoding . However, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level c has been dropped). This is referred to as dummy encoding. Figure 8.2: Eight observations containing a categorical feature X and the difference in how one-hot and dummy encoding transforms this feature. We can use step_dummy() to add a one-hot or dummy encoding to our recipe: # one-hot encode ohe &lt;- ames_recipe %&gt;% step_dummy(all_nominal(), one_hot = TRUE) # dummy encode de &lt;- ames_recipe %&gt;% step_dummy(all_nominal(), one_hot = FALSE) Recall our lesson on applying an ordinary least squares regression model to a categorical feature. Behind the scenes R was automatically dummy encoding our categorical feature; however, many (dare I say most) algorithms will not automatically do that for us. 8.5.2 Ordinal encoding If a categorical feature is naturally ordered then numerically encoding the feature based on its order is a natural choice (most commonly referred to as ordinal encoding). For example, the various quality features in the Ames housing data are ordinal in nature (ranging from Very_Poor to Very_Excellent). ames_train %&gt;% select(matches(&#39;Qual$|QC$|_Cond$&#39;)) ## # A tibble: 2,051 × 11 ## Overall_Qual Overall_Cond Exter_Qual Exter_Cond Bsmt_Qual Bsmt_Cond ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Very_Good Average Good Typical Good Good ## 2 Above_Average Average Good Typical Good Typical ## 3 Below_Average Average Typical Typical Good Typical ## 4 Very_Good Average Good Typical Good Typical ## 5 Above_Average Good Typical Typical Typical Good ## 6 Good Average Good Typical Good Typical ## 7 Above_Average Average Typical Typical Good Typical ## 8 Average Average Typical Typical Typical Typical ## 9 Average Good Typical Typical Fair Typical ## 10 Below_Average Average Typical Typical Typical Typical ## # ℹ 2,041 more rows ## # ℹ 5 more variables: Heating_QC &lt;chr&gt;, Kitchen_Qual &lt;chr&gt;, ## # Garage_Qual &lt;chr&gt;, Garage_Cond &lt;chr&gt;, Pool_QC &lt;chr&gt; Ordinal encoding these features provides a natural and intuitive interpretation and can logically be applied to all models. If your features are already ordered factors then you can simply apply step_ordinalscore() to ordinal encode: ord &lt;- ames_recipe %&gt;% step_ordinalscore(matches(&#39;Qual$|QC$|_Cond$&#39;)) However, if we look at our quality features we see they are characters instead of factors and their levels are not ordered. Moreover, some have a unique value that represents that feature doesn’t exist in the house (i.e. No_Basement). ames_train %&gt;% pull(Bsmt_Qual) %&gt;% unique() ## [1] &quot;Good&quot; &quot;Typical&quot; &quot;Fair&quot; &quot;Excellent&quot; ## [5] &quot;No_Basement&quot; &quot;Poor&quot; So in this case we’re going to apply several feature engineering steps to: convert quality features to factors with specified levels, convert any missed levels (i.e. No_Basement, No_Pool) to “None”, convert factor level to integer value (‘Very_Poor’ = 1, ‘Poor’ = 2, …, ‘Excellent’ = 10, ‘Very_Excellent’ = 11) # specify levels in order lvls &lt;- c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Typical&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;) # apply ordinal encoding to quality features ord_lbl &lt;- ames_recipe %&gt;% # 1. convert quality features to factors with specified levels step_string2factor(matches(&#39;Qual$|QC$|_Cond$&#39;), levels = lvls, ordered = TRUE) %&gt;% # 2. convert any missed levels (i.e. No_Basement, No_Pool) to &quot;None&quot; step_unknown(matches(&#39;Qual$|QC$|_Cond$&#39;), new_level = &quot;None&quot;) %&gt;% # 3. convert factor level to integer value step_ordinalscore(matches(&#39;Qual$|QC$|_Cond$&#39;)) Did this work, let’s take a look. If we want to apply a recipe to a data set we can apply: prep: estimate feature engineering parameters based on training data. bake: apply the prepped recipe to new, or the existing (new_data = NULL) data. We can see that our quality variables are now ordinal encoded. baked_recipe &lt;- ord_lbl %&gt;% prep(strings_as_factor = FALSE) %&gt;% bake(new_data = NULL) baked_recipe %&gt;% select(matches(&#39;Qual$|QC$|_Cond$&#39;)) ## # A tibble: 2,051 × 11 ## Overall_Qual Overall_Cond Exter_Qual Exter_Cond Bsmt_Qual Bsmt_Cond ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 9 5 8 6 8 8 ## 2 7 5 8 6 8 6 ## 3 4 5 6 6 8 6 ## 4 9 5 8 6 8 6 ## 5 7 8 6 6 6 8 ## 6 8 5 8 6 8 6 ## 7 7 5 6 6 8 6 ## 8 5 5 6 6 6 6 ## 9 5 8 6 6 3 6 ## 10 4 5 6 6 6 6 ## # ℹ 2,041 more rows ## # ℹ 5 more variables: Heating_QC &lt;int&gt;, Kitchen_Qual &lt;int&gt;, ## # Garage_Qual &lt;int&gt;, Garage_Cond &lt;int&gt;, Pool_QC &lt;int&gt; And if we want to see how the numeric values are mapped to the original data we can. Here, I just focus on the original Overall_Qual values and how they compare to the encoded values. encoded_Overall_Qual &lt;- baked_recipe %&gt;% select(Overall_Qual) %&gt;% rename(encoded_Overall_Qual = Overall_Qual) ames_train %&gt;% select(Overall_Qual) %&gt;% bind_cols(encoded_Overall_Qual) %&gt;% count(Overall_Qual, encoded_Overall_Qual) ## # A tibble: 10 × 3 ## Overall_Qual encoded_Overall_Qual n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Above_Average 7 509 ## 2 Average 5 587 ## 3 Below_Average 4 168 ## 4 Excellent 10 65 ## 5 Fair 3 30 ## 6 Good 8 419 ## 7 Poor 2 9 ## 8 Very_Excellent 11 26 ## 9 Very_Good 9 235 ## 10 Very_Poor 1 3 8.5.3 Lumping Sometimes features will contain levels that have very few observations. For example, there are 28 unique neighborhoods represented in the Ames housing data but several of them only have a few observations. ## # A tibble: 28 × 2 ## Neighborhood n ## &lt;chr&gt; &lt;int&gt; ## 1 Green_Hills 1 ## 2 Landmark 1 ## 3 Greens 6 ## 4 Blueste 8 ## 5 Northpark_Villa 16 ## 6 Veenker 18 ## 7 Briardale 20 ## 8 Bloomington_Heights 22 ## 9 Meadow_Village 26 ## 10 Clear_Creek 29 ## # ℹ 18 more rows Sometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 1% of the training sample into an “other” category. We can use step_other() to do so. However, lumping should be used sparingly as there is often a loss in model performance (Kuhn and Johnson 2013). Tree-based models often perform exceptionally well with high cardinality features and are not as impacted by levels with small representation. The following lumps all neighborhoods that represent less than 1% of observations into an “other” category. # Lump levels for two features rare_encoder &lt;- ames_recipe %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) 8.5.4 Knowledge check Using the Ames data, fill in the blanks to ordinal encode the Overall_Cond variable and lump all Neighborhoods that represent less than 1% of observations into an “other” category. # specify levels in order lvls &lt;- c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;) cat_encode &lt;- ames_recipe %&gt;% # 1. convert quality features to factors with specified levels step_string2factor(_______, levels = ___, ordered = ____) %&gt;% # 2. convert factor level to integer value step_ordinalscore(______) %&gt;% # 3. lump novel neighborhood levels step_other(______, threshold = ____, other = &quot;other&quot;) 8.6 Fit a model with a recipe Alright, so we know how to do a little feature engineering, now let’s put it to use. Recall that we are using the ames.csv data rather than the AmesHousing::make_ames() data. Because of this if we were to try train a multiple linear regression model on all the predictors we would get an error when applying that model to unseen data (i.e. ames_test). This is because there are some rare levels in some of the categorical features. linear_reg() %&gt;% fit(Sale_Price ~ ., data = ames_train) %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor Roof_Matl has new levels Metal, Roll If we remove all the categorical features with rare levels we can train a model and apply it to our test data. Note that our result differs from the last lesson because we are using ames.csv rather than the AmesHousing::make_ames() data and there are some subtle differences between those data sets. trbl_vars &lt;- c(&quot;MS_SubClass&quot;, &quot;Condition_2&quot;, &quot;Exterior_1st&quot;, &quot;Exterior_2nd&quot;, &quot;Misc_Feature&quot;, &quot;Roof_Matl&quot;, &quot;Electrical&quot;, &quot;Sale_Type&quot;) ames_smaller_train &lt;- ames_train %&gt;% select(-trbl_vars) linear_reg() %&gt;% fit(Sale_Price ~ ., data = ames_smaller_train) %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 31296. Unfortunately, we dropped the troublesome categorical features which may actually have some useful information in them. Also, we may be able to improve performance by applying other feature engineering steps. Let’s combine some feature engineering tasks into one recipe and then train a model with it. final_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_nzv(all_predictors(), unique_cut = 10) %&gt;% step_YeoJohnson(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_string2factor(matches(&#39;Qual$|QC$|_Cond$&#39;), levels = lvls, ordered = TRUE) %&gt;% step_unknown(matches(&#39;Qual$|QC$|_Cond$&#39;), new_level = &quot;None&quot;) %&gt;% step_integer(matches(&#39;Qual$|QC$|_Cond$&#39;)) %&gt;% step_other(all_nominal_predictors(), threshold = 0.01, other = &quot;other&quot;) We will want to use our recipe across several steps as we train and test our model. We will: Process the recipe using the training set: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors will have zero-variance in the training set, and should be slated for removal, what the mean and scale is in order to standardize the numeric features, etc. Apply the recipe to the training set: We create the final predictor set on the training set. Apply the recipe to the test set: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the feature engineering statistics computed from the training set are applied to the test set. To simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the workflows package from tidymodels to bundle our model with our recipe (ames_recipe). mlr_wflow &lt;- workflow() %&gt;% add_model(linear_reg()) %&gt;% add_recipe(final_recipe) mlr_wflow ## ══ Workflow ═══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ─────────────────────────────────────────────────────── ## 7 Recipe Steps ## ## • step_nzv() ## • step_YeoJohnson() ## • step_normalize() ## • step_string2factor() ## • step_unknown() ## • step_integer() ## • step_other() ## ## ── Model ────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Now, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors: mlr_fit &lt;- mlr_wflow %&gt;% fit(data = ames_train) The rf_fit object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions extract_fit_parsnip() and extract_recipe(). mlr_fit %&gt;% extract_fit_parsnip() %&gt;% tidy() ## # A tibble: 163 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 168020. 29181. 5.76 9.92e-9 ## 2 MS_SubClassOne_and_Half_Story_… -460. 26648. -0.0173 9.86e-1 ## 3 MS_SubClassOne_Story_1945_and_… 10808. 26368. 0.410 6.82e-1 ## 4 MS_SubClassOne_Story_1946_and_… 2180. 25692. 0.0849 9.32e-1 ## 5 MS_SubClassOne_Story_PUD_1946_… -18599. 28321. -0.657 5.11e-1 ## 6 MS_SubClassSplit_Foyer 7735. 27007. 0.286 7.75e-1 ## 7 MS_SubClassSplit_or_Multilevel 9553. 29836. 0.320 7.49e-1 ## 8 MS_SubClassTwo_Family_conversi… 7485. 6934. 1.08 2.81e-1 ## 9 MS_SubClassTwo_Story_1945_and_… 9382. 26359. 0.356 7.22e-1 ## 10 MS_SubClassTwo_Story_1946_and_… -1677. 25542. -0.0657 9.48e-1 ## # ℹ 153 more rows Just as in the last lesson, we can make predictions with our fit model object and evaluate the model performance. Here, we compute the RMSE on our ames_test data and we see we have a better performance than the previous model that had no feature engineering steps. mlr_fit %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 29729. 8.6.1 Knowledge check Using the boston_train data, fill in the blanks to train a model with the given recipe… The recipe should model cmedv as a function of all predictors, standardizes all numeric features, Normalizes all all numeric features with a Yeo-Johnson transformation # 1. Create recipe boston_recipe &lt;- recipe(_____ ~ _____, data = boston_train) %&gt;% step______(_____) %&gt;% step______(_____) # 2. Create a workflow object mlr_wflow &lt;- workflow() %&gt;% add_model(_____) %&gt;% add_recipe(_____) # 3. Fit and evaluate our model like we did before mlr_fit &lt;- mlr_wflow %&gt;% fit(data = boston_train) mlr_fit %&gt;% predict(boston_test) %&gt;% bind_cols(boston_test %&gt;% select(cmedv)) %&gt;% rmse(truth = cmedv, estimate = .pred) 8.7 Exercises Identify three new feature engineering steps that are provided by recipes. Why would these feature engineering steps be applicable to the Ames data? Using the boston.csv data, create a multiple linear regression model using all predictors in their current state. Now apply feature engineering steps to standardize and normalize the numeric features prior to modeling. Does the model performance improve? Using the AmesHousing::make_ames() rebuild the model we created in this section. However, rather than remove the trouble variables, apply a feature engineering step to lump novel levels together so that you can use those variables as predictors. Does the model performance improve? References Box, George EP, and David R Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological), 211–52. Carroll, Raymond J, and David Ruppert. 1981. “On Prediction and the Power Transformation Family.” Biometrika 68 (3): 609–15. Kuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Vol. 26. Springer. "],["lesson-3b-resampling.html", "9 Lesson 3b: Resampling 9.1 Learning objectives 9.2 Prerequisites 9.3 Resampling &amp; cross-validation 9.4 K-fold cross-validation 9.5 Bootstrap resampling 9.6 Alternative methods 9.7 Exercises", " 9 Lesson 3b: Resampling The last several lessons gave you a good introduction to building predictive models using the tidymodels construct. And as we trained our models we evaluated their performance on the test set, which we called the generalization error. However, the approach we’ve taken thus far to evaluate the generalization error can have some pitfalls. This lesson is going to go deeper into the idea of model evaluation and we’ll discuss how to incorporate resampling procedures to give you a more robust assessment of model performance. 9.1 Learning objectives By the end of this lesson you will be able to: Explain the reasoning for resampling procedures and when/why we should incorporate them into our ML workflow. Implement k-fold cross-validation procedures for more robust model evaluation. Implement bootstrap resampling procedures for more robust model evaluation. 9.2 Prerequisites This lesson leverages the following packages and data. library(tidymodels) ames &lt;- AmesHousing::make_ames() Let’s go ahead and create our train-test split: # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7, strata = Sale_Price) ames_train &lt;- training(split) ames_test &lt;- testing(split) 9.3 Resampling &amp; cross-validation In the previous lessons we split our data into a train and test set and we assessed the performance of our model on the test set. If we use a little feature engineering to take care of novel categorical levels, we see that our generalization RMSE based on the test set is $26,144. mlr_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_other(all_nominal_predictors(), threshold = 0.02, other = &quot;other&quot;) mlr_wflow &lt;- workflow() %&gt;% add_model(linear_reg()) %&gt;% add_recipe(mlr_recipe) mlr_fit &lt;- mlr_wflow %&gt;% fit(data = ames_train) mlr_fit %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 26144. Unfortunately, there are a few pitfalls to this approach: If our dataset is small, a single test set may not provide realistic expectations of our model’s performance on unseen data. A single test set does not provide us any insight on variability of our model’s performance. Using our test set to drive our model building process can bias our results via data leakage. Basically, the more we use the test data to assess various model performances, the less likely the test data is behaving like true, unseen data. It is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process. Resampling methods provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts of the training data. Figure 9.1: Illustration of resampling. (Kuhn, Max n.d.) This allows us to train and validate our model entirely on the training data and not touch the test data until we have selected a final “optimal” model. The two most commonly used resampling methods include k-fold cross-validation and bootstrap sampling. 9.4 K-fold cross-validation Cross-validation consists of repeating the procedure such that the training and testing sets are different each time. Generalization performance metrics are collected for each repetition and then aggregated. As a result we can get an estimate of the variability of the model’s generalization performance. k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. Figure 9.2: Illustration of k-fold sampling across a data sets index. The model is fit on \\(k-1\\) folds and then the remaining fold is used to compute model performance. This procedure is repeated k times; each time, a different fold is treated as the validation set. Consequently, with k-fold CV, every observation in the training data will be held out one time to be included in the assessment/validation set. This process results in k estimates of the generalization error (say \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging the k test errors, providing us with an approximation of the error we might expect on unseen data. Figure 9.3: Illustration of a 5-fold cross validation procedure. In practice, one typically uses k=5 or k=10. There is no formal rule as to the size of k; however, as k gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease. To implement k-fold CV we first make a resampling object. In this example we create a 10-fold resampling object. set.seed(35) kfolds &lt;- vfold_cv(ames_train, v = 10, strata = Sale_Price) We can now create our multiple linear regression workflow object as we did previously and fit our model across our 10-folds; we just use fit_resamples() rather than fit(). mlr_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_other(all_nominal_predictors(), threshold = 0.03, other = &quot;other&quot;) mlr_wflow &lt;- workflow() %&gt;% add_model(linear_reg()) %&gt;% add_recipe(mlr_recipe) # fit our model across the 10-fold CV mlr_fit_cv &lt;- mlr_wflow %&gt;% fit_resamples(kfolds) We can then get our average 10-fold cross validation error with collect_metrics(): collect_metrics(mlr_fit_cv) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 32547. 10 2455. Preprocessor1_Model1 ## 2 rsq standard 0.839 10 0.0237 Preprocessor1_Model1 If we want to see the model evaluation metric (i.e. RMSE) for each fold we just need to include summarize = FALSE. collect_metrics(mlr_fit_cv, summarize = FALSE) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 10 × 5 ## id .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Fold01 rmse standard 36485. Preprocessor1_Model1 ## 2 Fold02 rmse standard 37722. Preprocessor1_Model1 ## 3 Fold03 rmse standard 47136. Preprocessor1_Model1 ## 4 Fold04 rmse standard 24794. Preprocessor1_Model1 ## 5 Fold05 rmse standard 27203. Preprocessor1_Model1 ## 6 Fold06 rmse standard 28096. Preprocessor1_Model1 ## 7 Fold07 rmse standard 22781. Preprocessor1_Model1 ## 8 Fold08 rmse standard 36901. Preprocessor1_Model1 ## 9 Fold09 rmse standard 26510. Preprocessor1_Model1 ## 10 Fold10 rmse standard 37841. Preprocessor1_Model1 If we compare these results to our previous generalization error based on the test set, we see some differences. We now see that our average generalization RMSE across the ten holdout sets is $32,547 and individual validation set RMSEs range from $24,794 - $47,136. This provides us a little more robust expectations around how our model will perform on future unseen data. And it actually shows us that our single test set RMSE may have been biased and too optimistic! One other item to note - often, people assume that when creating the k-folds that R just selects the first n observations to be the first fold, the next n observations to be in the second fold, etc. However, this is not the case. Rather, R will randomly assign observations to each fold but will also ensure that each observation is only assigned to a single fold for validation purposes. The following illustrates a 10-fold cross validation on a dataset with 32 observations. Each observation is used once for validation and nine times for training. Figure 9.4: 10-fold cross validation on 32 observations. Each observation is used once for validation and nine times for training. 9.4.1 Knowledge check Using the boston.csv data… Fill in the blanks to create a multiple linear regression model using all predictors (without any feature engineering); however, rather than computing the generalization error using the test data, apply 10-fold cross-validation. What is the average cross-validation RMSE? What is the range of cross-validation RMSE values across all ten folds? # 1. Create a k-fold object set.seed(123) kfolds &lt;- vfold_cv(______, v = __, strata = cmedv) # 2. Create our workflow object mlr_recipe &lt;- recipe(cmedv ~ ___, data = boston_train) mlr_wflow &lt;- workflow() %&gt;% add_model(______) %&gt;% add_recipe(______) # 3. Fit our model on the k-fold object mlr_fit_cv &lt;- mlr_wflow %&gt;% fit_______(______) # 4. Assess our average cross validation error collect_______(______) 9.5 Bootstrap resampling A bootstrap sample is a random sample of the data taken with replacement (Efron and Tibshirani 1986). This means that, after a data point is selected for inclusion in the subset, it’s still available for further selection. A bootstrap sample is the same size as the original data set from which it was constructed. Figure 9.5 provides a schematic of bootstrap sampling where each bootstrap sample contains 12 observations just as in the original data set. Furthermore, bootstrap sampling will contain approximately the same distribution of values (represented by colors) as the original data set. Figure 9.5: Illustration of the bootstrapping process. Since samples are drawn with replacement, each bootstrap sample is likely to contain duplicate values. In fact, on average, \\(\\approx 63.21\\)% of the original sample ends up in any particular bootstrap sample. The original observations not contained in a particular bootstrap sample are considered out-of-bag (OOB). When bootstrapping, a model can be built on the selected samples and validated on the OOB samples; this is often done, for example, in random forests (which we’ll discuss in a later module). Since observations are replicated in bootstrapping, there tends to be less variability in the error measure compared with k-fold CV (Efron 1983). However, this can also increase the bias of your error estimate (we’ll discuss the concept of bias versus variance in a future module). This can be problematic with smaller data sets; however, for most average-to-large data sets (say \\(n \\geq 1,000\\)) this concern is often negligible. Figure 9.6 compares bootstrapping to 10-fold CV on a small data set with \\(n = 32\\) observations. A thorough introduction to the bootstrap and its use in R is provided in Davison, Hinkley, et al. (1997). Figure 9.6: Bootstrap sampling (left) versus 10-fold cross validation (right) on 32 observations. For bootstrap sampling, the observations that have zero replications (white) are the out-of-bag observations used for validation. We can create bootstrap samples easily with bootstraps(), as illustrated in the code chunk below. set.seed(35) bs_samples &lt;- bootstraps(ames_train, times = 10, strata = Sale_Price) Once we’ve created our bootstrap samples we can reuse the same mlr_wflow object we created earlier and refit it with the bootstrap samples. Our results again show that our average resampling RMSE is $38,765 – again, much higher than the single test set RMSE we obtained previously. # fit our model across the bootstrapped samples mlr_fit_bs &lt;- mlr_wflow %&gt;% fit_resamples(bs_samples) collect_metrics(mlr_fit_bs) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 38765. 9 1885. Preprocessor1_Model1 ## 2 rsq standard 0.780 9 0.0157 Preprocessor1_Model1 Bootstrapping is, typically, more of an internal resampling procedure that is naturally built into certain ML algorithms. This will become more apparent in later modules where we discuss bagging and random forests. 9.5.1 Knowledge check Using the boston.csv data… Fill in the blanks to create a multiple linear regression model using all predictors (without any feature engineering); however, apply 10 bootstrap samples to compute the cross-validation RMSE. What is the average cross-validation RMSE? What is the range of cross-validation RMSE values across all ten folds? How do the results compare to the 10-fold cross validation you performed earlier? # 1. Create a bootstrap object set.seed(123) bs_samples &lt;- bootstraps(______, times = ___, strata = ____) #&lt;&lt; # 2. Create our workflow object mlr_recipe &lt;- recipe(____ ~ __, data = boston_train) mlr_wflow &lt;- workflow() %&gt;% add_model(______) %&gt;% add_recipe(______) # 3. Fit our model on the bootstrap object mlr_fit_cv &lt;- mlr_wflow %&gt;% fit_______(______) # 4. Assess our average cross validation error ______ 9.6 Alternative methods It is important to note that there are other useful resampling procedures. If you’re working with time-series specific data then you will want to incorporate rolling origin and other time series resampling procedures. Hyndman and Athanasopoulos (2018) is the dominant, R-focused, time series resource3. Additionally, Efron (1983) developed the “632 method” and Efron and Tibshirani (1997) discuss the “632+ method”; both approaches seek to minimize biases experienced with bootstrapping on smaller data sets. Other methods exist but K-fold cross validation and bootstrapping are the dominant methods used and are often sufficient. 9.7 Exercises Use the Advertising.csv data to complete the following tasks. The Advertising.csv data contains three predictor variables - TV, Radio, and Newspaper, which represents a companies advertising budget for these respective mediums across 200 metropolitan markets. It also contains the response variable - Sales, which represents the total sales in thousands of units for a given product. Split the data into 70-30 training-test sets. Using 10-fold cross-validation, create a multiple linear regression model where Sales is a function of all three predictors (without any feature engineering). What is the average cross-validation RMSE? What is the range of cross-validation RMSE values across all ten folds? Using boostrap resampling with 10 bootstrap samples, create a multiple linear regression model where Sales is a function of all three predictors (without any feature engineering). What is the average bootstrap RMSE? What is the range of bootstrap RMSE values across all ten bootstrap samples? References Davison, Anthony Christopher, David Victor Hinkley, et al. 1997. Bootstrap Methods and Their Application. Vol. 1. Cambridge University Press. Efron, Bradley. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association 78 (382): 316–31. Efron, Bradley, and Robert Tibshirani. 1986. “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.” Statistical Science, 54–75. ———. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548–60. Hyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts. Kuhn, Max. n.d. “Tidymodels.” https://www.tidymodels.org/start/resampling/. See their open source book at https://www.otexts.org/fpp2↩︎ "],["overview-3.html", "10 Overview 10.1 Learning objectives 10.2 Estimated time requirement 10.3 Tasks", " 10 Overview The last several lessons gave you a good introduction to building predictive regression models using the Tidymodels construct. However, we haven’t discussed how to build predictive classification models. This module will introduce logistic regression, which is very similar to linear regression but for classification problems. In addition, this module introduces regularized regression, which is a slight twist on both linear and logistic regression. 10.1 Learning objectives By the end of this module you should be able to: Explain how a logistic regression model characterizes the data it is applied to. Fit, interpret, and assess the performance of simple and multiple logistic regression models. 10.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 2-3 hours Reading + videos: 3-4 hours 10.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-4a-logistic-regression.html", "11 Lesson 4a: Logistic Regression 11.1 Learning objectives 11.2 Prerequisites 11.3 Why logistic regression 11.4 Simple logistic regression 11.5 Interpretation 11.6 Multiple logistic regression 11.7 Assessing model accuracy 11.8 Cross-validation performance 11.9 Feature interpretation 11.10 Final thoughts 11.11 Exercises", " 11 Lesson 4a: Logistic Regression Linear regression is used to approximate the (linear) relationship between a continuous response variable and a set of predictor variables. However, when the response variable is binary (i.e., Yes/No), linear regression is not appropriate. Fortunately, analysts can turn to an analogous method, logistic regression, which is similar to linear regression in many ways. This module explores the use of logistic regression for binary response variables. Logistic regression can be expanded for problems where the response variable has more than two categories (i.e. High/Medium/Low, Flu/Covid/Allergies, Win/Lose/Tie); however, that goes beyond our intent here (see Faraway (2016a) for discussion of multinomial logistic regression in R). 11.1 Learning objectives By the end of this module you will: Understand why linear regression does not work for binary response variables. Know how to apply and interpret simple and multiple logistic regression models. Know how to assess model accuracy of various logistic regression models. 11.2 Prerequisites For this section we’ll use the following packages: # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) # Model interpretability packages library(vip) # variable importance To illustrate logistic regression concepts we’ll use the employee attrition data, where our intent is to predict the Attrition response variable (coded as \"Yes\"/\"No\"). As in the previous module, we’ll set aside 30% of our data as a test set to assess our generalizability error. churn_data_path &lt;- here::here(&quot;data&quot;, &quot;attrition.csv&quot;) churn &lt;- read_csv(churn_data_path) # Recode response variable as a factor churn &lt;- mutate(churn, Attrition = factor(Attrition)) # Create training (70%) and test (30%) sets set.seed(123) # for reproducibility churn_split &lt;- initial_split(churn, prop = .7, strata = &quot;Attrition&quot;) churn_train &lt;- training(churn_split) churn_test &lt;- testing(churn_split) 11.3 Why logistic regression To provide a clear motivation for logistic regression, assume we have credit card default data for customers and we want to understand if the current credit card balance of a customer is an indicator of whether or not they’ll default on their credit card. To classify a customer as a high- vs. low-risk defaulter based on their balance we could use linear regression; however, the left plot below illustrates how linear regression would predict the probability of defaulting. Unfortunately, for balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1. These inconsistencies only increase as our data become more imbalanced and the number of outliers increase. Contrast this with the logistic regression line (right plot) that is nonlinear (sigmoidal-shaped). Figure 11.1: Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1. To avoid the inadequacies of the linear model fit on a binary response, we must model the probability of our response using a function that gives outputs between 0 and 1 for all values of \\(x\\). Many functions meet this description. In logistic regression, we use the logistic function, which is defined as the following equation and produces the S-shaped curve in the right plot above. \\[\\begin{equation} p\\left(x\\right) = \\frac{e^{b_0 + b_1x}}{1 + e^{b_0 + b_1x}} \\end{equation}\\] The \\(b_i\\) parameters represent the coefficients as in linear regression and \\(p\\left(x\\right)\\) may be interpreted as the probability that the positive class (default in the above example) is present. The minimum for \\(p\\left(x\\right)\\) is obtained at \\(\\lim_{a \\rightarrow -\\infty} \\left[ \\frac{e^a}{1+e^a} \\right] = 0\\), and the maximum for \\(p\\left(x\\right)\\) is obtained at \\(\\lim_{a \\rightarrow \\infty} \\left[ \\frac{e^a}{1+e^a} \\right] = 1\\) which restricts the output probabilities to 0–1. Rearranging the above equation yields the logit transformation (which is where logistic regression gets its name): \\[\\begin{equation} g\\left(x\\right) = \\ln \\left[ \\frac{p\\left(x\\right)}{1 - p\\left(x\\right)} \\right] = b_0 + b_1 x \\end{equation}\\] Applying a logit transformation to \\(p\\left(x\\right)\\) results in a linear equation similar to the mean response in a simple linear regression model. Using the logit transformation also results in an intuitive interpretation for the magnitude of \\(b_1\\): the odds (e.g., of defaulting) increase multiplicatively by \\(\\exp\\left(b_1\\right)\\) for every one-unit increase in \\(x\\). A similar interpretation exists if \\(x\\) is categorical; see Agresti (2003), Chapter 5, for more details. This may seem overwhelming but don’t worry, the meaning of this math and how you interpret the predictor variable coefficients will become clearer later in this lesson. Your main take-away is that logistic regression: Models the probability of our response using a function that restricts this probability to be between 0 and 1. The coefficients of our logistic regression represent the multiplicative increase in the odds of our response variable for every one-unit increase in \\(x\\). 11.4 Simple logistic regression We will fit two logistic regression models in order to predict the probability of an employee attriting. The first predicts the probability of attrition based on their monthly income (MonthlyIncome) and the second is based on whether or not the employee works overtime (OverTime). To simplify the code we do not run cross validation procedures. We will in a later section but for now we simply want to get a grasp of interpreting a logistic regression model. We use logistic_reg() for our model object. Note that we do not set the engine for this model type. This means we will use the default engine, which is the glm package (see ?logistic_reg for details). lr_mod &lt;- logistic_reg() # model 1 lr_fit1 &lt;- lr_mod %&gt;% fit(Attrition ~ MonthlyIncome, data = churn_train) # model 2 lr_fit2 &lt;- lr_mod %&gt;% fit(Attrition ~ OverTime, data = churn_train) 11.5 Interpretation Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a log-odds (i.e., logit) scale. Unlike, linear regression, this can make interpretation of coefficients difficult. At a macro level, larger coefficients suggest that that feature increases the odds of the response more than smaller valued coefficients. However, for our purpose, it is easier to focus on the interpretation of the output. The following predicts the probability of employee attrition based on the two models we fit. So in this example, for the first observation our model predicts there is a 78% probability that the employee will not leave and a 22% probability that they will. lr_fit1 %&gt;% predict(churn_train, type = &quot;prob&quot;) ## # A tibble: 1,028 × 2 ## .pred_No .pred_Yes ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.784 0.216 ## 2 0.778 0.222 ## 3 0.779 0.221 ## 4 0.834 0.166 ## 5 0.772 0.228 ## 6 0.813 0.187 ## 7 0.784 0.216 ## 8 0.906 0.0937 ## 9 0.954 0.0463 ## 10 0.807 0.193 ## # ℹ 1,018 more rows Let’s use these predictions to assess how our logistic regression model views the relationship between our predictor variables and the response variable. In the first plot, we can see that as MonthlyIncome increases, the predicted probability of attrition decreases from a little over 0.25 to 0.025. If we look at the second plot, which plots the predicted probability of Attrition based on whether employees work OverTime. We can see that employees that work overtime have a 0.3 probability of attrition while those that don’t work overtime only have a 0.1 probability of attrition. Basically, working overtimes increases the probability of employee churn by a factor of 3! p1 &lt;- lr_fit1 %&gt;% predict(churn_train, type = &quot;prob&quot;) %&gt;% mutate(MonthlyIncome = churn_train$MonthlyIncome) %&gt;% ggplot(aes(MonthlyIncome, .pred_Yes)) + geom_point(alpha = .2) + scale_y_continuous(&quot;Probability of Attrition&quot;, limits = c(0, 1)) + ggtitle(&quot;Predicted probabilities for lr_fit1&quot;) p2 &lt;- lr_fit2 %&gt;% predict(churn_train, type = &quot;prob&quot;) %&gt;% mutate(OverTime = churn_train$OverTime) %&gt;% ggplot(aes(OverTime, .pred_Yes, color = OverTime)) + geom_boxplot(show.legend = FALSE) + geom_rug(sides = &quot;b&quot;, position = &quot;jitter&quot;, alpha = 0.2, show.legend = FALSE) + scale_y_continuous(&quot;Probability of Attrition&quot;, limits = c(0, 1)) + ggtitle(&quot;Predicted probabilities for lr_fit2&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 11.2: Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, lr_fit1 predicts a decreased probability of attrition and if employees work overtime lr_fit2 predicts an increased probability. The table below shows the coefficient estimates and related information that result from fitting a logistic regression model in order to predict the probability of Attrition = Yes for our two models. Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a log-odds (i.e., logit) scale. For lr_fit1, the estimated coefficient for MonthlyIncome is -0.000139, which is negative, indicating that an increase in MonthlyIncome is associated with a decrease in the probability of attrition. Similarly, for lr_fit2, employees who work OverTime are associated with an increased probability of attrition compared to those that do not work OverTime. tidy(lr_fit1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.886 0.157 -5.64 0.0000000174 ## 2 MonthlyIncome -0.000139 0.0000272 -5.10 0.000000344 tidy(lr_fit2) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.13 0.119 -17.9 1.46e-71 ## 2 OverTimeYes 1.29 0.176 7.35 2.01e-13 As discussed earlier, it is easier to interpret the coefficients using an exp() transformation: exp(coef(lr_fit1$fit)) ## (Intercept) MonthlyIncome ## 0.4122647 0.9998614 exp(coef(lr_fit2$fit)) ## (Intercept) OverTimeYes ## 0.1189759 3.6323389 Thus, the odds of an employee attriting in lr_fit1 increase multiplicatively by 0.9999 for every one dollar increase in MonthlyIncome, whereas the odds of attriting in lr_fit2 increase multiplicatively by 3.632 for employees that work OverTime compared to those that do not. Many aspects of the logistic regression output are similar to those discussed for linear regression. For example, we can use the estimated standard errors to get confidence intervals as we did for linear regression. confint(lr_fit1$fit) # for odds, you can use `exp(confint(model1))` ## 2.5 % 97.5 % ## (Intercept) -1.1932606571 -5.761048e-01 ## MonthlyIncome -0.0001948723 -8.803311e-05 confint(lr_fit2$fit) ## 2.5 % 97.5 % ## (Intercept) -2.3695727 -1.902409 ## OverTimeYes 0.9463761 1.635373 11.5.1 Knowledge check Load the spam data set from the kernlab package with the code below. Review the data documentation with ?spam. What is the response variable? Split the data into a train and test set using a 70-30 split. Pick a single predictor variable and apply a simple logistic regression model that models the type variable as a function of that predictor variable. Interpret the feature’s coefficient install.packages(&quot;kernlab&quot;) library(kernlab) data(spam) 11.6 Multiple logistic regression We can also extend our model as seen in our earlier equation so that we can predict a binary response using multiple predictors: \\[\\begin{equation} p\\left(X\\right) = \\frac{e^{b_0 + b_1 x_1 + \\cdots + b_p x_p }}{1 + e^{b_0 + b_1 x_1 + \\cdots + b_p x_p}} \\end{equation}\\] Let’s go ahead and fit a model that predicts the probability of Attrition based on the MonthlyIncome and OverTime. Our results show that both features have an impact on employee attrition; however, working OverTime tends to nearly double the probability of attrition! # model 3 lr_fit3 &lt;- lr_mod %&gt;% fit(Attrition ~ MonthlyIncome + OverTime, data = churn_train) tidy(lr_fit3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.33 0.177 -7.54 4.74e-14 ## 2 MonthlyIncome -0.000147 0.0000280 -5.27 1.38e- 7 ## 3 OverTimeYes 1.35 0.180 7.50 6.59e-14 lr_fit3 %&gt;% predict(churn_train, type = &quot;prob&quot;) %&gt;% mutate( MonthlyIncome = churn_train$MonthlyIncome, OverTime = churn_train$OverTime ) %&gt;% ggplot(aes(MonthlyIncome, .pred_Yes, color = OverTime)) + geom_point(alpha = 0.5, size = 0.8) + scale_y_continuous(&quot;Probability of Attrition&quot;, limits = c(0, 1)) + ggtitle(&quot;Predicted probabilities for lr_fit3&quot;) 11.6.1 Knowledge check Using the same spam data as in the previous Knowledge check… Pick two predictor variables and apply a logistic regression model that models the type variable as a function of these predictor variables. Interpret the features’s coefficients 11.7 Assessing model accuracy With a basic understanding of logistic regression under our belt, similar to linear regression our concern now shifts to how well our models predict. As discussed in lesson 1b, there are multiple metrics we can use for classification models. 11.7.1 Accuracy The most basic classification metric is accuracy rate. This answers the question - “Overall, how often is the classifier correct?” Our objective is to maximize this metric. We can compute the accuracy with accuracy(). Here, we compute our error metric with the test data but we’ll show shortly how to do so using resampling procedures as we learned about in the last lesson. # accuracy of our first model lr_fit1 %&gt;% predict(churn_test) %&gt;% bind_cols(churn_test %&gt;% select(Attrition)) %&gt;% accuracy(truth = Attrition, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.837 Unfortunately, the accuracy rate only tells us how many predictions were correct versus not. Sometimes we may want to understand more behind how our predictions are correct vs incorrect. To do so we can use… 11.7.2 Confusion matrix When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted an employee would leave and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. an employee that we did not predict to leave does). We can get the confusion matrix for a given fit model with conf_mat(). These results provide us with some very useful information. For lr_fit1, our model is predicting “No” for every observation! lr_fit1 %&gt;% predict(churn_test) %&gt;% bind_cols(churn_test %&gt;% select(Attrition)) %&gt;% conf_mat(truth = Attrition, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 370 72 ## Yes 0 0 For a better example, let’s train a model with all predictor variables. Now when we create our confusion matrix we see that our model predicts yes’s and no’s. lr_fit4 &lt;- lr_mod %&gt;% fit(Attrition ~ ., data = churn_train) lr_fit4 %&gt;% predict(churn_test) %&gt;% bind_cols(churn_test %&gt;% select(Attrition)) %&gt;% conf_mat(truth = Attrition, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 353 38 ## Yes 17 34 Using a confusion matrix can allow us to extract different meaningful metrics for our binary classifier. For example, given the above confusion matrix we can assess the following: Our model does far better at accurately predicting when an employee is not going to leave (Prediction = No &amp; Truth = No) versus accurately predicting when an employee is going to leave (Prediction = Yes &amp; Truth = Yes). When our model is inaccurate, we can see that it is from more false negatives (Prediction = No but Truth = Yes) than false positives (Prediction = Yes but Truth = No). This means our model is biased towards predicting that an employee is going to stay when in fact they end up leaving. When assessing model performance for a classification model, understanding how our model errors with false negatives and false positives is important. This information can be helpful to decision makers. Often, we want a classifier that has high accuracy and does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. One way to capture this is with area under the curve. 11.7.3 Area under the curve Area under the curve (AUC) is an approach that plots the false positive rate along the x-axis and the true positive rate along the y-axis. Technically, we call this plot the Receiver Operator Curve (ROC). A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. The AUC metric computes the area under the ROC; so our objective is to maximize the AUC value. Figure 11.3: ROC curve. For our logistic regression model we can get the ROC curve with the following: Notice how the y-axis is called “sensitivity” and the x-axis is “1=specificity”. These are just alternative terms referring to “true positive rate” and “false positive rate” respectively. lr_fit4 %&gt;% predict(churn_train, type = &quot;prob&quot;) %&gt;% mutate(truth = churn_train$Attrition) %&gt;% roc_curve(truth, .pred_No) %&gt;% autoplot() And we can compute the AUC metric with roc_auc(): lr_fit4 %&gt;% predict(churn_train, type = &quot;prob&quot;) %&gt;% mutate(truth = churn_train$Attrition) %&gt;% roc_auc(truth, .pred_No) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.886 11.7.4 Knowledge check Using the same spam data as in the previous Knowledge check… Apply a logistic regression model that models the type variable as a function of all predictor variables. Assess the accuracy of this model on the test data. Assess and interpret the confusion matrix results for this model on the test data. Plot the ROC curve for this model and compute the AUC (using the test data). 11.8 Cross-validation performance However, recall that our previous models were not based on cross validation procedures. If we re-perform our analysis using a 5-fold cross validation procedure we see that our average AUC metric is lower than the test set. This is probably a better indicator on how our model will perform, on average, across new data. # create resampling procedure set.seed(123) kfold &lt;- vfold_cv(churn_train, v = 5) # train model via cross validation results &lt;- fit_resamples(lr_mod, Attrition ~ ., kfold) # average AUC collect_metrics(results) %&gt;% filter(.metric == &quot;roc_auc&quot;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 roc_auc binary 0.830 5 0.0153 Preprocessor1_Model1 # AUC across all folds collect_metrics(results, summarize = FALSE) %&gt;% filter(.metric == &quot;roc_auc&quot;) ## # A tibble: 5 × 5 ## id .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Fold1 roc_auc binary 0.811 Preprocessor1_Model1 ## 2 Fold2 roc_auc binary 0.801 Preprocessor1_Model1 ## 3 Fold3 roc_auc binary 0.811 Preprocessor1_Model1 ## 4 Fold4 roc_auc binary 0.884 Preprocessor1_Model1 ## 5 Fold5 roc_auc binary 0.845 Preprocessor1_Model1 11.8.1 Knowledge check Using the same spam data as in the previous Knowledge checks… Apply a logistic regression model that models the type variable as a function of all predictor variables. Use a 10-fold cross validation procedure to compute the mean generalization AUC. 11.9 Feature interpretation Similar to linear regression, once our preferred logistic regression model is identified, we need to interpret how the features are influencing the results. As with normal linear regression models, variable importance for logistic regression models can be computed using the absolute value of the \\(z\\)-statistic for each coefficient. Using vip::vip() we can extract our top 20 influential variables. The following illustrates that for lr_fit4, working overtime is the most influential followed by the frequency of business travel and the distance from home that the employees office is. vip(lr_fit4$fit, num_features = 20) 11.9.1 Knowledge check Using the same spam data as in the previous Knowledge checks… Apply a logistic regression model that models the type variable as a function of all predictor variables. Use vip() to plot the top 20 most influential features. 11.10 Final thoughts Logistic regression provides an alternative to linear regression for binary classification problems. However, similar to linear regression, logistic regression suffers from the many assumptions involved in the algorithm (i.e. linear relationship of the coefficient, multicollinearity). Moreover, often we have more than two classes to predict which is commonly referred to as multinomial classification. Although multinomial extensions of logistic regression exist, the assumptions made only increase and, often, the stability of the coefficient estimates (and therefore the accuracy) decrease. Future modules will discuss more advanced algorithms that provide a more natural and trustworthy approach to binary and multinomial classification prediction. 11.11 Exercises Use the titanic.csv dataset available via Canvas for these exercise questions. This dataset contains information on the fate of 1,043 passengers on the fatal maiden voyage of the ocean liner ‘Titanic’. Our interest is in predicting whether or not a passenger survived. Because the variable we want to predict is binary (survived = Yes if the passenger survived and survived = No if they did not), logistic regression is appropriate. For this session we’ll assess three predictor variables: pclass, sex, and age. survived (dependent variable) = Yes if the passenger survived and No if they did not pclass (predictor variable) = the economic class of the passenger summarized as 1st, 2nd, 3rd, and Crew class. sex (predictor variable) = reported sex of the passenger (“male”, “female”) age (predictor variable) = age of passenger. Note that ages &lt;1 are for infants. Complete the following tasks: Using stratified sampling split the data into a training set and a test set. Use a 70/30 split (70% for the training data &amp; 30% for the test set). Apply a logistic regression model using all predictor variables and perform a 10-fold cross validation procedure. What is the mean cross-validation generalization error? Identify the influence of the three predictor variables. References Agresti, Alan. 2003. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley. Faraway, Julian J. 2016a. Extending the Linear Model with r: Generalized Linear, Mixed Effects and Nonparametric Regression Models. Vol. 124. CRC press. "],["lesson-4b-regularized-regression.html", "12 Lesson 4b: Regularized Regression 12.1 Learning objectives 12.2 Prerequisites 12.3 Why regularize? 12.4 Ridge penalty 12.5 Lasso penalty 12.6 Elastic nets 12.7 Implementation 12.8 Tuning our model 12.9 Feature importance 12.10 Classification problems 12.11 Final thoughts 12.12 Exercises", " 12 Lesson 4b: Regularized Regression Linear models (LMs) provide a simple, yet effective, approach to predictive modeling. Moreover, when certain assumptions required by LMs are met (e.g., constant variance), the estimated coefficients are unbiased and, of all linear unbiased estimates, have the lowest variance. However, in today’s world, data sets being analyzed typically contain a large number of features. As the number of features grow, certain assumptions typically break down and these models tend to overfit the training data, causing our generalization error to increase. Regularization methods provide a means to constrain or regularize the estimated coefficients, which can reduce the variance and decrease the generalization error. 12.1 Learning objectives By the end of this module you will know: Why reguralization is important. How to apply ridge, lasso, and elastic net regularized models. Extract and visualize the most influential features. 12.2 Prerequisites # Helper packages library(tidyverse) # general data munging &amp; visualization # Modeling packages library(tidymodels) # Model interpretability packages library(vip) # for variable importance # Stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 12.3 Why regularize? The easiest way to understand regularized regression is to explain how and why it is applied to ordinary least squares (OLS). The objective in OLS regression is to find the hyperplane (e.g., a straight line in two dimensions) that minimizes the sum of squared errors (SSE) between the observed and predicted response values (see Figure below). This means identifying the hyperplane that minimizes the grey lines, which measure the vertical distance between the observed (red dots) and predicted (blue line) response values. Figure 12.1: Figure: Fitted regression line using Ordinary Least Squares. More formally, the objective function being minimized can be written as: \\[\\begin{equation} \\text{minimize} \\left( SSE = \\sum^n_{i=1} \\left(y_i - \\hat{y}_i\\right)^2 \\right) \\end{equation}\\] Recall that we have use the following terms interchangably: Sum of squared errors (SSE) Residual sum of squares (RSS) However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model. Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results. Linear relationship: Linear regression assumes a linear relationship between the predictor and the response variable. More observations than predictors: Although not an issue with the Ames housing data, when the number of features exceeds the number of observations (\\(p &gt; n\\)), the OLS estimates are not obtainable. No or little multicollinearity: Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in OLS, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This obviously leads to an inaccurate interpretation of coefficients and makes it difficult to identify influential predictors. There are other assumptions that ordinary least squares regression makes that are often violated with larger data sets. Many real-life data sets, like those common to text mining and genomic studies are wide, meaning they contain a larger number of features (\\(p &gt; n\\)). As p increases, we’re more likely to violate some of the OLS assumptions, which can cause poor model performance. Consequently, alternative algorithms should be considered. Having a large number of features invites additional issues in using classic regression models. For one, having a large number of features makes the model much less interpretable. Additionally, when \\(p &gt; n\\), there are many (in fact infinite) solutions to the OLS problem! In such cases, it is useful (and practical) to assume that a smaller subset of the features exhibit the strongest effects (something called the bet on sparsity principle (see Hastie, Tibshirani, and Wainwright 2015, 2).). For this reason, we sometimes prefer estimation techniques that incorporate feature selection. One approach to this is called hard thresholding feature selection, which includes many of the traditional linear model selection approaches like forward selection and backward elimination. These procedures, however, can be computationally inefficient, do not scale well, and treat a feature as either in or out of the model (hence the name hard thresholding). In contrast, a more modern approach, called soft thresholding, slowly pushes the effects of irrelevant features toward zero, and in some cases, will zero out entire coefficients. As will be demonstrated, this can result in more accurate models that are also easier to interpret. With wide data (or data that exhibits multicollinearity), one alternative to OLS regression is to use regularized regression (also commonly referred to as penalized models or shrinkage methods as in J. Friedman, Hastie, and Tibshirani (2001) and Kuhn and Johnson (2013)) to constrain the total size of all the coefficient estimates. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model (at the expense of no longer being unbiased—a reasonable compromise). The objective function of a regularized regression model is similar to OLS, albeit with a penalty term \\(P\\). \\[\\begin{equation} \\text{minimize} \\left( SSE + P \\right) \\end{equation}\\] This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). This concept generalizes to all GLM models (e.g., logistic and Poisson regression) and even some survival models. So far, we have been discussing OLS and the sum of squared errors loss function. However, different models within the GLM family have different loss functions (see Chapter 4 of J. Friedman, Hastie, and Tibshirani (2001)). Yet we can think of the penalty parameter all the same—it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model’s loss function. Regularized regression constrains the size of the predictor variable coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model’s loss function. Constraining coefficients actually creates more stable models, which can improve model performance and generalization. There are three common penalty parameters we can implement: Ridge; Lasso (or LASSO); Elastic net (or ENET), which is a combination of ridge and lasso. 12.4 Ridge penalty Ridge regression (Hoerl and Kennard 1970) controls the estimated coefficients by adding \\(\\lambda \\sum^p_{j=1} \\beta_j^2\\) to the objective function. \\[\\begin{equation} \\text{minimize } \\left( SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\right) \\end{equation}\\] The size of this penalty, referred to as \\(L^2\\) (or Euclidean) norm, can take on a wide range of values, which is controlled by the tuning parameter \\(\\lambda\\). When \\(\\lambda = 0\\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \\(\\lambda \\rightarrow \\infty\\), the penalty becomes large and forces the coefficients toward zero (but not all the way). This is illustrated below where exemplar coefficients have been regularized with \\(\\lambda\\) ranging from 0 to over 8,000. Figure 12.2: Figure: Ridge regression coefficients for 15 exemplar predictor variables as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). As \\(\\lambda\\) grows larger, our coefficient magnitudes are more constrained. Although these coefficients were scaled and centered prior to the analysis, you will notice that some are quite large when \\(\\lambda\\) is near zero. Furthermore, you’ll notice that feature x1 has a large negative parameter that fluctuates until \\(\\lambda \\approx 7\\) where it then continuously shrinks toward zero. This is indicative of multicollinearity and likely illustrates that constraining our coefficients with \\(\\lambda &gt; 7\\) may reduce the variance, and therefore the error, in our predictions. In essence, the ridge regression model pushes many of the correlated features toward each other rather than allowing for one to be wildly positive and the other wildly negative. In addition, many of the less-important features also get pushed toward zero. This helps to provide clarity in identifying the important signals in our data. However, ridge regression does not perform feature selection and will retain all available features in the final model. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity). If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable. 12.5 Lasso penalty The lasso (least absolute shrinkage and selection operator) penalty (Tibshirani 1996) is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the \\(L^2\\) norm for an \\(L^1\\) norm: \\(\\lambda \\sum^p_{j=1} | \\beta_j|\\): \\[\\begin{equation} \\text{minimize } \\left( SSE + \\lambda \\sum^p_{j=1} | \\beta_j | \\right) \\end{equation}\\] Whereas the ridge penalty pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients all the way to zero as illustrated in below. Switching to the lasso penalty not only improves the model but it also conducts automated feature selection. Figure 12.3: Figure: Lasso regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). In the figure above we see that when \\(\\lambda &lt; 0.01\\) all 15 variables are included in the model, when \\(\\lambda \\approx 0.5\\) 9 variables are retained, and when \\(log\\left(\\lambda\\right) = 1\\) only 5 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal. 12.6 Elastic nets A generalization of the ridge and lasso penalties, called the elastic net (Zou and Hastie 2005), combines the two penalties: \\[\\begin{equation} \\text{minimize } \\left( SSE + \\lambda_1 \\sum^p_{j=1} \\beta_j^2 + \\lambda_2 \\sum^p_{j=1} | \\beta_j | \\right) \\end{equation}\\] Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. Figure 12.4: Figure: Elastic net coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). 12.7 Implementation We will start by applying a simple ridge model. In this example we simplify and focus on only four features: Gr_Liv_Area, Year_Built, Garage_Cars, and Garage_Area. In this example we will apply a \\(\\lambda\\) penalty parameter equal to 1. Since regularized methods apply a penalty to the coefficients, we need to ensure our coefficients are on a common scale. If not, then predictors with naturally larger values (e.g., total square footage) will be penalized more than predictors with naturally smaller values (e.g., total number of rooms). Takeaway - when using regularized regression models always standardize numeric features so they have mean 0 and standard deviation of 1. To apply a regularized model we still use linear_reg() but we change the engine and provide some additional arguments. There are a few engines that allow us to apply regularized models but the most popular is glmnet. In R, the \\(\\lambda\\) penalty parameter is represented by penalty and mixture controls the type of penalty (0 = ridge, 1 = lasso, and any value in between represents an elastic net). # Step 1: create ridge model object ridge_mod &lt;- linear_reg(penalty = 1000, mixture = 0) %&gt;% set_engine(&quot;glmnet&quot;) # Step 2: create model &amp; preprocessing recipe model_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Step 3: fit model workflow ridge_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) %&gt;% fit(data = ames_train) # Step 4: extract and tidy results ridge_fit %&gt;% extract_fit_parsnip() %&gt;% tidy() ## # A tibble: 309 × 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 154704. 1000 ## 2 Lot_Frontage -841. 1000 ## 3 Lot_Area 4067. 1000 ## 4 Year_Built 4962. 1000 ## 5 Year_Remod_Add 2876. 1000 ## 6 Mas_Vnr_Area 3940. 1000 ## 7 BsmtFin_SF_1 -957. 1000 ## 8 BsmtFin_SF_2 597. 1000 ## 9 Bsmt_Unf_SF -2925. 1000 ## 10 Total_Bsmt_SF 5941. 1000 ## # ℹ 299 more rows Using tidy() on our fit workflow provides the regularized coefficients. Note that these coefficients are interpreted differently than in previous modules since we standardized our features. Since our features are standardized to mean = 0 and standard deviation = 1, we interpret the coefficients as… For every 1 standard deviation above the mean Year_Built, the average predicted Sale_Price increases by $4,962. How well does our model perform, let’s use a 5-fold cross validation procedure to compute our generalization error: set.seed(123) kfolds &lt;- vfold_cv(ames_train, v = 5, strata = Sale_Price) workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) %&gt;% fit_resamples(kfolds) %&gt;% collect_metrics() %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 31373. 5 3012. Preprocessor1_Model1 12.7.1 Knowledge check Using the boston.csv data provided via Canvas fill in the blanks to… create train and test splits, create ridge model object using \\(\\lambda = 5000\\), create model (our response variable cmedv should be a function of all predictor variables) &amp; preprocessing recipe where you standardize all predictor variables, create 5-fold resampling object stratified on the response variable, fit model across resampling object and compute the cross validation RMSE. # Step 1: create train and test splits set.seed(123) # for reproducibility split &lt;- initial_split(_______, prop = 0.7, strata = &quot;cmedv&quot;) boston_train &lt;- training(split) boston_test &lt;- testing(split) # Step 2: create ridge model object ridge_mod &lt;- linear_reg(penalty = ____, mixture = __) %&gt;% set_engine(&quot;glmnet&quot;) # Step 3: create model &amp; preprocessing recipe model_recipe &lt;- recipe(cmedv ~ ., data = _______) %&gt;% step_normalize(________) # Step 4: create 5-fold resampling object set.seed(123) kfolds &lt;- vfold_cv(______, v = 5, strata = _______) # Step 5: fit model across resampling object and collect results workflow() %&gt;% add_recipe(________) %&gt;% add_model(________) %&gt;% fit_resamples(________) %&gt;% collect_metrics() %&gt;% filter(.metric == &#39;rmse&#39;) 12.8 Tuning our model It’s important to note that there are two main parameters that we, the data scientists, control setting the values for: mixture: the type of regularization (ridge, lasso, elastic net) we want to apply and, penalty: the strength of the regularization parameter, which we referred to as \\(\\lambda\\) in earlier sections. These parameters can be thought of as “the knobs to twiddle”4 and we often refer to them as hyperparameters. Hyperparameters (aka tuning parameters) are parameters that we can use to control the complexity of machine learning algorithms. Not all algorithms have hyperparameters (e.g., ordinary least squares); however, more advanced algorithms have at least one or more. The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we often go through iterations of testing out different values to determine which hyperparameter settings provide the optimal result. In the next module we’ll see how we can automate the hyperparameter tuning process. For now, we’ll simply take a manual approach. 12.8.1 Tuning regularization strength First, we’ll asses how the regularization strength impacts the performance of our model. In the two examples that follow we will use all the features in our Ames housing data; however, we’ll stick to the basic preprocessing of standardizing our numeric features and dummy encoding our categorical features. For both models we still use a ridge model (mixture = 0) but we assess two different values for \\(\\lambda\\) (penalty). Note how increasing the penalty leads to a lower RMSE. Having the larger penalty will not always lead to a lower RMSE, it just happens to be the case for this data set. This is likely because we have high collinearity across our predictors so a larger penalty is helping to constrain the coefficients and make them more stable. #################################### # Ridge model with penalty = 10000 # #################################### # create linear model object ridge_mod &lt;- linear_reg(penalty = 10000, mixture = 0) %&gt;% set_engine(&quot;glmnet&quot;) workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) %&gt;% fit_resamples(kfolds) %&gt;% collect_metrics() %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 30999. 5 2816. Preprocessor1_Model1 #################################### # Ridge model with penalty = 20000 # #################################### # create linear model object ridge_mod &lt;- linear_reg(penalty = 20000, mixture = 0) %&gt;% set_engine(&quot;glmnet&quot;) ridge_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) %&gt;% fit_resamples(kfolds) collect_metrics(ridge_fit) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 30678. 5 2598. Preprocessor1_Model1 12.8.2 Tuning regularization type We should also assess the type of regularization we want to apply (i.e. Ridge, Lasso, Elastic Net). It is not always definitive which type of regularization will perform best; however, often Ridge or some combination of both \\(L^1\\) and \\(L^2\\) (elastic net) performs best while Lasso is more useful if we desire to eliminate noisy features altogether. The following uses the same penalty as we did previously but changes the regularization type to Lasso (mixture = 1) and Elastic Net with a 50-50 mixture of \\(L^1\\) and \\(L^2\\) (mixture = 0.5). #################################### # Lasso model with penalty = 20000 # #################################### # create linear model object lasso_mod &lt;- linear_reg(penalty = 20000, mixture = 1) %&gt;% set_engine(&quot;glmnet&quot;) lasso_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(lasso_mod) %&gt;% fit_resamples(kfolds) collect_metrics(lasso_fit) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 48675. 5 2034. Preprocessor1_Model1 ########################################## # Elastic net model with penalty = 20000 # ########################################## # create linear model object elastic_mod &lt;- linear_reg(penalty = 20000, mixture = 0.5) %&gt;% set_engine(&quot;glmnet&quot;) elastic_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(elastic_mod) %&gt;% fit_resamples(kfolds) collect_metrics(elastic_fit) %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 39730. 5 2433. Preprocessor1_Model1 We see that in both cases the 5-fold cross validation error is higher than the Ridge model. 12.8.3 Tuning regularization type &amp; strength Unfortunately, using the same penalty value across the different models rarely works to find the optimal model. Not only do we need to tune the regularization type (Ridge, Lasso, Elastic Net), but we also need to tune the penalty value for each regularization type. This becomes far more tedious and we’d rather not manually test out many values. In the next module we’ll discuss how we can automate the tuning process to find optimal results. 12.8.4 Knowledge check Using the same boston training data as in the previous knowledge check, fill in the blanks to test a variety of values for… The type of regularization. Try a Lasso model versus an elastic net. The value of the penalty. Does a higher or lower penalty improve performance? # Step 2: create ridge model object glm_mod &lt;- linear_reg(penalty = ____, mixture = __) %&gt;% set_engine(&quot;glmnet&quot;) # Step 3: create model &amp; preprocessing recipe model_recipe &lt;- recipe(cmedv ~ ., data = boston_train) %&gt;% step_normalize(all_numeric_predictors()) # Step 4: create 5-fold resampling object set.seed(123) kfolds &lt;- vfold_cv(boston_train, v = 5, strata = cmedv) # Step 5: fit model across resampling object and collect results workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(glm_mod) %&gt;% fit_resamples(kfolds) %&gt;% collect_metrics() %&gt;% filter(.metric == &#39;rmse&#39;) 12.9 Feature importance For regularized models, importance is determined by magnitude of the standardized coefficients. Recall that ridge, lasso, and elastic net models push non-influential features to zero (or near zero). Consequently, very small coefficient values represent features that are not very important while very large coefficient values (whether negative or positive) represent very important features. We’ll check out the ridge model since that model performed best. We just need to fit a final model across all the training data, extract that final fit from the workflow object, and then use the vip package to visualize the variable importance scores for the top 20 features: final_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) %&gt;% fit(data = ames_train) final_fit %&gt;% extract_fit_parsnip() %&gt;% vip(num_features = 20, geom = &quot;point&quot;) 12.9.1 Knowledge check Based on the model you found to work best for the boston data… Fit a final model to the entire training data, Extract the final fitt from the workflow object, Plot the top 10 influential features. Which features are most influential in your model? 12.10 Classification problems We saw that we can apply regularization to a regression problem but we can also do the same for classification problems. The following applies the same procedures we used above but for the kernlab::spam data. library(kernlab) data(spam) # Step 1: create train and test splits set.seed(123) # for reproducibility split &lt;- initial_split(spam, prop = 0.7, strata = &quot;type&quot;) spam_train &lt;- training(split) spam_test &lt;- testing(split) # Step 2: create ridge model object ridge_mod &lt;- logistic_reg(penalty = 100, mixture = 0) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;classification&quot;) # Step 3: create model &amp; preprocessing recipe model_recipe &lt;- recipe(type ~ ., data = spam_train) %&gt;% step_normalize(all_numeric_predictors()) # Step 4: create workflow object to combine the recipe &amp; model spam_wf &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(ridge_mod) # Step 5: fit model across resampling object and collect results set.seed(123) kfolds &lt;- vfold_cv(spam_train, v = 5, strata = &quot;type&quot;) spam_wf %&gt;% fit_resamples(kfolds) %&gt;% collect_metrics() ## # A tibble: 3 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.606 5 0.000197 Preprocessor1_Model1 ## 2 brier_class binary 0.237 5 0.0000497 Preprocessor1_Model1 ## 3 roc_auc binary 0.946 5 0.00274 Preprocessor1_Model1 # Step 6: Fit final model on all training data final_fit &lt;- spam_wf %&gt;% fit(data = spam_train) # Step 7: Assess top 20 most influential features final_fit %&gt;% extract_fit_parsnip() %&gt;% vip(num_features = 20, geom = &quot;point&quot;) 12.11 Final thoughts Regularized regression provides many great benefits over traditional GLMs when applied to large data sets with lots of features. It provides a great option for handling the \\(n &gt; p\\) problem, helps minimize the impact of multicollinearity, and can perform automated feature selection. It also has relatively few hyperparameters which makes them easy to tune, computationally efficient compared to other algorithms discussed in later modules, and memory efficient. However, similar to GLMs, they are not robust to outliers in both the feature and target. Also, regularized regression models still assume a monotonic linear relationship (always increasing or decreasing in a linear fashion). It is also up to the analyst whether or not to include specific interaction effects. 12.12 Exercises Recall the Advertising.csv dataset we used in the Resampling lesson. Use this same data (available via Canvas) to… Split the data into 70-30 training-test sets. Apply a ridge model with Sales being the response variable. Perform a cross-validation procedure and test the model across various penalty parameter values. Which penalty parameter value resulted in the lowest RMSE? Repeat #2 but changing the regularization type to Lasso and Elastic net. Which regularization type results in the lowest RMSE? Using the best model from those that you tested out in #2 and #3, plot the feature importance. Which feature is most influential? Which is least influential? References Breiman, Leo et al. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer Series in Statistics New York, NY, USA: Hastie, T., R. Tibshirani, and M. Wainwright. 2015. Statistical Learning with Sparsity: The Lasso and Generalizations. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis. Hoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67. Kuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Vol. 26. Springer. Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological), 267–88. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. This phrase comes from Brad Efron’s comments in Breiman et al. (2001)↩︎ "],["overview-4.html", "13 Overview 13.1 Learning objectives 13.2 Estimated time requirement 13.3 Tasks", " 13 Overview In the last lesson we discussed regularization and were introduced to the idea of hyperparameters. We learned that we have control over these hyperparameter settings and we need to go through iterations of testing out different values to determine which hyperparameter settings provide the optimal result. However, rather than manually assess different values as we did in the last lesson, we’re going to learn how to automate the tuning process for more efficient and effective hyperparameter tuning. Then, we’re going to dive into the world of non-linear algorithms by introducing an extension of linear regression called multivariate adaptive regression splines. 13.1 Learning objectives By the end of this module you should be able to: Explain the bias-variance trade-off and apply efficient and effective hyperparameter tuning. Apply a multivariate adaptive regression splines to capture non-linear relationships in our data. 13.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 2-3 hours Reading + videos: 3-4 hours 13.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-5a-hyperparameter-tuning.html", "14 Lesson 5a: Hyperparameter Tuning 14.1 Learning objectives 14.2 Prerequisites 14.3 Bias-variance tradeoff 14.4 Hyperparameter tuning 14.5 Implementation 14.6 Exercises", " 14 Lesson 5a: Hyperparameter Tuning We learned in the last lesson that hyperparameters (aka tuning parameters) are parameters that we can use to control the complexity of machine learning algorithms. The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we often go through iterations of testing out different values to determine which hyperparameter settings provide the optimal result. As we add more hyperparameters, this becomes quite tedious to do manually so in this lesson we’ll learn how we can automate the tuning process to find the optimal (or near-optimal) settings for hyperparameters. 14.1 Learning objectives By the end of this module you will: Be able to explain the two components that make up prediction errors. Understand why hyperparameter tuning is an essential part of the machine learning process. Apply efficient and effective hyperparameter tuning with Tidymodels. 14.2 Prerequisites # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) # Model interpretability packages library(vip) # variable importance # Stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 14.3 Bias-variance tradeoff Prediction errors can be decomposed into two important subcomponents: error due to “bias” and error due to “variance”. There is often a tradeoff between a model’s ability to minimize bias and variance. Understanding how different sources of error lead to bias and variance helps us improve the data fitting process resulting in more accurate models. 14.3.1 Bias Bias is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. It measures how far off in general a model’s predictions are from the correct value, which provides a sense of how well a model can conform to the underlying structure of the data. Figure 14.1 illustrates an example where the polynomial model does not capture the underlying structure well. Linear models are classical examples of high bias models as they are less flexible and rarely capture non-linear, non-monotonic relationships. We can think of models with high bias as underfitting to the true patterns and relationships in our data. This is often because high bias models either oversimplify these relationships or are constrained in a way that cannot adequately form to the true, complex relationship that exists. These models tend to lead to high error on training and test data. We also need to think of bias-variance in relation to resampling. Models with high bias are rarely affected by the noise introduced by resampling. If a model has high bias, it will have consistency in its resampling performance as illustrated below: Figure 14.1: A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left). Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right). 14.3.2 Variance On the other hand, error due to variance is defined as the variability of a model prediction for a given data point. Many models (e.g., k-nearest neighbor, decision trees, gradient boosting machines) are very adaptable and offer extreme flexibility in the patterns that they can fit to. However, these models offer their own problems as they run the risk of overfitting to the training data. Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data. Models with high variance can be very adaptable and will conform very well to the patterns and relationships in the training data. In fact, these models will try to overfit to patterns and relationships in the training data so much that they are overly personalized to the training data and will not generalize well to data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data. Figure 14.2: A high variance k-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left). Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right). Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. Moreover, many algorithms that are capable of achieving high generalization performance have lots of hyperparameters that control the level of model complexity (i.e., the tradeoff between bias and variance). 14.3.3 Balancing the tradeoff We can think of bias and variance as two model attributes competing with one another. If our model is too simple and cannot conform to the relationships in our data then it is underfitting and will not generalize well. If our model is too flexible and overly conforms to the training data then it will also not generalize well. So our objective is to find a model with good balance that does not overfit nor underfit to the training data. This is a model that will generalize well. Figure 14.3: Our objective is to find a model with good balance that does not overfit nor underfit to the training data. This is a model that will generalize well. At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more hyperparameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. Figure 14.4: As more and more hyperparameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. The sweet spot for any model is the level of complexity that minimizes bias while keeping variance constrained. Understanding bias and variance is critical for understanding the behavior of prediction models, but in general what you really care about is overall error, not the specific decomposition. The sweet spot for any model is the level of complexity that minimizes bias while keeping variance constrained. To find this we need an effective and efficient hyperparameter tuning process. 14.4 Hyperparameter tuning Hyperparameters are the “knobs to twiddle” to control the complexity of machine learning algorithms and, therefore, the bias-variance trade-off. Not all algorithms have hyperparameters; however, as the complexity of our models increase (therefore the ability to capture and conform to more complex relationships in our data) we tend to see an increase in the number of hyperparameters. The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we need a method of identifying the optimal setting. For example, in the high variance example in the previous section, we illustrated a high variance k-nearest neighbor model. k-nearest neighbor models have a single hyperparameter (k) that determines the predicted value to be made based on the k nearest observations in the training data to the one being predicted. If k is small (e.g., \\(k=3\\)), the model will make a prediction for a given observation based on the average of the response values for the 3 observations in the training data most similar to the observation being predicted. This often results in highly variable predicted values because we are basing the prediction (in this case, an average) on a very small subset of the training data. As k gets bigger, we base our predictions on an average of a larger subset of the training data, which naturally reduces the variance in our predicted values (remember this for later, averaging often helps to reduce variance!). The figure below illustrates this point. Smaller k values (e.g., 2, 5, or 10) lead to high variance (but lower bias) and larger values (e.g., 150) lead to high bias (but lower variance). The optimal k value might exist somewhere between 20–50, but how do we know which value of k to use? Figure 14.5: k-nearest neighbor model with differing values for k. One way to perform hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy (as measured using k-fold CV, for instance). However, this can be very tedious work depending on the number of hyperparameters. An alternative approach is to perform a grid search. A grid search is an automated approach to searching across many combinations of hyperparameter values. For the simple example above, a grid search would predefine a candidate set of values for k (e.g., \\(k = 1, 2, \\dots, j\\)) and perform a resampling method (e.g., k-fold CV) to estimate which k value generalizes the best to unseen data. The plots in the below examples illustrate the results from a grid search to assess \\(k = 3, 5, \\dots, 150\\) using repeated 10-fold CV. The error rate displayed represents the average error for each value of k across all the repeated CV folds. On average, \\(k=46\\) was the optimal hyperparameter value to minimize error (in this case, RMSE which will be discussed shortly) on unseen data. Figure 14.6: Results from a grid search for a k-nearest neighbor model assessing values for k ranging from 3-25. We see high error values due to high model variance when k is small and we also see high errors values due to high model bias when k is large. The optimal model is found at k = 46. Throughout this course you’ll be exposed to different approaches to performing grid searches. In the above example, we used a full cartesian grid search, which assesses every hyperparameter value manually defined. However, as models get more complex and offer more hyperparameters, this approach can become computationally burdensome and requires you to define the optimal hyperparameter grid settings to explore. Additional approaches we’ll illustrate include random grid searches (Bergstra and Bengio 2012) which explores randomly selected hyperparameter values from a range of possible values, early stopping which allows you to stop a grid search once reduction in the error stops marginally improving, adaptive resampling via futility analysis (Kuhn 2014) which adaptively resamples candidate hyperparameter values based on approximately optimal performance, and more. 14.5 Implementation Recall our regularized regression model from the last lesson. With that model there are two two main hyperparameters that we need to tune: mixture: the type of regularization (ridge, lasso, elastic net) we want to apply and, penalty: the strength of the regularization parameter (\\(\\lambda\\)). Initially, we set mixture = 0 (Ridge model) and the strength of our regularization to penalty = 1000. # Step 1: create regularized model object reg_mod &lt;- linear_reg(mixture = 0, penalty = 1000) %&gt;% set_engine(&quot;glmnet&quot;) # Step 2: create model &amp; preprocessing recipe model_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Step 3. create resampling object set.seed(123) kfolds &lt;- vfold_cv(ames_train, v = 5, strata = Sale_Price) # Step 4: fit model workflow reg_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(reg_mod) %&gt;% fit_resamples(kfolds) # Step 5: assess results reg_fit %&gt;% collect_metrics() %&gt;% filter(.metric == &#39;rmse&#39;) ## # A tibble: 1 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 31373. 5 3012. Preprocessor1_Model1 We then manually iterated through different values of mixture and penalty to try find the optimal setting. Which is less than efficient. 14.5.1 Tuning Rather than specify set values for mixture and penalty, let’s instead build our model in a way that uses placeholders for values. We can do this using the tune() function: reg_mod &lt;- linear_reg(mixture = tune(), penalty = tune()) %&gt;% set_engine(&quot;glmnet&quot;) We can create a regular grid of values to try using some convenience functions for each hyperparameter: reg_grid &lt;- grid_regular(mixture(), penalty(), levels = 5) The function grid_regular() is from the dials package. It chooses sensible values to try for each hyperparameter; here, we asked for 5 values each. Since we have two to tune, grid_regular() returns \\(5 \\times 5 = 25\\) different possible tuning combinations to try in a data frame. reg_grid ## # A tibble: 25 × 2 ## mixture penalty ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0000000001 ## 2 0.25 0.0000000001 ## 3 0.5 0.0000000001 ## 4 0.75 0.0000000001 ## 5 1 0.0000000001 ## 6 0 0.0000000316 ## 7 0.25 0.0000000316 ## 8 0.5 0.0000000316 ## 9 0.75 0.0000000316 ## 10 1 0.0000000316 ## # ℹ 15 more rows Now that we have our model with hyperparameter value placeholders and a grid of hyperparameter values to assess we can create a workflow object as we’ve done in the past and use tune_grid() to train our 25 models using our k-fold cross validation resamples. # tune tuning_results &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(reg_mod) %&gt;% tune_grid(resamples = kfolds, grid = reg_grid) # assess results tuning_results %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) ## # A tibble: 25 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0000000001 0 rmse standard 31373. 5 3012. Prepro… ## 2 0.0000000316 0 rmse standard 31373. 5 3012. Prepro… ## 3 0.00001 0 rmse standard 31373. 5 3012. Prepro… ## 4 0.00316 0 rmse standard 31373. 5 3012. Prepro… ## 5 1 0 rmse standard 31373. 5 3012. Prepro… ## 6 0.0000000001 0.25 rmse standard 37912. 5 4728. Prepro… ## 7 0.0000000316 0.25 rmse standard 37912. 5 4728. Prepro… ## 8 0.00001 0.25 rmse standard 37912. 5 4728. Prepro… ## 9 0.00316 0.25 rmse standard 37912. 5 4728. Prepro… ## 10 1 0.25 rmse standard 37912. 5 4728. Prepro… ## # ℹ 15 more rows We can assess our best models with show_best(), which by default will show the top 5 performing models based on the desired metric. In this case we see that our top 5 models use the Ridge regularization (mixture = 0) and across all the penalties we get the same cross-validation RMSE (31,373). tuning_results %&gt;% show_best(metric = &quot;rmse&quot;) ## # A tibble: 5 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0000000001 0 rmse standard 31373. 5 3012. Preproc… ## 2 0.0000000316 0 rmse standard 31373. 5 3012. Preproc… ## 3 0.00001 0 rmse standard 31373. 5 3012. Preproc… ## 4 0.00316 0 rmse standard 31373. 5 3012. Preproc… ## 5 1 0 rmse standard 31373. 5 3012. Preproc… We can also use the select_best() function to pull out the single set of hyperparameter values for our best regularization model: tuning_results %&gt;% select_best(metric = &quot;rmse&quot;) ## # A tibble: 1 × 3 ## penalty mixture .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0000000001 0 Preprocessor1_Model01 14.5.2 More tuning Based on the above results, we may wish to do another iteration and adjust the hyperparameter values to assess. Since all our best models were Ridge models we may want to set our model to use a Ridge penalty but then just tune the strength of the penalty. Here, we create another hyperparameter grid but we specify the range we want to search through. Note that penalty automatically applies a log transformation so by saying range = c(0, 5) I am actually saying to search between 1 - 100,000. reg_mod &lt;- linear_reg(mixture = 0, penalty = tune()) %&gt;% set_engine(&quot;glmnet&quot;) reg_grid &lt;- grid_regular(penalty(range = c(0, 5)), levels = 10) Now we can search again and we see that by using slightly higher penalty values we improve our performance. tuning_results &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(reg_mod) %&gt;% tune_grid(resamples = kfolds, grid = reg_grid) tuning_results %&gt;% show_best(metric = &quot;rmse&quot;) ## # A tibble: 5 × 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 27826. rmse standard 30633. 5 2503. Preprocessor1_Model… ## 2 7743. rmse standard 31160. 5 2903. Preprocessor1_Model… ## 3 1 rmse standard 31373. 5 3012. Preprocessor1_Model… ## 4 3.59 rmse standard 31373. 5 3012. Preprocessor1_Model… ## 5 12.9 rmse standard 31373. 5 3012. Preprocessor1_Model… 14.5.3 Finalizing our model We can update (or “finalize”) our workflow object with the values from select_best(). This now creates a final model workflow with the optimal hyperparameter values. best_hyperparameters &lt;- select_best(tuning_results, metric = &quot;rmse&quot;) final_wf &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(reg_mod) %&gt;% finalize_workflow(best_hyperparameters) final_wf ## ══ Workflow ═══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ─────────────────────────────────────────────────────── ## 2 Recipe Steps ## ## • step_normalize() ## • step_dummy() ## ## ── Model ────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 27825.5940220713 ## mixture = 0 ## ## Computational engine: glmnet We can then use this final workflow to do further assessments. For example, if we want to train this workflow on the entire data set and look at which predictors are most influential we can: final_wf %&gt;% fit(data = ames_train) %&gt;% extract_fit_parsnip() %&gt;% vip() 14.6 Exercises Using the same kernlab::spam data we saw in the section 12.10… Split the data into 70-30 training-test sets. Apply a regularized classification model (type is our response variable) but use the tune() and grid_regular() approach we saw in this lesson to automatically tune the mixture and penalty hyperparameters. Use a 5-fold cross-validation procedure. Which hyperparameter values maximize the AUC (roc_auc) metric? Retrain a final model with these optimal hyperparameters and identify the top 10 most influential predictors. References Bergstra, James, and Yoshua Bengio. 2012. “Random Search for Hyper-Parameter Optimization.” Journal of Machine Learning Research 13 (Feb): 281–305. Kuhn, Max. 2014. “Futility Analysis in the Cross-Validation of Machine Learning Models.” arXiv Preprint arXiv:1405.6974. "],["lesson-5b-multivariate-adaptive-regression-splines.html", "15 Lesson 5b: Multivariate Adaptive Regression Splines 15.1 Learning objectives 15.2 Prerequisites 15.3 Nonlinearity 15.4 Multivariate adaptive regression splines 15.5 Fitting a MARS model 15.6 Tuning 15.7 Feature interpretation 15.8 Final thoughts 15.9 Exercises", " 15 Lesson 5b: Multivariate Adaptive Regression Splines The previous modules discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding nonlinear model terms (e.g., squared terms, interaction effects, and other transformations of the original features); however, to do so you the analyst must know the specific nature of the nonlinearities and interactions a priori. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities and interactions in the data that help maximize predictive accuracy. This lesson discusses multivariate adaptive regression splines (MARS) (J. H. Friedman 1991), an algorithm that automatically creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future modules will focus on other nonlinear algorithms. 15.1 Learning objectives By the end of this lesson you will know how: MARS models incorporates non-linear relationships with hinge functions (aka “knots”). How to train and tune MARS models using the Tidymodels construct. How to identify and visualize the most influential features in a MARS models. 15.2 Prerequisites # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) # for fitting MARS models # Model interpretability packages library(vip) # for variable importance library(pdp) # for variable relationships # Stratified sampling with the rsample package set.seed(123) ames &lt;- AmesHousing::make_ames() split &lt;- rsample::initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- rsample::training(split) ames_test &lt;- rsample::testing(split) 15.3 Nonlinearity In the previous modules, we focused on linear models (where the analyst has to explicitly specify any nonlinear relationships and interaction effects). We illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients. However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy. We can extend linear models to capture any non-linear relationship. Typically, this is done by explicitly including polynomial terms (e.g., \\(x_i^2\\)) or step functions. Polynomial regression is a form of regression in which the relationship between \\(X\\) and \\(Y\\) is modeled as a \\(d\\)th degree polynomial in \\(X\\). For example, the following equation represents a polynomial regression function where \\(Y\\) is modeled as a \\(d\\)-th degree polynomial in \\(X\\). Generally speaking, it is unusual to use \\(d\\) greater than 3 or 4 as the larger \\(d\\) becomes, the easier the function fit becomes overly flexible and oddly shaped…especially near the boundaries of the range of \\(X\\) values. Increasing \\(d\\) also tends to increase the presence of multicollinearity. \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x^2_i + \\beta_3 x^3_i \\dots + \\beta_d x^d_i + \\epsilon_i, \\end{equation}\\] An alternative to polynomials is to use step functions. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of \\(X\\) into bins, and fit a simple constant (e.g., the mean response) in each. This amounts to converting a continuous feature into an ordered categorical variable such that our linear regression function is converted to the following equation \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\beta_3 C_3(x_i) \\dots + \\beta_d C_d(x_i) + \\epsilon_i, \\end{equation}\\] where \\(C_1(x_i)\\) represents \\(x_i\\) values ranging from \\(c_1 \\leq x_i &lt; c_2\\), \\(C_2\\left(x_i\\right)\\) represents \\(x_i\\) values ranging from \\(c_2 \\leq x_i &lt; c_3\\), \\(\\dots\\), \\(C_d\\left(x_i\\right)\\) represents \\(x_i\\) values ranging from \\(c_{d-1} \\leq x_i &lt; c_d\\). The figure below contrasts linear, polynomial, and step function fits for non-linear, non-monotonic simulated data. Figure 15.1: Blue line represents predicted (y) values as a function of x for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional linear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function cutting x into six categorical levels. Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at what points of a variable \\(X\\) should cut points be made for the step functions. Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unnecessary time commitment from an analyst to determine these explicit non-linear settings. 15.4 Multivariate adaptive regression splines Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (knots) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). For example, consider our non-linear, non-monotonic data above where \\(Y = f\\left(X\\right)\\). The MARS procedure will first look for the single point across the range of X values where two different linear relationships between Y and X achieve the smallest error (e.g., smallest SSE). What results is known as a hinge function \\(h\\left(x-a\\right)\\), where \\(a\\) is the cutpoint value. For a single knot (Figure (A)), our hinge function is \\(h\\left(\\text{x}-1.183606\\right)\\) such that our two linear models for Y are \\[\\begin{equation} \\text{y} = \\begin{cases} \\beta_0 + \\beta_1(1.183606 - \\text{x}) &amp; \\text{x} &lt; 1.183606, \\\\ \\beta_0 + \\beta_1(\\text{x} - 1.183606) &amp; \\text{x} &gt; 1.183606 \\end{cases} \\end{equation}\\] Once the first knot has been found, the search continues for a second knot which is found at \\(x = 4.898114\\) (plot B in figure below). This results in three linear models for y: \\[\\begin{equation} \\text{y} = \\begin{cases} \\beta_0 + \\beta_1(1.183606 - \\text{x}) &amp; \\text{x} &lt; 1.183606, \\\\ \\beta_0 + \\beta_1(\\text{x} - 1.183606) &amp; \\text{x} &gt; 1.183606 \\quad \\&amp; \\quad \\text{x} &lt; 4.898114, \\\\ \\beta_0 + \\beta_1(4.898114 - \\text{x}) &amp; \\text{x} &gt; 4.898114 \\end{cases} \\end{equation}\\] Figure 15.2: Examples of fitted regression splines of one (A), two (B), three (C), and four (D) knots. This procedure continues until many knots are found, producing a (potentially) highly non-linear prediction equation. Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. Consequently, once the full set of knots has been identified, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as “pruning” and we can use cross-validation, as we have with the previous models, to find the optimal number of knots. 15.5 Fitting a MARS model 15.5.1 Fitting a basic model We’ll start by fitting a basic model using Gr_Liv_Area and Year_Built to predict our Sales_Price. In both R, the modeling algorithm will assess all potential knots across all supplied features and then will prune to the optimal number of knots based on an expected change in \\(R^2\\) (for the training data) of less than 0.001. This calculation is performed by the Generalized cross-validation (GCV) procedure, which is a computational shortcut for linear models that produces an approximate leave-one-out cross-validation error metric (Golub, Heath, and Wahba 1979). Note that MARS models do not require normalization or standardization of numeric features. In R, we use the parsnip::mars() function which provides a default setting of using the earth package, which is the most popular package for MARS functionality in R. When you run the below code you may get an error that states: This engine requires some package installs: ‘earth’. This is because the engine that Tidymodels uses is the earth package. If you get this error then install the earth package with install.packages(“earth”) and rerun the code. mars_fit &lt;- mars(mode = &quot;regression&quot;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, ames_train) mars_fit ## parsnip model object ## ## Selected 9 of 10 terms, and 2 of 2 predictors ## Termination condition: RSq changed by less than 0.001 at 10 terms ## Importance: Gr_Liv_Area, Year_Built ## Number of terms at each degree of interaction: 1 8 (additive model) ## GCV 1802525892 RSS 3.632344e+12 GRSq 0.7208907 RSq 0.7252347 In the results above, the GCV is reported along with other metrics (\\(R^2\\), residual sum of squares - RSS). We can use this info to compute the RMSE, which is in the low to mid $40K range. Keep in mind that this RMSE is not a cross-validated RMSE, just the RMSE on the training data (we’ll do some cross-validation shortly). # RMSE sqrt(mars_fit$fit$rss / nrow(ames_train)) ## [1] 42103.92 Looking at the coefficients below, you’ll notice that we see a knot in Gr_Liv_Area at 3395. This suggests that as homes exceed 3395 square feet there is a change in the linear relationship between Gr_Liv_Area and Sale_Price compared to homes that are less than 3395 square feet. In fact, we can see multiple knots across the range of Gr_Liv_Area and Year_Built. Our model has taken two features (Gr_Liv_Area and Year_Built) and split them into 8 features by creating knots along these feature values. # coefficients mars_fit$fit$coefficients ## Sale_Price ## (Intercept) 295628.31484 ## h(Gr_Liv_Area-3395) -772.99460 ## h(3395-Gr_Liv_Area) -61.66360 ## h(2002-Year_Built) -685.97770 ## h(Gr_Liv_Area-2169) 27.57111 ## h(Gr_Liv_Area-3194) 516.04236 ## h(Gr_Liv_Area-1456) 41.77324 ## h(Year_Built-1977) 910.94873 ## h(Year_Built-2004) 12519.67884 The most important tuning parameter in a MARS model is the number of knots to use for a given feature. However, this is automatically baked into the algorithm so we get this tuning for free! 15.5.2 Fitting a full model Next, lets go ahead and fit a full model to include all Ames housing features. As you’ll see in the results we experience significant improvement in the training RMSE (low $20K). However, recall that this is not a cross-validated RMSE, but rather the RMSE for the training data in which the model was fit. Consequently, this is likely an overfit model but we’ll perform a cross-validation approach in the next section to get a more accurate generalized RMSE. MARS models are no different then other algorithms where we need to convert categorical features to numeric values. In R, the MARS modeling engine “earth” will automatically one-hot encode all categorical features. This leads to over 300 features in our dataset; however, when you look at the results you will see that only a fraction of these features are used in the trained model (note it says “29 of 308 predictors”). However, there will actually be more coefficients then the features used because of the knots. For example of the 29 features, 41 coefficients are developed since many of these features are used multiple times (i.e. h(Gr_Liv_Area-3194), h(3194-Gr_Liv_Area)). We call this process of feature elimination and knot refinement as “pruning the knots”. mars_fit &lt;- mars(mode = &quot;regression&quot;) %&gt;% fit(Sale_Price ~ ., ames_train) mars_fit ## parsnip model object ## ## Selected 41 of 45 terms, and 29 of 308 predictors ## Termination condition: RSq changed by less than 0.001 at 45 terms ## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, ... ## Number of terms at each degree of interaction: 1 40 (additive model) ## GCV 511589986 RSS 967008439675 GRSq 0.9207836 RSq 0.9268516 # RMSE sqrt(mars_fit$fit$rss / nrow(ames_train)) ## [1] 21724.22 # coefficients mars_fit$fit$coefficients ## Sale_Price ## (Intercept) 2.275972e+05 ## h(Gr_Liv_Area-3194) -2.879934e+02 ## h(3194-Gr_Liv_Area) -5.861227e+01 ## h(Year_Built-2002) 3.128137e+03 ## h(2002-Year_Built) -4.506370e+02 ## h(Total_Bsmt_SF-2223) -6.531310e+02 ## h(2223-Total_Bsmt_SF) -2.903002e+01 ## h(1656-Bsmt_Unf_SF) 2.152273e+01 ## Overall_QualVery_Excellent 1.198770e+05 ## Overall_QualExcellent 7.325741e+04 ## Overall_QualVery_Good 3.065703e+04 ## h(2-Kitchen_AbvGr) 2.199288e+04 ## h(Second_Flr_SF-1427) 3.071810e+02 ## h(Year_Remod_Add-1973) 3.198457e+02 ## h(Lot_Area-6720) 4.956740e-01 ## h(6720-Lot_Area) -3.972312e+00 ## FunctionalTyp 1.642861e+04 ## h(Bedroom_AbvGr-4) -1.304304e+04 ## h(4-Bedroom_AbvGr) 3.044473e+03 ## h(Garage_Cars-2) 1.351805e+04 ## h(2-Garage_Cars) -5.099554e+03 ## h(Total_Bsmt_SF-2136) 5.756167e+02 ## NeighborhoodCrawford 2.015380e+04 ## Overall_QualGood 1.167108e+04 ## Condition_2PosN -1.164630e+05 ## Overall_CondGood 3.000370e+04 ## Overall_CondVery_Good 3.380410e+04 ## Overall_CondAbove_Average 2.231569e+04 ## h(Gr_Liv_Area-2898) 1.591394e+02 ## NeighborhoodGreen_Hills 9.830591e+04 ## Roof_MatlWdShngl 6.744107e+04 ## h(Longitude--93.6572) -1.229288e+05 ## h(-93.6572-Longitude) -1.570905e+05 ## NeighborhoodStone_Brook 3.162134e+04 ## Overall_CondExcellent 3.886202e+04 ## Overall_CondAverage 1.480704e+04 ## h(Total_Bsmt_SF-1295) 3.234334e+01 ## NeighborhoodNorthridge 2.693805e+04 ## h(Fireplaces-1) 6.808179e+03 ## h(1-Fireplaces) -4.374554e+03 ## NeighborhoodNorthridge_Heights 1.455416e+04 15.5.3 Fitting a full model with interactions In addition to pruning the number of knots, MARS models allow us to also assess potential interactions between different hinge functions. The following example illustrates by allowing second degree interactions. You can see that now our model includes interaction terms between a maximum of two hinge functions (e.g. in the results you see h(Year_Built-2002)*h(Gr_Liv_Area-2398) which represents an interaction effect for those houses built after 2002 and have more than 2,398 square feet of living space). mars_fit &lt;- mars(mode = &quot;regression&quot;, prod_degree = 2) %&gt;% fit(Sale_Price ~ ., ames_train) mars_fit ## parsnip model object ## ## Selected 45 of 50 terms, and 27 of 308 predictors ## Termination condition: RSq changed by less than 0.001 at 50 terms ## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, ... ## Number of terms at each degree of interaction: 1 20 24 ## GCV 420267019 RSS 7.70355e+11 GRSq 0.9349244 RSq 0.9417272 # coefficients mars_fit$fit$coefficients ## Sale_Price ## (Intercept) 3.473694e+05 ## h(Gr_Liv_Area-3194) 2.302940e+02 ## h(3194-Gr_Liv_Area) -7.005261e+01 ## h(Year_Built-2002) 5.242461e+03 ## h(2002-Year_Built) -7.360079e+02 ## h(Total_Bsmt_SF-2223) 1.181093e+02 ## h(2223-Total_Bsmt_SF) -4.901140e+01 ## h(Year_Built-2002)*h(Gr_Liv_Area-2398) 9.035037e+00 ## h(Year_Built-2002)*h(2398-Gr_Liv_Area) -3.382639e+00 ## h(Bsmt_Unf_SF-625)*h(3194-Gr_Liv_Area) -1.145129e-02 ## h(625-Bsmt_Unf_SF)*h(3194-Gr_Liv_Area) 9.905591e-03 ## h(2002-Year_Built)*h(Total_Bsmt_SF-912) -7.674355e-01 ## h(2002-Year_Built)*h(912-Total_Bsmt_SF) 2.277350e-01 ## Mas_Vnr_TypeStone*h(Gr_Liv_Area-3194) -5.876451e+02 ## h(Bedroom_AbvGr-4) -9.649772e+03 ## h(4-Bedroom_AbvGr) 5.208836e+03 ## Overall_QualVery_Excellent 1.095179e+05 ## Overall_QualExcellent -1.578730e+05 ## Overall_QualVery_Good 3.794092e+04 ## h(Lot_Area-6720) 3.325029e+00 ## h(6720-Lot_Area) -4.188039e+00 ## h(2002-Year_Built)*h(Year_Remod_Add-1971) 7.085099e+00 ## NeighborhoodCrawford*h(2002-Year_Built) 3.213659e+02 ## h(Lot_Area-6720)*h(Garage_Cars-3) -1.943527e+00 ## h(Lot_Area-6720)*h(3-Garage_Cars) -1.237131e+00 ## Overall_QualExcellent*FoundationPConc 2.295300e+05 ## Overall_QualGood 1.940091e+04 ## Overall_QualAbove_Average*h(2223-Total_Bsmt_SF) 6.438535e+00 ## h(2002-Year_Built)*Sale_ConditionNormal 2.183528e+02 ## h(Lot_Area-6720)*h(144-Screen_Porch) -1.051737e-02 ## FunctionalTyp 1.481969e+04 ## h(Kitchen_AbvGr-1) -1.802740e+04 ## h(First_Flr_SF-2444) -7.656120e+01 ## h(2444-First_Flr_SF) -7.045054e+00 ## NeighborhoodStone_Brook*h(Year_Built-2002) 1.033983e+04 ## Overall_CondGood*h(2002-Year_Built) 1.598754e+02 ## h(Longitude--93.6563) -9.367281e+05 ## h(-93.6563-Longitude) -9.165180e+05 ## h(3194-Gr_Liv_Area)*h(Longitude--93.6559) 4.432167e+02 ## h(3194-Gr_Liv_Area)*h(-93.6559-Longitude) 3.803470e+02 ## FunctionalTyp*h(Garage_Area-542) 3.500866e+01 ## NeighborhoodGreen_Hills*FunctionalTyp 9.112140e+04 ## h(4-Bedroom_AbvGr)*h(Fireplaces-1) 5.820943e+03 ## h(4-Bedroom_AbvGr)*h(1-Fireplaces) -2.482522e+03 ## h(Bsmt_Unf_SF-1237)*h(4-Bedroom_AbvGr) -3.737860e+01 15.5.4 Knowledge check Using the boston.csv dataset: Apply a MARS model where cmedv is the response variable and rm and lstat are the two predictor variables. What is the training data RSS for the final model? Assess the coefficients. How many hinges/knots are there? Can you interpret the coefficients? Apply a MARS model that uses all possible predictor variables. What is the training data RSS for the final model? Assess the coefficients. How many hinges/knots are there? Can you interpret the coefficients? Apply a MARS model that uses all possible predictor variables and allows for two-way interactions. What is the training data RSS for the final model? Assess the coefficients. How many hinges/knots are there? Can you interpret the coefficients? 15.6 Tuning There are two important tuning parameters associated with MARS models: the maximum degree of interactions and the number of terms retained in the final model. We need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of CV model performance on the training data rather than an exact k-fold CV process). As in the previous lesson, we’ll perform a CV grid search to identify the optimal hyperparameter mix. Below, we set up a grid that assesses different combinations of interaction complexity and the number of terms to retain in the final model. Rarely is there any benefit in assessing greater than 3-rd degree interactions. Also, realize that tuning MARS models can become computationally intense. Its often beneficial to start with 10 evenly spaced values for the maximum terms and then you can always zoom in to a region once you find an approximate optimal solution. In R, our hyperparameters are num_terms (number of terms retained in the model) and prod_degrees (maximum degree of interaction allowed). Note that in R we can use num_terms() and prod_degree() from the dials package to create a tuning grid. By default prod_degree() will use values of 1 and 2 (no interactions and 2 degree interactions). Our results show that our top 5 performing models all include 2 degree interactions and between 37-49 total terms (“knots”). # create MARS model object mars_mod &lt;- mars(mode = &quot;regression&quot;, num_terms = tune(), prod_degree = tune()) # create k-fold cross validation object set.seed(123) folds &lt;- vfold_cv(ames_train, v = 5) # create our model recipe model_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # create a hyper parameter tuning grid hyper_grid &lt;- grid_regular( num_terms(range = c(1, 100)), prod_degree(), levels = 50 ) # train our model across the hyper parameter grid results &lt;- tune_grid(mars_mod, model_recipe, resamples = folds, grid = hyper_grid) # get best results show_best(results, metric = &quot;rmse&quot;) ## # A tibble: 5 × 8 ## num_terms prod_degree .metric .estimator ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37 2 rmse standard ## 2 43 2 rmse standard ## 3 45 2 rmse standard ## 4 47 2 rmse standard ## 5 49 2 rmse standard ## # ℹ 4 more variables: mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt; autoplot(results) 15.6.1 Knowledge check Using the boston.csv dataset apply a MARS model that uses all possible predictor variables and tune for the number of terms and interactions. Use a 5-fold cross validation procedure to tune your model. For number of terms used (num_terms) assess values between 1-50. For possible interaction degrees use the default prod_degree(). Assess a total of 10 values from each parameter (levels = 10). Which model(s) provide the lowest cross validated RMSE? What hyperparameter values provide these optimal results? 15.7 Feature interpretation MARS models use a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction can be used as the variable importance measure (\"gcv\"). Since MARS will automatically include and exclude terms during the pruning process, it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (\"rss\"); however, you will often see very little difference between these methods. The following examples extract the feature importance values based on RSS. The below shows that our most influential feature Gr_Liv_Area followed by Year_Built, Total_Bsmt_SF, Overall_QualExcellent, etc. best_hyperparameters &lt;- select_best(results, metric = &quot;rmse&quot;) final_wf &lt;- workflow() %&gt;% add_model(mars_mod) %&gt;% add_formula(Sale_Price ~ .) %&gt;% finalize_workflow(best_hyperparameters) # plot top 20 influential variables final_wf %&gt;% fit(data = ames_train) %&gt;% extract_fit_parsnip() %&gt;% vip(20, type = &quot;rss&quot;) Since our relationship between our response variable and the predictor variables are now non-linear, it becomes helpful to visualize the relationship between the most influential feature(s) and the response variable to see how they relate. We can do that with partial dependence plots (PDPs). PDPs plot the change in the average predicted value of our model (\\(\\hat y\\)) as specified feature(s) vary over their marginal distribution. We can then use the pdp package to extract the partial dependence values in order to plot the relationship between predicted Sale_Price and Gr_Liv_Area. Don’t get too hung up in the code here. But you can see that based on our PDP, as homes exceed 3,200ish square feet we see a steepening increase in the relationship. # prediction function pdp_pred_fun &lt;- function(object, newdata) { mean(predict(object, newdata, type = &quot;numeric&quot;)$.pred) } # use the pdp package to extract partial dependence predictions # and then plot final_wf %&gt;% fit(data = ames_train) %&gt;% pdp::partial( pred.var = &quot;Gr_Liv_Area&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% ggplot(aes(Gr_Liv_Area, yhat)) + geom_line() + scale_y_continuous(labels = scales::dollar) 15.8 Final thoughts There are several advantages to MARS. First, MARS naturally handles mixed types of predictors (quantitative and qualitative). MARS considers all possible binary partitions of the categories for a qualitative predictor into two groups.5 Each group then generates a pair of piecewise indicator functions for the two categories. MARS also requires minimal feature engineering (e.g., feature scaling) and performs automated feature selection. For example, since MARS scans each predictor to identify a split that improves predictive accuracy, non-informative features will not be chosen. Furthermore, highly correlated predictors do not impede predictive accuracy as much as they do with OLS models. However, one disadvantage to MARS models is that they’re typically slower to train. Since the algorithm scans each value of each predictor for potential cutpoints, computational performance can suffer as both \\(n\\) and \\(p\\) increase. Also, although correlated predictors do not necessarily impede model performance, they can make model interpretation difficult. When two features are nearly perfectly correlated, the algorithm will essentially select the first one it happens to come across when scanning the features. Then, since it randomly selected one, the correlated feature will likely not be included as it adds no additional explanatory power. 15.9 Exercises Using the same kernlab::spam data we saw in the section 12.10… Split the data into 70-30 training-test sets. Apply a MARS classification model where type is our response variable and use all possible predictor variables. Use a 5-fold cross-validation procedure. Tune number of terms used (num_terms) with values ranging 1-100. Tune number of interaction degrees with the default prod_degree() values. Assess a total of 50 values from each parameter (levels = 50). Which model(s) have the highest AUC (roc_auc) scores? What hyperparameter values provide these optimal results? Use the hyperparameter values that provide the best results to finalize your workflow and and identify the top 20 most influential predictors. Bonus: See if you can create a PDP plot for the #1 most influential variable. What does the relationship between this feature and the response variable look like? References Friedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics, 1–67. Golub, Gene H, Michael Heath, and Grace Wahba. 1979. “Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.” Technometrics 21 (2): 215–23. This is very similar to CART-like decision trees which you’ll be exposed to in a later module.↩︎ "],["overview-5.html", "16 Overview 16.1 Learning objectives 16.2 Estimated time requirement 16.3 Tasks", " 16 Overview Tree-based models are a class of nonparametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. Such divide-and-conquer methods can produce simple rules that are easy to interpret and visualize with tree diagrams. As we’ll see, decision trees offer many benefits; however, they typically lack in predictive performance compared to more complex algorithms like neural networks and MARS. However, more advanced decision tree ensemble algorithms — like bagging and random forests — combine together many decision trees and can perform quite well. 16.1 Learning objectives By the end of this module you should be able to: Explain how decision tree models partition data and how the depth of a tree impacts performance. Train, fit, tune and assess decision tree models. Explain and apply decision tree ensemble algorithms such as bagging and random forests. 16.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 3-4 hours Reading + videos: 4 hours 16.3 Tasks Work through the 3 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-6a-decision-trees.html", "17 Lesson 6a: Decision Trees 17.1 Learning objectives 17.2 Prerequisites 17.3 Structure 17.4 Partitioning 17.5 How deep? 17.6 Fitting a decision tree 17.7 Tuning 17.8 Feature interpretation 17.9 Final thoughts 17.10 Exercises", " 17 Lesson 6a: Decision Trees Tree-based models are a class of nonparametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. Predictions are obtained by fitting a simpler model (e.g., a constant like the average response value) in each region. Such divide-and-conquer methods can produce simple rules that are easy to interpret and visualize with tree diagrams. As we’ll see, decision trees offer many benefits; however, they typically lack in predictive performance compared to more complex algorithms like neural networks and MARS. However, future modules will discuss powerful ensemble algorithms—like random forests and gradient boosting machines—which are constructed by combining together many decision trees in a clever way. This module will provide you with a strong foundation in decision trees. 17.1 Learning objectives By the end of this module you will know: How decision tree models partition data and how the depth of a tree impacts performance. Train, fit, tune and assess decision tree models. Identify important features and visualize their influence on the response. 17.2 Prerequisites # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) # Model interpretability packages library(vip) # for variable importance library(pdp) # for variable relationships set.seed(123) ames &lt;- AmesHousing::make_ames() split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 17.3 Structure There are many methodologies for constructing decision trees but the most well-known is the classification and regression tree (CART) algorithm proposed in Breiman (1984).6 A basic decision tree partitions the training data into homogeneous subgroups (i.e., groups with similar response values) and then fits a simple constant in each subgroup (e.g., the mean of the within group response values for regression). The subgroups (also called nodes) are formed recursively using binary partitions formed by asking simple yes-or-no questions about each feature (e.g., is age &lt; 18?). This is done a number of times until a suitable stopping criteria is satisfied (e.g., a maximum depth of the tree is reached). After all the partitioning has been done, the model predicts the output based on (1) the average response values for all observations that fall in that subgroup (regression problem), or (2) the class that has majority representation (classification problem). For classification, predicted probabilities can be obtained using the proportion of each class within the subgroup. What results is an inverted tree-like structure such as that in the below figure. In essence, our tree is a set of rules that allows us to make predictions by asking simple yes-or-no questions about each feature. For example, if the customer is loyal, has household income greater than $150,000, and is shopping in a store, the exemplar tree diagram below would predict that the customer will redeem a coupon. Figure 17.1: Exemplar decision tree predicting whether or not a customer will redeem a coupon (yes or no) based on the customer’s loyalty, household income, last month’s spend, coupon placement, and shopping mode. We refer to the first subgroup at the top of the tree as the root node (this node contains all of the training data). The root node shows the first feature that best splits the data into two groups. The final subgroups at the bottom of the tree are called the terminal nodes or leaves. These terminal nodes represent predicted values once you have traversed a particular path down the tree. Every subgroup in between is referred to as an internal node. The connections between nodes are called branches. Figure 17.2: Terminology of a decision tree. 17.4 Partitioning As illustrated above, CART uses binary recursive partitioning (it’s recursive because each split or rule depends on the the splits above it). The objective at each node is to find the “best” feature (\\(x_i\\)) to partition the remaining data into one of two regions (\\(R_1\\) and \\(R_2\\)) such that the overall error between the actual response (\\(y_i\\)) and the predicted constant (\\(c_i\\)) is minimized. For regression problems, the objective function to minimize is the total SSE as defined in the following equation: \\[\\begin{equation} SSE = \\sum_{i \\in R_1}\\left(y_i - c_1\\right)^2 + \\sum_{i \\in R_2}\\left(y_i - c_2\\right)^2 \\end{equation}\\] For classification problems, the partitioning is usually made to maximize the reduction in cross-entropy or the Gini index.7 In both regression and classification trees, the objective of partitioning is to minimize dissimilarity in the terminal nodes. However, we suggest Therneau, Atkinson, et al. (1997) for a more thorough discussion regarding binary recursive partitioning. Having found the best feature/split combination, the data are partitioned into two regions and the splitting process is repeated on each of the two regions (hence the name binary recursive partitioning). This process is continued until a suitable stopping criterion is reached (e.g., a maximum depth is reached or the tree becomes “too complex”). It’s important to note that a single feature can be used multiple times in a tree. For example, say we have data generated from a simple \\(\\sin\\) function with Gaussian noise: \\(Y_i \\stackrel{iid}{\\sim} N\\left(\\sin\\left(X_i\\right), \\sigma^2\\right)\\), for \\(i = 1, 2, \\dots, 500\\). A regression tree built with a single root node (often referred to as a decision stump) leads to a split occurring at \\(x = 3.1\\). Figure 17.3: Decision tree illustrating the single split on feature x (left). The resulting decision boundary illustrates the predicted value when x &lt; 3.1 (0.64), and when x &gt; 3.1 (-0.67) (right). If we build a deeper tree, we’ll continue to split on the same feature (\\(x\\)) as illustrated below. This is because \\(x\\) is the only feature available to split on so it will continue finding the optimal splits along this feature’s values until a pre-determined stopping criteria is reached. Figure 17.4: Decision tree illustrating with depth = 3, resulting in 7 decision splits along values of feature x and 8 prediction regions (left). The resulting decision boundary (right). However, even when many features are available, a single feature may still dominate if it continues to provide the best split after each successive partition. For example, a decision tree applied to the iris data set (R. A. Fisher 1936) where the species of the flower (setosa, versicolor, and virginica) is predicted based on two features (sepal width and sepal length) results in an optimal decision tree with two splits on each feature. Also, note how the decision boundary in a classification problem results in rectangular regions enclosing the observations. The predicted value is the response class with the greatest proportion within the enclosed region. Figure 17.5: Decision tree for the iris classification problem (left). The decision boundary results in rectangular regions that enclose the observations. The class with the highest proportion in each region is the predicted value (right). 17.5 How deep? This leads to an important question: how deep (i.e., complex) should we make the tree? If we grow an overly complex tree as in the below figure, we tend to overfit to our training data resulting in poor generalization performance. Figure 17.6: Overfit decision tree with 56 splits. Consequently, there is a balance to be achieved in the depth and complexity of the tree to optimize predictive performance on future unseen data. To find this balance, we have two primary approaches: (1) early stopping and (2) pruning. 17.5.1 Early stopping Early stopping explicitly restricts the growth of the tree. There are several ways we can restrict tree growth but two of the most common approaches are to restrict the tree depth to a certain level or to restrict the minimum number of observations allowed in any terminal node. When limiting tree depth we stop splitting after a certain depth (e.g., only grow a tree that has a depth of 5 levels). The shallower the tree the less variance we have in our predictions; however, at some point we can start to inject too much bias as shallow trees (e.g., stumps) are not able to capture interactions and complex patterns in our data. When restricting minimum terminal node size (e.g., leaf nodes must contain at least 10 observations for predictions) we are deciding to not split intermediate nodes which contain too few data points. At the far end of the spectrum, a terminal node’s size of one allows for a single observation to be captured in the leaf node and used as a prediction (in this case, we’re interpolating the training data). This results in high variance and poor generalizability. On the other hand, large values restrict further splits therefore reducing variance. These two approaches can be implemented independently of one another; however, they do have interaction effects as illustrated below. Figure 17.7: Illustration of how early stopping affects the decision boundary of a regression decision tree. The columns illustrate how tree depth impacts the decision boundary and the rows illustrate how the minimum number of observations in the terminal node influences the decision boundary. 17.5.2 Pruning An alternative to explicitly specifying the depth of a decision tree is to grow a very large, complex tree and then prune it back to find an optimal subtree. We find the optimal subtree by using a cost complexity parameter (\\(\\alpha\\)) that penalizes our objective function for the number of terminal nodes of the tree (\\(T\\)) as in the following equation. \\[\\begin{equation} \\texttt{minimize} \\left\\{ SSE + \\alpha \\vert T \\vert \\right\\} \\end{equation}\\] For a given value of \\(\\alpha\\) we find the smallest pruned tree that has the lowest penalized error. You may recognize the close association to the lasso penalty discussed in the regularized regression lesson. As with the regularization methods, smaller penalties tend to produce more complex models, which result in larger trees. Whereas larger penalties result in much smaller trees. Consequently, as a tree grows larger, the reduction in the SSE must be greater than the cost complexity penalty. Typically, we evaluate multiple models across a spectrum of \\(\\alpha\\) and use CV to identify the optimal value and, therefore, the optimal subtree that generalizes best to unseen data. Figure 17.8: To prune a tree, we grow an overly complex tree (left) and then use a cost complexity parameter to identify the optimal subtree (right). 17.6 Fitting a decision tree 17.6.1 Fitting a basic model To illustrate some of the concepts we’ve mentioned we’ll start by implementing models using just the Gr_Liv_Area and Year_Built features in our Ames housing data. In R we use the decision_tree() model and we’ll use the rpart package as our model engine. In this example we will not set a specific depth of our tree; rather, rpart automatically builds a fully deep tree and then prunes it to attempt to find an optimal tree depth. # Step 1: create decision tree model object dt_mod &lt;- decision_tree(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;rpart&quot;) # Step 2: create model recipe model_recipe &lt;- recipe( Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train ) # Step 3: fit model workflow dt_fit &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(dt_mod) %&gt;% fit(data = ames_train) # Step 4: results dt_fit ## ══ Workflow [trained] ═════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: decision_tree() ## ## ── Preprocessor ─────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ────────────────────────────────────────────────────────────── ## n= 2049 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 2049 1.321981e+13 180922.6 ## 2) Year_Built&lt; 1985.5 1228 2.698241e+12 141467.4 ## 4) Gr_Liv_Area&lt; 1486 840 7.763942e+11 125692.6 ## 8) Year_Built&lt; 1952.5 317 2.687202e+11 107761.5 * ## 9) Year_Built&gt;=1952.5 523 3.439725e+11 136561.0 * ## 5) Gr_Liv_Area&gt;=1486 388 1.260276e+12 175619.2 ## 10) Gr_Liv_Area&lt; 2663.5 372 8.913502e+11 170223.5 * ## 11) Gr_Liv_Area&gt;=2663.5 16 1.062939e+11 301068.8 * ## 3) Year_Built&gt;=1985.5 821 5.750622e+12 239937.1 ## 6) Gr_Liv_Area&lt; 1963 622 1.813069e+12 211699.5 ## 12) Gr_Liv_Area&lt; 1501.5 285 3.483774e+11 182098.8 * ## 13) Gr_Liv_Area&gt;=1501.5 337 1.003788e+12 236732.8 ## 26) Year_Built&lt; 2004.5 198 3.906393e+11 217241.1 * ## 27) Year_Built&gt;=2004.5 139 4.307663e+11 264498.0 * ## 7) Gr_Liv_Area&gt;=1963 199 1.891416e+12 328197.2 ## 14) Gr_Liv_Area&lt; 2390.5 107 5.903157e+11 290924.0 ## 28) Year_Built&lt; 2004.5 69 1.168804e+11 253975.7 * ## 29) Year_Built&gt;=2004.5 38 2.081946e+11 358014.5 * ## 15) Gr_Liv_Area&gt;=2390.5 92 9.795556e+11 371547.5 * We can use rpart.plot() to plot our tree. This is only useful if we have a relatively small tree to visualize; however, most trees we will build will be far too large to attempt to visualize. In this case, we see that the root node (first node) splits our data based on (Year_Built). For those observations where the home is built after 1985 we follow the right half of the decision tree and for those where the home is built in or prior to 1985 we follow the left half of the decision tree. rpart.plot::rpart.plot(dt_fit$fit$fit$fit) However, to understand how our model is performing we want to perform cross validation. We see that this single decision tree is not performing spectacularly well with the average RMSE across our 5 folds equaling just under $50K. # create resampling procedure set.seed(13) kfold &lt;- vfold_cv(ames_train, v = 5) # train model results &lt;- fit_resamples(dt_mod, model_recipe, kfold) # model results collect_metrics(results) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 47178. 5 1193. Preprocessor1_Model1 ## 2 rsq standard 0.655 5 0.0129 Preprocessor1_Model1 17.6.2 Fitting a full model Next, lets go ahead and fit a full model to include all Ames housing features. We do not need to one-hot encode our features as rpart will naturally handle categorical features. By including all features we see some improvement in our model performance as our average cross validated RMSE is now in the low $40K range. # create model recipe with all features full_model_recipe &lt;- recipe( Sale_Price ~ ., data = ames_train ) # train model results &lt;- fit_resamples(dt_mod, full_model_recipe, kfold) # model results collect_metrics(results) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 40510. 5 1694. Preprocessor1_Model1 ## 2 rsq standard 0.745 5 0.0148 Preprocessor1_Model1 17.6.3 Knowledge check Using the boston.csv dataset: Apply a default decision tree model where cmedv is the response variable and rm and lstat are the two predictor variables. Assess the resulting tree and explain the first decision node. Pick a branch and explain the decision nodes as you traverse down the branch. Apply a decision tree model that uses all possible predictor variables. Assess the resulting tree and explain the first decision node. Pick a branch and explain the decision nodes as you traverse down the branch. Use a 5-fold cross validation procedure to compare the model in #1 to the model in #2. Which model performs best? 17.7 Tuning As previously mentioned, the tree depth is the primary factor that impacts performance. We can control tree depth via a few different parameters: Max depth: we can explicitly state the maximum depth a tree can be grown. Minimum observations for a split: The minimum number of samples required to split an internal node. This limits a tree from continuing to grow as the number of observations in a give node becomes smaller. Cost complexity parameter: acts as a regularization mechanism by penalizing the objective function. There is not one best approach to use and often different combinations of these parameter settings improves model performance. The following will demonstrate a small grid search across 3 different values for each of these parameters (\\(3^3 = 27\\) total setting combinations). # create model object with tuning options dt_mod &lt;- decision_tree( mode = &quot;regression&quot;, cost_complexity = tune(), tree_depth = tune(), min_n = tune() ) %&gt;% set_engine(&quot;rpart&quot;) # create the hyperparameter grid hyper_grid &lt;- grid_regular( cost_complexity(), tree_depth(), min_n() ) # hyperparameter value combinations to be assessed hyper_grid ## # A tibble: 27 × 3 ## cost_complexity tree_depth min_n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.0000000001 1 2 ## 2 0.00000316 1 2 ## 3 0.1 1 2 ## 4 0.0000000001 8 2 ## 5 0.00000316 8 2 ## 6 0.1 8 2 ## 7 0.0000000001 15 2 ## 8 0.00000316 15 2 ## 9 0.1 15 2 ## 10 0.0000000001 1 21 ## # ℹ 17 more rows We can now perform our grid search using tune_grid(). We see the optimal model decreases our average CV RMSE into the mid $30K range. It is common to run additional grid searches after the first grid search. These additional grid searches uses the first grid search to find parameter values that perform well and then continue to analyze additional ranges around these values. # train our model across the hyper parameter grid set.seed(123) results &lt;- tune_grid(dt_mod, full_model_recipe, resamples = kfold, grid = hyper_grid) # get best results show_best(results, metric = &quot;rmse&quot;, n = 10) ## # A tibble: 10 × 9 ## cost_complexity tree_depth min_n .metric .estimator mean n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.0000000001 8 21 rmse standard 34883. 5 ## 2 0.00000316 8 21 rmse standard 34883. 5 ## 3 0.0000000001 15 21 rmse standard 34986. 5 ## 4 0.00000316 15 21 rmse standard 34986. 5 ## 5 0.0000000001 15 40 rmse standard 36018. 5 ## 6 0.00000316 15 40 rmse standard 36018. 5 ## 7 0.0000000001 8 40 rmse standard 36150. 5 ## 8 0.00000316 8 40 rmse standard 36150. 5 ## 9 0.00000316 8 2 rmse standard 37161. 5 ## 10 0.0000000001 8 2 rmse standard 37173. 5 ## # ℹ 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt; 17.7.1 Knowledge check Using the boston.csv dataset apply a decision tree model that models cmedv as a function of all possible predictor variables and tune the following hyperparameters with a 5-fold cross validation procedure: Tune the cost complexity values with the default cost_complexity() values. Tune the depth of the tree with the default tree_depth() values. Tune the minimum number of observations in a node with the default min_n() values. Assess a total of 5 values from each parameter (levels = 5). Which model(s) provide the lowest cross validated RMSE? What hyperparameter values provide these optimal results? 17.8 Feature interpretation To measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance. We can use a similar approach as we have in the previous lessons to plot the most influential features in our decision tree models. # get best hyperparameter values best_model &lt;- select_best(results, metric = &#39;rmse&#39;) # put together final workflow final_wf &lt;- workflow() %&gt;% add_recipe(full_model_recipe) %&gt;% add_model(dt_mod) %&gt;% finalize_workflow(best_model) # fit final workflow across entire training data final_fit &lt;- final_wf %&gt;% fit(data = ames_train) # plot feature importance final_fit %&gt;% extract_fit_parsnip() %&gt;% vip(20) And similar the MARS model, since our relationship between our response variable and the predictor variables are non-linear, it becomes helpful to visualize the relationship between the most influential feature(s) and the response variable to see how they relate. Recall that we can do that with PDP plots. Here, we see that the overall quality of a home doesn’t have a big impact unless the homes are rated very good to very excellent. # prediction function pdp_pred_fun &lt;- function(object, newdata) { mean(predict(object, newdata, type = &quot;numeric&quot;)$.pred) } # use the pdp package to extract partial dependence predictions # and then plot final_fit %&gt;% pdp::partial( pred.var = &quot;Overall_Qual&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% ggplot(aes(Overall_Qual, yhat)) + geom_col() + scale_y_continuous(labels = scales::dollar) And if we do a similar plot for the Gr_Liv_Area variable we can see the non-linear relationship between the square footage of a home and the predicted Sale_Price that exists. final_fit %&gt;% pdp::partial( pred.var = &quot;Gr_Liv_Area&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% autoplot() + scale_y_continuous(labels = scales::dollar) 17.8.1 Knowledge check Using the boston.csv dataset and the model from the previous Knowledge check that performed best… Plot the top 10 most influential features. Create and explain a PDP plot of the most influential feature. 17.9 Final thoughts Decision trees have a number of advantages. Trees require very little pre-processing. This is not to say feature engineering may not improve upon a decision tree, but rather, that there are no pre-processing requirements. Monotonic transformations (e.g., \\(\\log\\), \\(\\exp\\), and \\(\\sqrt{}\\)) are not required to meet algorithm assumptions as in many parametric models; instead, they only shift the location of the optimal split points. Outliers typically do not bias the results as much since the binary partitioning simply looks for a single location to make a split within the distribution of each feature. Decision trees can easily handle categorical features without preprocessing. For unordered categorical features with more than two levels, the classes are ordered based on the outcome (for regression problems, the mean of the response is used and for classification problems, the proportion of the positive outcome class is used). For more details see J. Friedman, Hastie, and Tibshirani (2001), Breiman and Ihaka (1984), Ripley (2007), W. D. Fisher (1958), and Loh and Vanichsetakul (1988). Missing values often cause problems with statistical models and analyses. Most procedures deal with them by refusing to deal with them—incomplete observations are tossed out. However, most decision tree implementations can easily handle missing values in the features and do not require imputation. This is handled in various ways but most commonly by creating a new “missing” class for categorical variables or using surrogate splits (see Therneau, Atkinson, et al. (1997) for details). However, individual decision trees generally do not often achieve state-of-the-art predictive accuracy. In this module, we saw that the best pruned decision tree, although it performed better than linear regression, had a very poor RMSE (~$41,000) compared to some of the other models we’ve built. This is driven by the fact that decision trees are composed of simple yes-or-no rules that create rigid non-smooth decision boundaries. Furthermore, we saw that deep trees tend to have high variance (and low bias) and shallow trees tend to be overly bias (but low variance). In the modules that follow, we’ll see how we can combine multiple trees together into very powerful prediction models called ensembles. 17.10 Exercises Using the same kernlab::spam data we saw in the section 12.10… Split the data into 70-30 training-test sets. Apply a decision tree classification model where type is our response variable and use all possible predictor variables. Use a 5-fold cross-validation procedure. Tune the cost complexity values with the default cost_complexity() values. Tune the depth of the tree with the default tree_depth() values. Tune the minimum number of observations in a node with the default min_n() values. Assess a total of 5 values from each parameter (levels = 5). Which model(s) have the highest AUC (roc_auc) scores? What hyperparameter values provide these optimal results? Use the hyperparameter values that provide the best results to finalize your workflow and and identify the top 20 most influential predictors. Bonus: See if you can create a PDP plot for the #1 most influential variable. What does the relationship between this feature and the response variable look like? References Breiman, Leo. 1984. Classification and Regression Trees. Routledge. Breiman, Leo, and Ross Ihaka. 1984. Nonlinear Discriminant Analysis via Scaling and ACE. Department of Statistics, University of California. Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. Fisher, Walter D. 1958. “On Grouping for Maximum Homogeneity.” Journal of the American Statistical Association 53 (284): 789–98. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer Series in Statistics New York, NY, USA: Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” Journal of Computational and Graphical Statistics 15 (3): 651–74. Kass, Gordon V. 1980. “An Exploratory Technique for Investigating Large Quantities of Categorical Data.” Applied Statistics, 119–27. Loh, Wei-Yin, and Nunta Vanichsetakul. 1988. “Tree-Structured Classification via Generalized Discriminant Analysis.” Journal of the American Statistical Association 83 (403): 715–25. Quinlan, J Ross et al. 1996. “Bagging, Boosting, and C4. 5.” In AAAI/IAAI, Vol. 1, 725–30. Quinlan, J. Ross. 1986. “Induction of Decision Trees.” Machine Learning 1 (1): 81–106. Ripley, Brian D. 2007. Pattern Recognition and Neural Networks. Cambridge University Press. Therneau, Terry M, Elizabeth J Atkinson, et al. 1997. “An Introduction to Recursive Partitioning Using the RPART Routines.” Mayo Foundation. Other decision tree algorithms include the Iterative Dichotomiser 3 (J. Ross Quinlan 1986), C4.5 (J. Ross Quinlan et al. 1996), Chi-square automatic interaction detection (Kass 1980), Conditional inference trees (Hothorn, Hornik, and Zeileis 2006), and more.↩︎ Gini index and cross-entropy are the two most commonly applied loss functions used for decision trees. Classification error is rarely used to determine partitions as they are less sensitive to poor performing splits (J. Friedman, Hastie, and Tibshirani 2001).↩︎ "],["lesson-6b-bagging.html", "18 Lesson 6b: Bagging 18.1 Learning objectives 18.2 Prerequisites 18.3 Why and when bagging works 18.4 Fitting a bagged decision tree model 18.5 Tuning 18.6 Feature interpretation 18.7 Final thoughts 18.8 Exercises", " 18 Lesson 6b: Bagging In an earlier module we learned about bootstrapping as a resampling procedure, which creates b new bootstrap samples by drawing samples with replacement of the original training data. This module illustrates how we can use bootstrapping to create an ensemble of predictions. Bootstrap aggregating, also called bagging, is one of the first ensemble algorithms8 machine learning practitioners learn and is designed to improve the stability and accuracy of regression and classification algorithms. By model averaging, bagging helps to reduce variance and minimize overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. 18.1 Learning objectives By the end of this module you will know: How and why bagging improves decision tree performance. How to implement bagging and ensure you are optimizing bagging performance. How to identify influential features and their effects on the response variable. 18.2 Prerequisites # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) library(baguette) # you may need to install.packages this package # Model interpretability packages library(vip) # for variable importance library(pdp) # for variable relationships set.seed(123) ames &lt;- AmesHousing::make_ames() split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 18.3 Why and when bagging works Bootstrap aggregating (bagging) prediction models is a general method for fitting multiple versions of a prediction model and then combining (or ensembling) them into an aggregated prediction (Breiman 1996). Bagging is a fairly straight forward algorithm in which b bootstrap copies of the original training data are created, the regression or classification algorithm (commonly referred to as the base learner) is applied to each bootstrap sample and, in the regression context, new predictions are made by averaging the predictions together from the individual base learners. When dealing with a classification problem, the base learner predictions are combined using plurality vote or by averaging the estimated class probabilities together. This is represented in the below equation where \\(X\\) is the record for which we want to generate a prediction, \\(\\widehat{f_{bag}}\\) is the bagged prediction, and \\(\\widehat{f_1}\\left(X\\right), \\widehat{f_2}\\left(X\\right), \\dots, \\widehat{f_b}\\left(X\\right)\\) are the predictions from the individual base learners. \\[\\begin{equation} \\widehat{f_{bag}} = \\widehat{f_1}\\left(X\\right) + \\widehat{f_2}\\left(X\\right) + \\cdots + \\widehat{f_b}\\left(X\\right) \\end{equation}\\] Because of the aggregation process, bagging effectively reduces the variance of an individual base learner (i.e., averaging reduces variance); however, bagging does not always improve upon an individual base learner. As discussed in our bias vs variance discussion, some models have larger variance than others. Bagging works especially well for unstable, high variance base learners—algorithms whose predicted output undergoes major changes in response to small changes in the training data (Dietterich 2000b, 2000a). This includes algorithms such as decision trees and KNN (when k is sufficiently small). However, for algorithms that are more stable or have high bias, bagging offers less improvement on predicted outputs since there is less variability (e.g., bagging a linear regression model will effectively just return the original predictions for large enough \\(b\\)). The general idea behind bagging is referred to as the “wisdom of the crowd” effect and was popularized by Surowiecki (2005). It essentially means that the aggregation of information in large diverse groups results in decisions that are often better than could have been made by any single member of the group. The more diverse the group members are then the more diverse their perspectives and predictions will be, which often leads to better aggregated information. Think of estimating the number of jelly beans in a jar at a carinival. While any individual guess is likely to be way off, you’ll often find that the averaged guesses tends to be a lot closer to the true number. This is illustrated in the below plot, which compares bagging \\(b = 100\\) polynomial regression models, MARS models, and CART decision trees. You can see that the low variance base learner (polynomial regression) gains very little from bagging while the higher variance learner (decision trees) gains significantly more. Not only does bagging help minimize the high variability (instability) of single trees, but it also helps to smooth out the prediction surface. Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome. A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample. The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say \\(n \\geq 1,000\\)). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance. Think of the OOB estimate of generalization performance as an unstructured, but free CV statistic. 18.4 Fitting a bagged decision tree model Recall in the decision tree module that our optimally tuned single decision tree model was obtaining a mid-$30K RMSE. In this first implementation we’ll perform the same process; however, instead of a single decision tree we’ll use bagging to create and aggregate performance across 5 bagged trees. In R we use baguette::bag_tree to create a bagged tree model object, which can be applied to decision tree or MARS models. Note how we do not need to perform any feature engineering as decision trees (and therefore bagged decision trees) can handle numeric and categorical features just as they are. Note how aggregating just 5 decision trees improves our RMSE to around $30K. # create model recipe with all features model_recipe &lt;- recipe( Sale_Price ~ ., data = ames_train ) # create bagged CART model object and # start with 5 bagged trees tree_mod &lt;- bag_tree() %&gt;% set_engine(&quot;rpart&quot;, times = 5) %&gt;% set_mode(&quot;regression&quot;) # create resampling procedure set.seed(13) kfold &lt;- vfold_cv(ames_train, v = 5) # train model results &lt;- fit_resamples(tree_mod, model_recipe, kfold) # model results collect_metrics(results) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 29850. 5 1270. Preprocessor1_Model1 ## 2 rsq standard 0.861 5 0.0145 Preprocessor1_Model1 18.4.1 Knowledge check Using the boston.csv dataset: Apply a bagged decision tree model where cmedv is the response variable and use all possible predictor variables. Use a 5-fold cross validation procedure to assess… Bagging 5 trees Bagging 10 trees Bagging 25 trees How does the model perform in each scenario? Do adding more trees improve performance? 18.5 Tuning One thing to note is that typically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more). Consequently, we can treat the number of trees as a tuning parameter where our focus is to make sure we are using enough trees for our error metric to converge at a optimal value (minimize RMSE in our case). However, rarely do we run the risk of including too many trees as we typically will not see our error metric degrade after adding more trees than necessary. The primary concern is compute efficiency, we want enough trees to reach an optimal resampling error; however, not more trees than necessary as it will slow down computational efficiency. In this example we use a hyperparameter grid to assess 5, 25, 50, 100, and 200 trees. We see an initial decrease but as we start using 50+ trees we only see marginal changes in our RMSE, suggesting we have likely reached enough trees to minimize the RMSE and any additional differences are likely due to the standard error that occurs during the bootstrapping process. # create bagged CART model object with # tuning option set for number of bagged trees tree_mod &lt;- bag_tree() %&gt;% set_engine(&quot;rpart&quot;, times = tune()) %&gt;% set_mode(&quot;regression&quot;) # create the hyperparameter grid hyper_grid &lt;- expand.grid(times = c(5, 25, 50, 100, 200)) # train our model across the hyperparameter grid set.seed(123) results &lt;- tune_grid(tree_mod, model_recipe, resamples = kfold, grid = hyper_grid) # model results show_best(results, metric = &quot;rmse&quot;) ## # A tibble: 5 × 7 ## times .metric .estimator mean n ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 200 rmse standard 26175. 5 ## 2 100 rmse standard 26430. 5 ## 3 50 rmse standard 26536. 5 ## 4 25 rmse standard 26909. 5 ## 5 5 rmse standard 30276. 5 ## # ℹ 2 more variables: std_err &lt;dbl&gt;, ## # .config &lt;chr&gt; When bagging trees we can also tune the same hyperparameters that we tuned with decision trees (cost_complexity, tree_depth, min_n). See bag_tree? for details. However, the default values for these hyperparameters is set to grow our trees to the maximum depth and not prune them at all. It has been shown that bagging many deep, overfit trees typically leads to best performance. 18.5.1 Knowledge check Using the boston.csv dataset: Apply a bagged decision tree model where cmedv is the response variable and use all possible predictor variables. Use a 5-fold cross validation procedure and tune the number of trees to assess 5, 10, 25, 50, 100, and 150 trees. How does the model perform in each scenario? Do adding more trees improve performance? At what point do we experience diminishing returns in our cross-validated RMSE? 18.6 Feature interpretation Unfortunately, due to the bagging process, models that are normally perceived as interpretable are no longer so. However, we can still make inferences about how features are influencing our model. Recall in the decision tree module that we measure feature importance based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree. For bagged decision trees, this process is similar. For each tree, we compute the sum of the reduction of the loss function across all splits. We then aggregate this measure across all trees for each feature. The features with the largest average decrease in SSE (for regression) are considered most important. The following extracts the feature importance for our best performing bagged decision trees. Unfortunately, vip::vip() doesn’t work naturally with models from bag_tree so we need to manually extract the variable importance scores and plot them with ggplot. We see that Overall_Qual and Neighborhood are the top two most influential features. # identify best model best_hyperparameters &lt;- results %&gt;% select_best(metric = &quot;rmse&quot;) # finalize workflow object final_wf &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(tree_mod) %&gt;% finalize_workflow(best_hyperparameters) # final fit on training data final_fit &lt;- final_wf %&gt;% fit(data = ames_train) # plot top 20 influential variables vi &lt;- final_fit %&gt;% extract_fit_parsnip() %&gt;% .[[&#39;fit&#39;]] %&gt;% var_imp() %&gt;% slice(1:20) ggplot(vi, aes(value, reorder(term, value))) + geom_col() + ylab(NULL) + xlab(&quot;Feature importance&quot;) We can then plot the partial dependence of the most influential feature to see how it influences the predicted values. We see that as overall quality increase we see a significant increase in the predicted sale price. # prediction function pdp_pred_fun &lt;- function(object, newdata) { predict(object, newdata, type = &quot;numeric&quot;)$.pred } # use the pdp package to extract partial dependence predictions # and then plot final_fit %&gt;% pdp::partial( pred.var = &quot;Overall_Qual&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% ggplot(aes(Overall_Qual, yhat)) + geom_boxplot() + scale_y_continuous(labels = scales::dollar) Now let’s plot the PDP for the Gr_Liv_Area and see how that variable relates to the predicted sale price. # prediction function pdp_pred_fun &lt;- function(object, newdata) { mean(predict(object, newdata, type = &quot;numeric&quot;)$.pred) } # use the pdp package to extract partial dependence predictions # and then plot final_fit %&gt;% pdp::partial( pred.var = &quot;Gr_Liv_Area&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% autoplot() + scale_y_continuous(labels = scales::dollar) 18.6.1 Knowledge check Using the boston.csv dataset and the model from the previous Knowledge check that performed best… Plot the top 10 most influential features. Create and explain a PDP plot of the most influential feature. 18.7 Final thoughts Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed. However, using various interpretability algorithms such as VIPs and PDPs, we can still make inferences about how our bagged model leverages feature information. Also, since bagging consists of independent processes, the algorithm is easily parallelizable. However, when bagging trees, a problem still exists. Although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships. For example, if we create six decision trees with different bootstrapped samples of the Boston housing data (Harrison Jr and Rubinfeld 1978), we see a similar structure as the top of the trees. Although there are 15 predictor variables to split on, all six trees have both lstat and rm variables driving the first few splits. We use the Boston housing data in this example because it has fewer features and shorter names than the Ames housing data. Consequently, it is easier to compare multiple trees side-by-side; however, the same tree correlation problem exists in the Ames bagged model. Figure 18.1: Six decision trees based on different bootstrap samples. In the next lesson we’ll see how to resolve this concern using a modification of bagged trees called random forest. 18.8 Exercises Using the same kernlab::spam data we saw in the section 12.10… Split the data into 70-30 training-test sets. Apply a bagged decision tree model modeling the type response variable as a function of all available features. How many trees are required before the loss function stabilizes? How does the model performance compare to the decision tree model applied in the previous lesson’s exercise? Which 10 features are considered most influential? Are these the same features that have been influential in previous model? Create partial dependence plots for the top two most influential features. Explain the relationship between the feature and the predicted values. References ———. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. Dietterich, Thomas G. 2000a. “An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization.” Machine Learning 40 (2): 139–57. ———. 2000b. “Ensemble Methods in Machine Learning.” In International Workshop on Multiple Classifier Systems, 1–15. Springer. Harrison Jr, David, and Daniel L Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” Journal of Environmental Economics and Management 5 (1): 81–102. Surowiecki, James. 2005. The Wisdom of Crowds. Anchor. Also commonly referred to as a meta-algorithm.↩︎ "],["lesson-6c-random-forests.html", "19 Lesson 6c: Random Forests 19.1 Learning objectives 19.2 Prerequisites 19.3 Extending bagging 19.4 Out-of-the-box performance 19.5 Hyperparameters 19.6 Tuning 19.7 Feature interpretation 19.8 Final thoughts 19.9 Exercises", " 19 Lesson 6c: Random Forests Random forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance. They have become a very popular “out-of-the-box” or “off-the-shelf” learning algorithm that enjoys good predictive performance with relatively little hyperparameter tuning. Many modern implementations of random forests exist; however, Leo Breiman’s algorithm (Breiman 2001) has largely become the authoritative procedure. This module will cover the fundamentals of random forests. 19.1 Learning objectives By the end of this module you will know: How to implement a random forest model along with the hyperparameters that are commonly toggled in these algorithms. Multiple strategies for performing a grid search. How to identify influential features and their effects on the response variable. 19.2 Prerequisites # Helper packages library(tidyverse) # for data wrangling &amp; plotting # Modeling packages library(tidymodels) # Model interpretability packages library(vip) # for variable importance library(pdp) # for variable relationships set.seed(123) ames &lt;- AmesHousing::make_ames() split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 19.3 Extending bagging Random forests are built using the same fundamental principles as decision trees and bagging. Bagging trees introduces a random component into the tree building process by building many trees on bootstrapped copies of the training data. Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance. However, as we saw in the last module, simply bagging trees results in tree correlation that limits the effect of variance reduction. Random forests help to reduce tree correlation by injecting more randomness into the tree-growing process.9 More specifically, while growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the split variable is limited to a random subset of \\(m_{try}\\) of the original \\(p\\) features. Typical default values are \\(m_{try} = \\frac{p}{3}\\) (regression) and \\(m_{try} = \\sqrt{p}\\) (classification) but this should be considered a tuning parameter. The basic algorithm for a regression or classification random forest can be generalized as follows: 1. Given a training data set 2. Select number of trees to build (n_trees) 3. for i = 1 to n_trees do 4. | Generate a bootstrap sample of the original data 5. | Grow a regression/classification tree to the bootstrapped data 6. | for each split do 7. | | Select m_try variables at random from all p variables 8. | | Pick the best variable/split-point among the m_try 9. | | Split the node into two child nodes 10. | end 11. | Use typical tree model stopping criteria to determine when a | tree is complete (but do not prune) 12. end 13. Output ensemble of trees When \\(m_{try} = p\\), the algorithm is equivalent to bagging decision trees. Since the algorithm randomly selects a bootstrap sample to train on and a random sample of features to use at each split, a more diverse set of trees is produced which tends to lessen tree correlation beyond bagged trees and often dramatically increase predictive power. 19.4 Out-of-the-box performance Random forests have become popular because they tend to provide very good out-of-the-box performance. Although they have several hyperparameters that can be tuned, the default values tend to produce good results. Moreover, Probst, Bischl, and Boulesteix (2018) illustrated that among the more popular machine learning algorithms, random forests have the least variability in their prediction accuracy when tuning. For example, if we train a random forest model with all hyperparameters set to their default values, we get RMSEs comparable to some of the best model’s we’ve run thus far (without any tuning). In R we will want to use the ranger package as our random forest engine. Similar to other examples we need to set the mode of machine learning model to either a regression or classification modeling objective. # create model recipe with all features model_recipe &lt;- recipe( Sale_Price ~ ., data = ames_train ) # create random forest model object rf_mod &lt;- rand_forest(mode = &quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;) # create resampling procedure set.seed(13) kfold &lt;- vfold_cv(ames_train, v = 5) # train model results &lt;- fit_resamples(rf_mod, model_recipe, kfold) # model results collect_metrics(results) ## # A tibble: 2 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 26981. 5 1119. Preprocessor1_Model1 ## 2 rsq standard 0.895 5 0.00862 Preprocessor1_Model1 19.4.1 Knowledge check Using the boston.csv dataset: Apply a random forest model where cmedv is the response variable and use all possible predictor variables. Use the default hyperparameter settings and apply a 5-fold cross validation procedure to assess to assess the model performance. 19.5 Hyperparameters Although random forests perform well out-of-the-box, there are several tunable hyperparameters that we should consider when training a model. Although we briefly discuss the main hyperparameters, Probst, Wright, and Boulesteix (2019) provide a much more thorough discussion. The main hyperparameters to consider include: The number of trees in the forest The number of features to consider at any given split: \\(m_{try}\\) The complexity (depth) of each tree 19.5.1 Number of trees The first consideration is the number of trees within your random forest. Although not technically a hyperparameter, the number of trees needs to be sufficiently large to stabilize the error rate. A good rule of thumb is to start with 10 times the number of features as illustrated below); however, as you adjust other hyperparameters such as \\(m_{try}\\) and node size, more or fewer trees may be required. More trees provide more robust and stable error estimates and variable importance measures; however, the impact on computation time increases linearly with the number of trees. A good rule of thumb is to start with the number of predictor variables (\\(p\\)) times 10 (\\(p \\times 10\\)) trees and adjust as necessary. Figure 19.1: The Ames data has 80 features and starting with 10 times the number of features typically ensures the error estimate converges. 19.5.2 \\(m_{try}\\) The hyperparameter that controls the split-variable randomization feature of random forests is often referred to as \\(m_{try}\\) and it helps to balance low tree correlation with reasonable predictive strength. With regression problems the default value is often \\(m_{try} = \\frac{p}{3}\\) and for classification \\(m_{try} = \\sqrt{p}\\). However, when there are fewer relevant predictors (e.g., noisy data) a higher value of \\(m_{try}\\) tends to perform better because it makes it more likely to select those features with the strongest signal. When there are many relevant predictors, a lower \\(m_{try}\\) might perform better. Start with five evenly spaced values of \\(m_{try}\\) across the range 2–\\(p\\) centered at the recommended default as illustrated below. For the Ames data, an mtry value slightly lower (21) than the default (26) improves performance. Figure 19.2: For the Ames data, an mtry value in the low to mid 20s improves performance. 19.5.3 Tree complexity Random forests are built on individual decision trees; consequently, most random forest implementations have one or more hyperparameters that allow us to control the depth and complexity of the individual trees. This will often include hyperparameters such as node size, max depth, max number of terminal nodes, or the required node size to allow additional splits. Node size is probably the most common hyperparameter to control tree complexity and most implementations use the default values of one for classification and five for regression as these values tend to produce good results (Dı́az-Uriarte and De Andres 2006; Goldstein, Polley, and Briggs 2011). However, Segal (2004) showed that if your data has many noisy predictors and higher \\(m_{try}\\) values are performing best, then performance may improve by increasing node size (i.e., decreasing tree depth and complexity). Moreover, if computation time is a concern then you can often decrease run time substantially by increasing the node size and have only marginal impacts to your error estimate as illustrated below. When adjusting node size start with three values between 1–10 and adjust depending on impact to accuracy and run time. Increasing node size to reduce tree complexity will often have a larger impact on computation speed (right) than on your error estimate. Figure 19.3: Increasing node size to reduce tree complexity will often have a larger impact on computation speed (right) than on your error estimate. 19.5.4 Others There are many other hyperparameters within random forest models; however, the above mentioned ones are the most common and, often, most influential in the performance of our model. For more discussion around random forest hyperparameters see Probst, Wright, and Boulesteix (2019). 19.6 Tuning The following performs a grid search over the mtry (number of features to randomly use for a given tree) and min_n (controls tree depth) hyperparameters. Notice how we don’t actually tune the trees parameter. Rather, setting this to a value greater than the number of features \\(\\times\\) 10 is sufficient. Since we have 80 features we set it to at least, if not greater than \\(80 \\times 10 = 800\\). The following grid search results in a search of 25 different hyperparameter combinations, which results in a grid search time of about 14 minutes! Also, note the importance = “impurity” code we added to set_engine(). We’ll discuss why we add this shortly. # create random forest model object with tuning option rf_mod &lt;- rand_forest( mode = &quot;regression&quot;, trees = 1000, mtry = tune(), min_n = tune() ) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;permutation&quot;) # create the hyperparameter grid hyper_grid &lt;- grid_regular( mtry(range = c(2, 80)), min_n(range = c(1, 20)), levels = 5 ) # train our model across the hyper parameter grid set.seed(123) results &lt;- tune_grid(rf_mod, model_recipe, resamples = kfold, grid = hyper_grid) # model results show_best(results, metric = &quot;rmse&quot;) ## # A tibble: 5 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 41 1 rmse standard 25988. 5 1021. Preprocessor1_Mo… ## 2 60 1 rmse standard 26151. 5 1106. Preprocessor1_Mo… ## 3 21 5 rmse standard 26167. 5 1109. Preprocessor1_Mo… ## 4 21 1 rmse standard 26207. 5 1113. Preprocessor1_Mo… ## 5 41 5 rmse standard 26223. 5 1052. Preprocessor1_Mo… 19.6.1 Knowledge check Using the boston.csv dataset: Apply a random forest model where cmedv is the response variable and use all possible predictor variables. Use a 5-fold cross validation procedure and tune mtry, min_n, and trees. Assess 3 levels of each hyperparameter ranging from: trees: use a range from 50-500 mtry: use a range from 2-15 min_n: use a range 1-10 Which combination of hyperparameters perform best. What is the lowest cross-validated RMSE and how does this compare to previous models on the boston data? 19.7 Feature interpretation Computing feature importance and feature effects for random forests follow the same procedure as discussed in the bagging module. For each tree in our random forest, we compute the sum of the reduction of the loss function across all splits for a given predictor variable. We then aggregate this measure across all trees for each feature. The features with the largest average decrease in SSE (for regression) are considered most important. This is called the “impurity” method for computing feature importance. And to get this measure for our random forests we need to add importance = “impurity” to set_engine() as we did in the last section. # get optimal hyperparameters best_hyperparameters &lt;- select_best(results, metric = &quot;rmse&quot;) # create final workflow object final_rf_wf &lt;- workflow() %&gt;% add_recipe(model_recipe) %&gt;% add_model(rf_mod) %&gt;% finalize_workflow(best_hyperparameters) # fit final workflow object final_fit &lt;- final_rf_wf %&gt;% fit(data = ames_train) # plot feature importance final_fit %&gt;% extract_fit_parsnip() %&gt;% vip(num_features = 20) We can then plot the partial dependence of the most influential feature to see how it influences the predicted values. We see that as overall quality increase we see a significant increase in the predicted sale price. # prediction function pdp_pred_fun &lt;- function(object, newdata) { predict(object, newdata, type = &quot;numeric&quot;)$.pred } # use the pdp package to extract partial dependence predictions # and then plot final_fit %&gt;% pdp::partial( pred.var = &quot;Overall_Qual&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% ggplot(aes(Overall_Qual, yhat)) + geom_boxplot() + scale_y_continuous(labels = scales::dollar) Now let’s plot the PDP for the Gr_Liv_Area and see how that variable relates to the predicted sale price. # prediction function pdp_pred_fun &lt;- function(object, newdata) { mean(predict(object, newdata, type = &quot;numeric&quot;)$.pred) } # use the pdp package to extract partial dependence predictions # and then plot final_fit %&gt;% pdp::partial( pred.var = &quot;Gr_Liv_Area&quot;, pred.fun = pdp_pred_fun, grid.resolution = 10, train = ames_train ) %&gt;% autoplot() + scale_y_continuous(labels = scales::dollar) 19.7.1 Knowledge check Using the boston.csv dataset and the model from the previous Knowledge check that performed best… Plot the top 10 most influential features. Create and explain a PDP plot of the most influential feature. 19.8 Final thoughts Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. They come with all the benefits of decision trees (with the exception of surrogate splits) and bagging but greatly reduce instability and between-tree correlation. And due to the added split variable selection attribute, random forests are also faster than bagging as they have a smaller feature search space at each tree split. However, random forests will still suffer from slow computational speed as your data sets get larger but, similar to bagging, the algorithm is built upon independent steps, and most modern implementations allow for parallelization to improve training time. 19.9 Exercises Using the same kernlab::spam data we saw in the section 12.10… Split the data into 70-30 training-test sets. Apply a default random forest model modeling the type response variable as a function of all available features. Now tune the trees, mtry, and min_n hyperparameters to find the best performing combination of hyperparameters. How does the model performance compare to the decision tree model and bagged decision tree model applied in the previous two lesson exercises? Which 10 features are considered most influential? Are these the same features that have been influential in previous models? Create partial dependence plots for the top two most influential features. Explain the relationship between the feature and the predicted values. References ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Dı́az-Uriarte, Ramón, and Sara Alvarez De Andres. 2006. “Gene Selection and Classification of Microarray Data Using Random Forest.” BMC Bioinformatics 7 (1): 3. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer Series in Statistics New York, NY, USA: Goldstein, Benjamin A, Eric C Polley, and Farren BS Briggs. 2011. “Random Forests for Genetic Association Studies.” Statistical Applications in Genetics and Molecular Biology 10 (1). Probst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018. “Tunability: Importance of Hyperparameters of Machine Learning Algorithms.” arXiv Preprint arXiv:1802.09596. Probst, Philipp, Marvin N Wright, and Anne-Laure Boulesteix. 2019. “Hyperparameters and Tuning Strategies for Random Forest.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1301. Segal, Mark R. 2004. “Machine Learning Benchmarks and Random Forest Regression.” UCSF: Center for Bioinformatics and Molecular Biostatistics. See J. Friedman, Hastie, and Tibshirani (2001) for a mathematical explanation of the tree correlation phenomenon.↩︎ "],["computing-environment.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ────────────────────────────────────────────────────── ## setting value ## version R version 4.4.1 (2024-06-14) ## os macOS Sonoma 14.4.1 ## system x86_64, darwin20 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2024-07-16 ## rstudio 2024.04.2+764 Chocolate Cosmos (desktop) ## pandoc 3.1.11 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/x86_64/ (via rmarkdown) ## ## ─ Packages ────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## baguette * 1.0.2 2024-02-13 [1] CRAN (R 4.4.0) ## broom * 1.0.6 2024-05-17 [1] CRAN (R 4.4.0) ## caret * 6.0-94 2023-03-21 [1] CRAN (R 4.4.0) ## DiagrammeR * 1.0.11 2024-02-02 [1] CRAN (R 4.4.0) ## dials * 1.2.1 2024-02-22 [1] CRAN (R 4.4.0) ## dplyr * 1.1.4 2023-11-17 [1] CRAN (R 4.4.0) ## earth * 5.3.3 2024-02-26 [1] CRAN (R 4.4.0) ## forcats * 1.0.0 2023-01-29 [1] CRAN (R 4.4.0) ## Formula * 1.2-5 2023-02-24 [1] CRAN (R 4.4.0) ## ggplot2 * 3.5.1 2024-04-23 [1] CRAN (R 4.4.0) ## glmnet * 4.1-8 2023-08-22 [1] CRAN (R 4.4.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.4.0) ## infer * 1.0.7 2024-03-25 [1] CRAN (R 4.4.0) ## kableExtra * 1.4.0 2024-01-24 [1] CRAN (R 4.4.0) ## kernlab * 0.9-32 2023-01-31 [1] CRAN (R 4.4.0) ## kknn * 1.3.1 2016-03-26 [1] CRAN (R 4.4.0) ## lattice * 0.22-6 2024-03-20 [1] CRAN (R 4.4.1) ## lubridate * 1.9.3 2023-09-27 [1] CRAN (R 4.4.0) ## Matrix * 1.7-0 2024-04-26 [1] CRAN (R 4.4.1) ## modeldata * 1.4.0 2024-06-19 [1] CRAN (R 4.4.0) ## parsnip * 1.2.1 2024-03-22 [1] CRAN (R 4.4.0) ## pdp * 0.8.1 2022-06-07 [1] CRAN (R 4.4.0) ## plotly * 4.10.4 2024-01-13 [1] CRAN (R 4.4.0) ## plotmo * 3.6.3 2024-02-26 [1] CRAN (R 4.4.0) ## plotrix * 3.8-4 2023-11-10 [1] CRAN (R 4.4.0) ## plotROC * 2.3.1 2023-10-06 [1] CRAN (R 4.4.0) ## purrr * 1.0.2 2023-08-10 [1] CRAN (R 4.4.0) ## randomForest * 4.7-1.1 2022-05-23 [1] CRAN (R 4.4.0) ## ranger * 0.16.0 2023-11-12 [1] CRAN (R 4.4.0) ## readr * 2.1.5 2024-01-10 [1] CRAN (R 4.4.0) ## recipes * 1.1.0 2024-07-04 [1] CRAN (R 4.4.0) ## reshape2 * 1.4.4 2020-04-09 [1] CRAN (R 4.4.0) ## rpart * 4.1.23 2023-12-05 [1] CRAN (R 4.4.1) ## rpart.plot * 3.1.2 2024-02-26 [1] CRAN (R 4.4.0) ## rsample * 1.2.1 2024-03-25 [1] CRAN (R 4.4.0) ## scales * 1.3.0 2023-11-28 [1] CRAN (R 4.4.0) ## stringr * 1.5.1 2023-11-14 [1] CRAN (R 4.4.0) ## TeachingDemos * 2.13 2024-02-16 [1] CRAN (R 4.4.0) ## tibble * 3.2.1 2023-03-20 [1] CRAN (R 4.4.0) ## tidymodels * 1.2.0 2024-03-25 [1] CRAN (R 4.4.0) ## tidyr * 1.3.1 2024-01-24 [1] CRAN (R 4.4.0) ## tidyverse * 2.0.0 2023-02-22 [1] CRAN (R 4.4.0) ## tune * 1.2.1 2024-04-18 [1] CRAN (R 4.4.0) ## vip * 0.4.1 2023-08-21 [1] CRAN (R 4.4.0) ## workflows * 1.1.4 2024-02-19 [1] CRAN (R 4.4.0) ## workflowsets * 1.1.0 2024-03-21 [1] CRAN (R 4.4.0) ## yardstick * 1.3.1 2024-03-21 [1] CRAN (R 4.4.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library ## ## ─ Python configuration ────────────────────────────────────────────── ## python: /Users/b294776/.virtualenvs/r-tensorflow/bin/python ## libpython: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/config-3.10-darwin/libpython3.10.dylib ## pythonhome: /Users/b294776/.virtualenvs/r-tensorflow:/Users/b294776/.virtualenvs/r-tensorflow ## version: 3.10.3 (v3.10.3:a342a49189, Mar 16 2022, 09:34:18) [Clang 13.0.0 (clang-1300.0.29.30)] ## numpy: /Users/b294776/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/numpy ## numpy_version: 1.26.4 ## tensorflow: /Users/b294776/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow ## ## NOTE: Python version was forced by import(&quot;tensorflow&quot;) ## ## ───────────────────────────────────────────────────────────────────── "],["references.html", "References", " References Agresti, Alan. 2003. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley. Bergstra, James, and Yoshua Bengio. 2012. “Random Search for Hyper-Parameter Optimization.” Journal of Machine Learning Research 13 (Feb): 281–305. Box, George EP, and David R Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological), 211–52. Breiman, Leo. 1984. Classification and Regression Trees. Routledge. ———. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Breiman, Leo et al. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Breiman, Leo, and Ross Ihaka. 1984. Nonlinear Discriminant Analysis via Scaling and ACE. Department of Statistics, University of California. Carroll, Raymond J, and David Ruppert. 1981. “On Prediction and the Power Transformation Family.” Biometrika 68 (3): 609–15. Davison, Anthony Christopher, David Victor Hinkley, et al. 1997. Bootstrap Methods and Their Application. Vol. 1. Cambridge University Press. De Cock, Dean. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3). Dietterich, Thomas G. 2000a. “An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization.” Machine Learning 40 (2): 139–57. ———. 2000b. “Ensemble Methods in Machine Learning.” In International Workshop on Multiple Classifier Systems, 1–15. Springer. Dı́az-Uriarte, Ramón, and Sara Alvarez De Andres. 2006. “Gene Selection and Classification of Microarray Data Using Random Forest.” BMC Bioinformatics 7 (1): 3. Efron, Bradley. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association 78 (382): 316–31. Efron, Bradley, and Robert Tibshirani. 1986. “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.” Statistical Science, 54–75. ———. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548–60. Faraway, Julian J. 2016a. Extending the Linear Model with r: Generalized Linear, Mixed Effects and Nonparametric Regression Models. Vol. 124. CRC press. ———. 2016b. Linear Models with r. Chapman; Hall/CRC. Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. Fisher, Walter D. 1958. “On Grouping for Maximum Homogeneity.” Journal of the American Statistical Association 53 (284): 789–98. Friedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics, 1–67. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer Series in Statistics New York, NY, USA: Goldstein, Benjamin A, Eric C Polley, and Farren BS Briggs. 2011. “Random Forests for Genetic Association Studies.” Statistical Applications in Genetics and Molecular Biology 10 (1). Golub, Gene H, Michael Heath, and Grace Wahba. 1979. “Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.” Technometrics 21 (2): 215–23. Harrison Jr, David, and Daniel L Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” Journal of Environmental Economics and Management 5 (1): 81–102. Hastie, T., R. Tibshirani, and M. Wainwright. 2015. Statistical Learning with Sparsity: The Lasso and Generalizations. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis. Hoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67. Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” Journal of Computational and Graphical Statistics 15 (3): 651–74. Hyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts. Kass, Gordon V. 1980. “An Exploratory Technique for Investigating Large Quantities of Categorical Data.” Applied Statistics, 119–27. Kuhn, Max. n.d. “Tidymodels.” https://www.tidymodels.org/start/resampling/. Kuhn, Max. 2014. “Futility Analysis in the Cross-Validation of Machine Learning Models.” arXiv Preprint arXiv:1405.6974. Kuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Vol. 26. Springer. Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li. 2005. Applied Linear Statistical Models. 5th ed. McGraw Hill. Lipovetsky, Stan. 2020. Taylor &amp; Francis. Loh, Wei-Yin, and Nunta Vanichsetakul. 1988. “Tree-Structured Classification via Generalized Discriminant Analysis.” Journal of the American Statistical Association 83 (403): 715–25. Probst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018. “Tunability: Importance of Hyperparameters of Machine Learning Algorithms.” arXiv Preprint arXiv:1802.09596. Probst, Philipp, Marvin N Wright, and Anne-Laure Boulesteix. 2019. “Hyperparameters and Tuning Strategies for Random Forest.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1301. Quinlan, J Ross et al. 1996. “Bagging, Boosting, and C4. 5.” In AAAI/IAAI, Vol. 1, 725–30. Quinlan, J. Ross. 1986. “Induction of Decision Trees.” Machine Learning 1 (1): 81–106. Ripley, Brian D. 2007. Pattern Recognition and Neural Networks. Cambridge University Press. Segal, Mark R. 2004. “Machine Learning Benchmarks and Random Forest Regression.” UCSF: Center for Bioinformatics and Molecular Biostatistics. Surowiecki, James. 2005. The Wisdom of Crowds. Anchor. Therneau, Terry M, Elizabeth J Atkinson, et al. 1997. “An Introduction to Recursive Partitioning Using the RPART Routines.” Mayo Foundation. Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological), 267–88. Wolpert, David H. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
