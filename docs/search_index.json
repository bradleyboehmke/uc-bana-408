[["index.html", "Data Mining with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Mining with R Bradley Boehmke Syllabus This is the primary “textbook” for the Machine Learning section of the UC BANA 4080 Data Mining course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Mining with R! This course provides an intensive, hands-on introduction to data mining and analysis techniques. You will learn the fundamental skills required to extract informative attributes, relationships, and patterns from data sets. You will gain hands-on experience with exploratory data analysis, data visualization, unsupervised learning techniques such as clustering and dimension reduction, and supervised learning techniques such as linear regression, regularized regression, decision trees, random forests, and more! You will also be exposed to some more advanced topics such as ensembling techniques, deep learning, model stacking, and model interpretation. Together, this will provide you with a solid foundation of tools and techniques applied in organizations to aid modern day data-driven decision making. Learning Objectives Upon successfully completing this course, you will be able to: Apply data wrangling techniques to manipulate and prepare data for analysis. Use exploratory data analysis and visualization to provide descriptive insights of data. Apply common unsupervised learning algorithms to find common groupings of observations and features in a given dataset. Describe and apply a sound analytic modeling process. Apply, compare, and contrast various predictive modeling techniques. Have the resources and understanding to continue advancing your data mining and analysis capabilities. …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material This course is split into two main sections - Data Wrangling and Machine Learning. The data wrangling section will provide you the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. The primary course material for this section is provided via this free online book. The second section focused on machine learning section will expose you to several algorithms to identify hidden patterns and relationships within your data. The primary course material for this part of the course is provided via this free online book. There will also be recorded lectures and additional supplementary resources provided via Canvas. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “TODO” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. These labs will be provided via the course website on Canvas and the submission of these labs will also be done through the course website on Canvas. Please check Canvas for due dates for these labs. Projects: There will be two projects designed for you to put to work the tools and knowledge that you gain throughout this course. This provides you with multiple benefits. - It will provide you with more experience using data wrangling tools on real life data sets. - It helps you become a self-directed learner. As a data scientist, a large part of your job is to self-direct your learning and interests to find unique and creative ways to find insights in data. - It starts to build your data science portfolio. Establishing a data science portfolio is a great way to show potential employers your ability to work with data. Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description DATA WRANGLING 1 Introduction R fundamentals &amp; the Rstudio IDE Deeper understanding of vectors 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Data structures &amp; importing data 3 Tidy Data and Data Manipulation Data manipulation &amp; summarization Tidy data 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to text &amp; date-time data 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions MACHINE LEARNING 7 Introduction to Applied Modeling Introduction to tidymodels Feature engineering &amp; model evaluation/selection 8 First regression models Ordinary least squares (OLS) OLS cousins 9 First classification models Logistic regression Assessing classification models 10 More regression cousins Regularized regression Multi-adaptive Regression Splines (MARS) 11 Venturing away from linearity K-Nearest neighbor Decision trees 12 Ensembling trees Bagging Random forests 13 Ensembling trees continued Gradient boosting XGBoost and other variants 14 Deep learning Feedforward neural nets A survey of deep learning extensions 15 Unsupervised Learning Clustering. Dimension reductions Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A video demonstrating this topic is available in Canvas. A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-4080/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA4080 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R Brandon Greenwell, Hands-on Machine Learning with R "],["overview.html", "1 Overview 1.1 Learning objectives 1.2 Estimated time requirement 1.3 Tasks", " 1 Overview Before introducing specific machine learning (ML) algorithms, it is important that we have a solid understanding of the overall objective of ML algorithms and the common problems they can address. Consequently, this module provides an introduction to ML and starts to introduce parts of the ML modeling process that you’ll routinely see in future modeling lessons. 1.1 Learning objectives By the end of this module you should be able to: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic predictive model. 1.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 3 hours Reading + videos: 4 hours 1.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material). "],["lesson-1a-intro-to-machine-learning.html", "2 Lesson 1a: Intro to machine learning 2.1 Learning objectives 2.2 Supervised learning 2.3 Unsupervised learning 2.4 Machine Learning in 2.5 The data sets 2.6 What You’ll Learn Next 2.7 Exercises", " 2 Lesson 1a: Intro to machine learning Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include: Predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge. Segmenting customers based on common attributes or purchasing behavior for targeted marketing. Predicting coupon redemption rates for a given marketing campaign. Predicting customer churn so an organization can perform preventative intervention. And many more! In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision needed during training. 2.1 Learning objectives This lesson will introduce you to some fundamental concepts around ML and this class. By the end of this lesson you will: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Be able to import and explore the data sets we’ll use through various examples. 2.2 Supervised learning A predictive model is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks; using home attributes to predict the sales price; using employee attributes to predict the likelihood of attrition; using patient attributes and symptoms to predict the risk of readmission; using production attributes to predict time to market. Each of these examples has a defined learning task; they each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this course we’ll use various terms interchangeably for \\(X\\): “predictor variable”, “independent variable”, “attribute”, “feature”, “predictor” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the target values. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Most supervised learning problems can be bucketed into one of two categories, regression or classification, which we discuss next. 2.2.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between $80,000 and $755,000). The figure below illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane. Figure 2.1: Average home sales price as a function of year built and total square footage. 2.2.2 Classification problems When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: Did a customer redeem a coupon (coded as yes/no or 1/0)? Did a customer churn (coded as yes/no or 1/0)? Did a customer click on our online ad (coded as yes/no or 1/0)? Classifying customer reviews: Binary: positive vs. negative. Multinomial: extremely negative to extremely positive on a 0–5 Likert scale. Figure 2.2: Classification problem modeling ‘Yes’/‘No’ response based on three features. However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., “yes” or “no”), we often want to predict the probability of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, many of the supervised learning algorithms we cover in this class can be applied to both. These algorithms have become the most popular machine learning applications in recent years. 2.2.3 Knowledge check Identify the features, response variable, and the type of supervised model required for the following tasks: There is an online retailer that wants to predict whether you will click on a certain featured product given your demographics, the current products in your online basket, and the time since your previous purchase. A bank wants to use a customers historical data such as the number of loans they’ve had, the time it took to payoff those loans, previous loan defaults, the number of new loans within the past two years, along with the customers income and level of education to determine if they should issue a new loan for a car. If the bank above does issue a new loan, they want to use the same information to determine the interest rate of the new loan issued. To better plan incoming and outgoing flights, an airline wants to use flight information such as scheduled flight time, day/month of year, number of passengers, airport departing from, airport arriving to, distance to travel, and weather warnings to determine if a flight will be delayed. What if the above airline wants to use the same information to predict the number of minutes a flight will arrive late or early? 2.3 Unsupervised learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression). Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised! Despite its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of unsupervised learning models can be used as inputs to downstream supervised learning models. 2.3.1 Knowledge check Identify the type of unsupervised model required for the following tasks: Say you have a YouTube channel. You may have a lot of data about the subscribers of your channel. What if you want to use that data to detect groups of similar subscribers? Say you’d like to group Ohio counties together based on the demographics of their residents. A retailer has collected hundreds of attributes about all their customers; however, many of those features are highly correlated. They’d like to reduce the number of features down by combining all those highly correlated features into groups. 2.4 Machine Learning in Historically, the R ecosystem provides a wide variety of ML algorithm implementations. This has its benefits; however, this also has drawbacks as it requires the users to learn many different formula interfaces and syntax nuances. More recently, development on a group of packages called Tidymodels has helped to make implementation easier. The tidymodels collection allows you to perform discrete parts of the ML workflow with discrete packages: rsample for data splitting and resampling recipes for data pre-processing and feature engineering parsnip for applying algorithms tune for hyperparameter tuning yardstick for measuring model performance and several others! Throughout this course you’ll be exposed to several of these packages. Go ahead and make sure you have the following packages installed. Just like the tidyverse package, when you install tidymodels you are actually installing several packages that exist in the tidymodels ecosystem as discussed above. # common data wrangling and visualization install.packages(&quot;tidyverse&quot;) install.packages(&quot;vip&quot;) install.packages(&quot;here&quot;) # modeling install.packages(&quot;tidymodels&quot;) packageVersion(&quot;tidymodels&quot;) ## [1] &#39;0.2.0&#39; library(tidymodels) ## ── Attaching packages ──────────────────────────── tidymodels 0.2.0 ── ## ✔ broom 1.0.0 ✔ rsample 0.1.1 ## ✔ dials 1.0.0 ✔ tibble 3.1.8 ## ✔ dplyr 1.0.9 ✔ tidyr 1.2.0 ## ✔ infer 1.0.2 ✔ tune 0.2.0 ## ✔ modeldata 1.0.0 ✔ workflows 0.2.6 ## ✔ parsnip 1.0.0 ✔ workflowsets 0.2.1 ## ✔ purrr 0.3.4 ✔ yardstick 1.0.0 ## ✔ recipes 0.2.0 ## ── Conflicts ─────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks plotly::filter(), stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ recipes::step() masks stats::step() ## • Search for functions across packages at https://www.tidymodels.org/find/ 2.4.1 Knowledge check Check out the Tidymodels website: https://www.tidymodels.org/. Identify which packages can be used for: Efficiently splitting your data Optimizing hyperparameters Measuring the effectiveness of your model Working with correlation matrices 2.5 The data sets The data sets chosen for this course allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this course is to demonstrate how to implement ML workflows, we make the assumption that you have already spent significant time wrangling, cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this course such as: Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process). Recoding variable names and values so that they are meaningful and more interpretable. Tidying data so that each column is a discrete variable and each row is an individual observation. Recoding, removing, or some other approach to handling missing values. Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. As mentioned above, these data sets are fairly common data sets that provide good benchmarks to compare and illustrate ML workflows. Although some of these data sets are available in R, we will import these data sets from a .csv file to ensure consistency over time. 2.5.1 Boston housing The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978) , it contains 13 attributes to predict the median property value. problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes # data file path library(here) data_path &lt;- here(&quot;data&quot;) # access data boston &lt;- readr::read_csv(here(data_path, &quot;boston.csv&quot;)) # initial dimension dim(boston) ## [1] 506 16 # features dplyr::select(boston, -cmedv) ## # A tibble: 506 × 15 ## lon lat crim zn indus chas nox rm age dis rad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.0 42.3 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 ## 2 -71.0 42.3 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 ## 3 -70.9 42.3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 ## 4 -70.9 42.3 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 ## 5 -70.9 42.3 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 ## 6 -70.9 42.3 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 ## 7 -70.9 42.3 0.0883 12.5 7.87 0 0.524 6.01 66.6 5.56 5 ## 8 -70.9 42.3 0.145 12.5 7.87 0 0.524 6.17 96.1 5.95 5 ## 9 -70.9 42.3 0.211 12.5 7.87 0 0.524 5.63 100 6.08 5 ## 10 -70.9 42.3 0.170 12.5 7.87 0 0.524 6.00 85.9 6.59 5 ## # … with 496 more rows, and 4 more variables: tax &lt;dbl&gt;, ## # ptratio &lt;dbl&gt;, b &lt;dbl&gt;, lstat &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(boston$cmedv) ## [1] 24.0 21.6 34.7 33.4 36.2 28.7 2.5.2 Pima Indians Diabetes A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and published in smith1988using , it contains 8 attributes to predict the presence of diabetes. problem type: supervised binary classification response variable: diabetes positive or negative response (i.e. “pos”, “neg”) features: 8 observations: 768 objective: use biological attributes to predict the presence of diabetes # access data pima &lt;- readr::read_csv(here(data_path, &quot;pima.csv&quot;)) # initial dimension dim(pima) ## [1] 768 9 # features dplyr::select(pima, -diabetes) ## # A tibble: 768 × 8 ## pregnant glucose pressure triceps insulin mass pedigree age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 148 72 35 0 33.6 0.627 50 ## 2 1 85 66 29 0 26.6 0.351 31 ## 3 8 183 64 0 0 23.3 0.672 32 ## 4 1 89 66 23 94 28.1 0.167 21 ## 5 0 137 40 35 168 43.1 2.29 33 ## 6 5 116 74 0 0 25.6 0.201 30 ## 7 3 78 50 32 88 31 0.248 26 ## 8 10 115 0 0 0 35.3 0.134 29 ## 9 2 197 70 45 543 30.5 0.158 53 ## 10 8 125 96 0 0 0 0.232 54 ## # … with 758 more rows ## # ℹ Use `print(n = ...)` to see more rows # response variable head(pima$diabetes) ## [1] &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; 2.5.3 Iris flowers The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper (Fisher 1936) . It is sometimes called Anderson’s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. problem type: supervised multinomial classification response variable: species (i.e. “setosa”, “virginica”, “versicolor”) features: 4 observations: 150 objective: use plant leaf attributes to predict the type of flower # access data iris &lt;- readr::read_csv(here(data_path, &quot;iris.csv&quot;)) # initial dimension dim(iris) ## [1] 150 5 # features dplyr::select(iris, -Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows ## # ℹ Use `print(n = ...)` to see more rows # response variable head(iris$Species) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; 2.5.4 Ames housing The Ames housing data set is an alternative to the Boston housing data set and provides a more comprehensive set of home features to predict sales price. More information can be found in De Cock (2011) . problem type: supervised regression response variable: Sale_Price (i.e., $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home # access data ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) # initial dimension dim(ames) ## [1] 2930 81 # features dplyr::select(ames, -Sale_Price) ## # A tibble: 2,930 × 80 ## MS_SubClass MS_Zo…¹ Lot_F…² Lot_A…³ Street Alley Lot_S…⁴ Land_…⁵ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_194… Reside… 141 31770 Pave No_A… Slight… Lvl ## 2 One_Story_194… Reside… 80 11622 Pave No_A… Regular Lvl ## 3 One_Story_194… Reside… 81 14267 Pave No_A… Slight… Lvl ## 4 One_Story_194… Reside… 93 11160 Pave No_A… Regular Lvl ## 5 Two_Story_194… Reside… 74 13830 Pave No_A… Slight… Lvl ## 6 Two_Story_194… Reside… 78 9978 Pave No_A… Slight… Lvl ## 7 One_Story_PUD… Reside… 41 4920 Pave No_A… Regular Lvl ## 8 One_Story_PUD… Reside… 43 5005 Pave No_A… Slight… HLS ## 9 One_Story_PUD… Reside… 39 5389 Pave No_A… Slight… Lvl ## 10 Two_Story_194… Reside… 60 7500 Pave No_A… Regular Lvl ## # … with 2,920 more rows, 72 more variables: Utilities &lt;chr&gt;, ## # Lot_Config &lt;chr&gt;, Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Qual &lt;chr&gt;, Overall_Cond &lt;chr&gt;, ## # Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, ## # Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, ## # Mas_Vnr_Type &lt;chr&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;chr&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(ames$Sale_Price) ## [1] 215000 105000 172000 244000 189900 195500 2.5.5 Attrition The employee attrition data set was originally provided by IBM Watson Analytics Lab and is a fictional data set created by IBM data scientists to explore what employee attributes influence attrition. problem type: supervised binomial classification response variable: Attrition (i.e., “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) # access data attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) # initial dimension dim(attrition) ## [1] 1470 31 # features dplyr::select(attrition, -Attrition) ## # A tibble: 1,470 × 30 ## Age BusinessTra…¹ Daily…² Depar…³ Dista…⁴ Educa…⁵ Educa…⁶ Envir…⁷ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 41 Travel_Rarely 1102 Sales 1 College Life_S… Medium ## 2 49 Travel_Frequ… 279 Resear… 8 Below_… Life_S… High ## 3 37 Travel_Rarely 1373 Resear… 2 College Other Very_H… ## 4 33 Travel_Frequ… 1392 Resear… 3 Master Life_S… Very_H… ## 5 27 Travel_Rarely 591 Resear… 2 Below_… Medical Low ## 6 32 Travel_Frequ… 1005 Resear… 2 College Life_S… Very_H… ## 7 59 Travel_Rarely 1324 Resear… 3 Bachel… Medical High ## 8 30 Travel_Rarely 1358 Resear… 24 Below_… Life_S… Very_H… ## 9 38 Travel_Frequ… 216 Resear… 23 Bachel… Life_S… Very_H… ## 10 36 Travel_Rarely 1299 Resear… 27 Bachel… Medical High ## # … with 1,460 more rows, 22 more variables: Gender &lt;chr&gt;, ## # HourlyRate &lt;dbl&gt;, JobInvolvement &lt;chr&gt;, JobLevel &lt;dbl&gt;, ## # JobRole &lt;chr&gt;, JobSatisfaction &lt;chr&gt;, MaritalStatus &lt;chr&gt;, ## # MonthlyIncome &lt;dbl&gt;, MonthlyRate &lt;dbl&gt;, NumCompaniesWorked &lt;dbl&gt;, ## # OverTime &lt;chr&gt;, PercentSalaryHike &lt;dbl&gt;, PerformanceRating &lt;chr&gt;, ## # RelationshipSatisfaction &lt;chr&gt;, StockOptionLevel &lt;dbl&gt;, ## # TotalWorkingYears &lt;dbl&gt;, TrainingTimesLastYear &lt;dbl&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(attrition$Attrition) ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 2.5.6 Hitters This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. The idea was to illustrate if and how major league baseball player’s batting performance could predict their salary. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. Note that the data does contain the players name but this should be removed during analysis and is not a valid feature. problem type: supervised regression response variable: Salary features: 19 observations: 322 objective: use baseball player’s batting attributes to predict their salary. # access data hitters &lt;- readr::read_csv(here(data_path, &quot;hitters.csv&quot;)) # initial dimension dim(hitters) ## [1] 322 21 # features dplyr::select(hitters, -Salary, -Player) ## # A tibble: 322 × 19 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 293 66 1 30 29 14 1 293 66 1 30 ## 2 315 81 7 24 38 39 14 3449 835 69 321 ## 3 479 130 18 66 72 76 3 1624 457 63 224 ## 4 496 141 20 65 78 37 11 5628 1575 225 828 ## 5 321 87 10 39 42 30 2 396 101 12 48 ## 6 594 169 4 74 51 35 11 4408 1133 19 501 ## 7 185 37 1 23 8 21 2 214 42 1 30 ## 8 298 73 0 24 24 7 3 509 108 0 41 ## 9 323 81 6 26 32 8 2 341 86 6 32 ## 10 401 92 17 49 66 65 13 5206 1332 253 784 ## # … with 312 more rows, and 8 more variables: CRBI &lt;dbl&gt;, ## # CWalks &lt;dbl&gt;, League &lt;chr&gt;, Division &lt;chr&gt;, PutOuts &lt;dbl&gt;, ## # Assists &lt;dbl&gt;, Errors &lt;dbl&gt;, NewLeague &lt;chr&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(hitters$Salary) ## [1] NA 475.0 480.0 500.0 91.5 750.0 2.6 What You’ll Learn Next The lessons that follow are designed to help you understand the individual sub-tasks of an ML project. The focus is to have an intuitive understanding of each discrete sub-task and algorithm. Once you understand when, where, and why these sub-tasks are performed you will be able to transfer this knowledge to other projects. The concepts you will learn include: Provide an overview of the ML modeling process: data splitting model fitting model validation and tuning performance measurement feature engineering Cover common supervised learners: linear regression regularized regression K-nearest neighbors decision trees bagging &amp; random forests gradient boosting Cover common unsupervised learners: K-means clustering Principal component analysis Along the way you’ll learn about: each algorithm’s hyperparameters model interpretation feature importance and more! 2.7 Exercises Identify four real-life applications of supervised and unsupervised problems. Explain what makes these problems supervised versus unsupervised. For each problem identify the target variable (if applicable) and potential features. Identify and contrast a regression problem with a classification problem. What is the target variable in each problem and why would being able to accurately predict this target be beneficial to society? What are potential features and where could you collect this information? What is determining if the problem is a regression or a classification problem? Identify three open source data sets suitable for machine learning (e.g., https://bit.ly/35wKu5c). Explain the type of machine learning models that could be constructed from the data (e.g., supervised versus unsupervised and regression versus classification). What are the dimensions of the data? Is there a code book that explains who collected the data, why it was originally collected, and what each variable represents? If the data set is suitable for supervised learning, which variable(s) could be considered as a useful target? Which variable(s) could be considered as features? Identify examples of misuse of machine learning in society. What was the ethical concern? References "],["lesson-1b-first-model-with-tidymodels.html", "3 Lesson 1b: First model with Tidymodels 3.1 Learning objectives 3.2 Prerequisites 3.3 Data splitting 3.4 Building models 3.5 Making predictions 3.6 Evaluating model performance 3.7 Exercises", " 3 Lesson 1b: First model with Tidymodels Much like exploratory data analysis (EDA), the machine learning (ML) process is very iterative and heuristic-based. With minimal knowledge of the problem or data at hand, it is difficult to know which ML method will perform best. This is known as the no free lunch theorem for ML (Wolpert 1996). Consequently, it is common for many ML approaches to be applied, evaluated, and modified before a final, optimal model can be determined. Performing this process correctly provides great confidence in our outcomes. If not, the results will be useless and, potentially, damaging.1 Approaching ML modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature variables, minimizing data leakage, tuning hyperparameters, and assessing model performance. Many books and courses portray the modeling process as a short sprint. A better analogy would be a marathon where many iterations of these steps are repeated before eventually finding the final optimal model. This process is illustrated below. Figure 3.1: General predictive machine learning process. Before introducing specific algorithms, this lesson introduces concepts that are fundamental to the ML modeling process and that you’ll see briskly covered in future modeling lessons. More specifically, this lesson is designed to get you acquainted with building predictive models using the Tidymodels construct. We’ll focus on the process of splitting our data for improved generalizability, using Tidymodel’s parsnip package for constructing our models, along with yardstick to measure model performance. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. 3.1 Learning objectives By the end of this lesson you will be able to: Split your data into training and test sets. Instantiate, train, fit, and evaluate a basic model. 3.2 Prerequisites For this lesson we’ll primarily use the tidymodels package. library(tidymodels) library(here) The two data sets we’ll use are ames and attrition. data_path &lt;- here(&quot;data&quot;) ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) When performing classification models our response variable needs to be a factor (or sometimes as 0 vs. 1). Consequently, the code chunk below sets the Attrition response variable as a factor rather than as a character. attrition &lt;- attrition %&gt;% dplyr::mutate(Attrition = as.factor(Attrition)) 3.3 Data splitting A major goal of the machine learning process is to find an algorithm \\(f\\left(X\\right)\\) that most accurately predicts future values (\\(\\hat{Y}\\)) based on a set of features (\\(X\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets: Training set: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production). Test set: having chosen a final model, these data are used to estimate an unbiased assessment of the model’s performance, which we refer to as the generalization error. Figure 3.2: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60% (training)–40% (testing), 70%–30%, or 80%–20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind: Spending too much in training (e.g., \\(&gt;80\\%\\)) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting). Sometimes too much spent in testing (\\(&gt;40\\%\\)) won’t allow us to get a good assessment of model parameters. Other factors should also influence the allocation proportions. For example, very large training sets (e.g., \\(n &gt; 100\\texttt{K}\\)) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as \\(p \\geq n\\) (where \\(p\\) represents the number of features), larger samples sizes are often required to identify consistent signals in the features. The two most common ways of splitting data include simple random sampling and stratified sampling. 3.3.1 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the distribution of your response variable (\\(Y\\)). Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this course we’ll often use the seed 123 for reproducibility but the number itself has no special meaning. # create train/test split set.seed(123) # for reproducibility split &lt;- initial_split(ames, prop = 0.7) train &lt;- training(split) test &lt;- testing(split) # dimensions of training data dim(train) ## [1] 2051 81 With sufficient sample size, this sampling approach will typically result in a similar distribution of \\(Y\\) (e.g., Sale_Price in the ames data) between your training and test sets, as illustrated below. train %&gt;% mutate(id = &#39;train&#39;) %&gt;% bind_rows(test %&gt;% mutate(id = &#39;test&#39;)) %&gt;% ggplot(aes(Sale_Price, color = id)) + geom_density() 3.3.2 Stratified sampling If we want to explicitly control the sampling so that our training and test sets have similar \\(Y\\) distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90% of observations with response “Yes” and 10% with response “No”). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will segment \\(Y\\) into quantiles and randomly sample from each. To perform stratified sampling we simply apply the strata argument in initial_split. set.seed(123) split_strat &lt;- initial_split(attrition, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions. # original response distribution table(attrition$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8387755 0.1612245 # response distribution for training data table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8394942 0.1605058 # response distribution for test data table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8371041 0.1628959 3.3.3 Knowledge check Import the penguins data from the modeldata package Create a 70-30 stratified train-test split (species is the target variable). What are the response variable proportions for the train and test data sets? 3.4 Building models The R ecosystem provides a wide variety of ML algorithm implementations. This makes many powerful algorithms available at your fingertips. Moreover, there are almost always more than one package to perform each algorithm (e.g., there are over 20 packages for fitting random forests). There are pros and cons to this wide selection; some implementations may be more computationally efficient while others may be more flexible. This also has resulted in some drawbacks as there are inconsistencies in how algorithms allow you to define the formula of interest and how the results and predictions are supplied. Fortunately, the tidymodels ecosystem is simplifying this and, in particular, the Parsnip package provides one common interface to train many different models supplied by other packages. Consequently, we’ll focus on building models the tidymodels way. To create and fit a model with parsnip we follow 3 steps: Create a model type Choose an “engine” Fit our model Let’s illustrate by building a linear regression model. For our first model we will simply use two features from our training data - total square feet of the home (Gr_Liv_Area) and year built (Year_Built) to predict the sale price (Sale_Price). We can use tidy() to get results of our model’s parameter estimates and their statistical properties. Although the summary() function can provide this output, it gives the results back in an unwieldy format. Go ahead, and run summary(lm_ols) to compare the results to what we see below. Many models have a tidy() method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names) lm_ols &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) tidy(lm_ols) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2157423. 69234. -31.2 8.09e-175 ## 2 Gr_Liv_Area 94.4 2.12 44.4 2.54e-302 ## 3 Year_Built 1114. 35.5 31.4 5.30e-177 Now, you may have noticed that I only applied two of the three steps I mentioned previously: Create a model type Choose an “engine” Fit our model The reason is because most model objects (linear_reg() in this example) have a default engine. linear_reg() by default uses lm for ordinary least squares. But we can always change the engine. For example, say I wanted to use keras to perform gradient descent linear regression, then I could change the engine to keras but use the same code workflow. For this code to run successfully on your end you need to have the keras and tensorflow packages installed on your machine. Depending on your current setup this could be an easy process or you could run into problems. If you run into problems don’t fret, this is primarily just to illustrate how we can change engines. lm_sgd &lt;- linear_reg() %&gt;% set_engine(&#39;keras&#39;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) When we talk about ‘engines’ we’re really just referring to packages that provide the desired algorithm. Each model object has different engines available to use and they are all documented. For example check out the help file for linear_reg (?linear_reg) and you’ll see the different engines available (lm, brulee, glm, glmnet, etc.) The beauty of this workflow is that if we want to explore different models we can simply change the model object. For example, say we wanted to run a K-nearest neighbor model. We can just use nearest_neighbor(). In this example we have pretty much the same code as above except we added the line of code set_mode(). This is because most algorithms require you to specify if you are building a regression model or a classification model. When you run this code you’ll probably get an error message saying that “This engine requires some package installs: ‘kknn’.” This just means you need to install.packages(‘kknn’) and then you should be able to successfully run this code. knn &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) You can see all the different model objects available at https://parsnip.tidymodels.org/reference/index.html 3.4.1 Knowledge check If you haven’t already done so, create a 70-30 stratified train-test split on the attrition data (note: Attrition is the response variable). Using the logistic_reg() model object, fit a model using Age, DistanceFromHome, and JobLevel as the features. Now train a K-nearest neighbor model using the ‘kknn’ engine and be sure to set the mode to be a classification model. 3.5 Making predictions We have fit a few different models. Now, if we want to see our predictions we can simply apply predict() and feed it the data set we want to make predictions on. Here, we can see the predictions made on our training data for our ordinary least square linear regression model. lm_ols %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 217657. ## 2 214276. ## 3 223425. ## 4 260324. ## 5 109338. ## 6 195106. ## 7 222217. ## 8 126175. ## 9 98550. ## 10 120811. ## # … with 2,041 more rows ## # ℹ Use `print(n = ...)` to see more rows And here we get the predicted values for our KNN model. knn %&gt;% predict(train) ## # A tibble: 2,051 × 1 ## .pred ## &lt;dbl&gt; ## 1 194967. ## 2 192240 ## 3 174220 ## 4 269760 ## 5 113617. ## 6 173672 ## 7 174820 ## 8 120796 ## 9 114560 ## 10 121346 ## # … with 2,041 more rows ## # ℹ Use `print(n = ...)` to see more rows 3.5.1 Knowledge check Make predictions on the test data using the logistic regression model you built on the attrition data. Now make predictions using the K-nearest neighbor model. 3.6 Evaluating model performance It is important to understand how our model is performing. With ML models, measuring performance means understanding the predictive accuracy – the difference between a predicted value and the actual value. We measure predictive accuracy with loss functions. There are many loss functions to choose from when assessing the performance of a predictive model, each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the “optimal model”. Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric. 3.6.1 Regression models The most common loss functions for regression models include: MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\))2. The squared component results in larger errors having larger penalties. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Let’s compute the RMSE of our OLS regression model. Remember, we want to assess our model’s performance on the test data not the training data since that gives us a better idea of how our model generalizes. To do so, the following: Makes predictions with our test data, Adds the actual Sale_Price values from our test data, Computes the RMSE. lm_ols %&gt;% predict(test) %&gt;% bind_cols(test %&gt;% select(Sale_Price)) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 45445. The RMSE value suggests that, on average, our model mispredicts the expected sale price of a home by about $45K. 3.6.2 Classification models There are many loss functions used for classification models. For simplicity we’ll just focus on the overall classification accuracy. I’ll illustrate with the attrition data. Here, we build a logistic regression model that seeks to predict Attrition based on all available features. In R, using a “.” as in Attrition ~ . is a shortcut for saying use all available features to predict Attrition. We then follow the same process as above to make predictions on the test data, add the actual test values for Attrition, and then compute the accuracy rate. logit &lt;- logistic_reg() %&gt;% fit(Attrition ~ ., data = train_strat) logit %&gt;% predict(test_strat) %&gt;% bind_cols(test_strat %&gt;% select(Attrition)) %&gt;% accuracy(truth = Attrition, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.876 3.6.3 Knowledge check Compute the accuracy rate of your logistic regression model for the attrition data. Now compute the accuracy rate of your K-nearest neighbor model. 3.7 Exercises For this exercise we’ll use the Boston housing data set. The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978), it contains 13 attributes to predict the median property value. Data attributes: problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes Modeling tasks: Import the Boston housing data set (boston.csv) and split it into a training set and test set using a 70-30% split. How many observations are in the training set and test set? Compare the distribution of cmedv between the training set and test set. Fit a linear regression model using all available features to predict cmedv and compute the RMSE on the test data. Fit a K-nearest neighbor model that uses all available features to predict cmedv and compute the RMSE on the test data. How do these models compare? References "],["overview-1.html", "4 Overview 4.1 Learning objectives 4.2 Estimated time requirement 4.3 Tasks", " 4 Overview In the last module we discussed the basics of fitting a model. Part of this process included creating a model type and we illustrated how to apply linear regression, K-nearest neighbor, and logistic regression models. Yet, we didn’t really discuss these algorithms and what they are doing under the hood. We’ll turn our attention to that now and we’ll start by looking at a fundamental algorithm – linear regression. 4.1 Learning objectives By the end of this module you should be able to: Explain how a linear regression model characterizes the data it is applied to. Fit, interpret, and assess the performance of simple and multiple linear regression models. 4.2 Estimated time requirement The estimated time to go through the module lessons is about: Reading only: 3 hours Reading + videos: 4 hours 4.3 Tasks Work through the 2 module lessons. Upon finishing each lesson take the associated lesson quizzes on Canvas. Be sure to complete the lesson quiz no later than the due date listed on Canvas. Check Canvas for this week’s lab, lab quiz due date, and any additional content (i.e. in-class material) "],["lesson-2a-simple-linear-regression.html", "5 Lesson 2a: Simple linear regression 5.1 Learning objectives 5.2 Prerequisites 5.3 Correlation 5.4 Simple linear regression 5.5 Making predictions 5.6 Assessing model accuracy 5.7 Exercises 5.8 Other resources", " 5 Lesson 2a: Simple linear regression Linear regression, a staple of classical statistical modeling, is one of the simplest algorithms for doing supervised learning. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later modules, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good starting point for more advanced approaches because many of the more sophisticated statistical learning approaches can be seen as generalizations to or extensions of ordinary linear regression. Consequently, it is important to have a good understanding of linear regression before studying more complex learning methods. This lesson introduces simple linear regression with an emphasis on prediction, rather than explanation. Modeling for explanation: When you want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(x\\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables. Modeling for prediction: When you want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\). Check out Lipovetsky (2020) for a great introduction to linear regression for explanation. 5.1 Learning objectives By the end of this lesson you will know how to: Fit a simple linear regression model. Interpret results of a simple linear regression model. Assess the performance of a simple linear regression model. 5.2 Prerequisites This lesson leverages the following packages: # Data wrangling &amp; visualization packages library(tidyverse) # Modeling packages library(tidymodels) We’ll also continue working with the ames data set: # stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 5.3 Correlation Correlation is a single-number statistic that measures the extent that two variables are related (“co-related”) to one another. For example, say we want to understand the relationship between the total above ground living space of a home (Gr_Liv_Area) and the home’s sale price (Sale_Price). Looking at the following scatter plot we can see that some relationship does exist. It appears that as Gr_Liv_Area increases the Sale_Price of a home increases as well. ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) Correlation allows us to quantify this relationship. We can compute the correlation with the following: ames_train %&gt;% summarize(correlation = cor(Gr_Liv_Area, Sale_Price)) ## # A tibble: 1 × 1 ## correlation ## &lt;dbl&gt; ## 1 0.708 The value of the correlation coefficient varies between +1 and -1. In our example, the correlation coefficient is 0.71. When the value of the correlation coefficient lies around ±1, then it is said to be a perfect degree of association between the two variables (near +1 implies a strong positive association and near -1 implies a strong negative association). As the correlation coefficient nears 0, the relationship between the two variables weakens with a near 0 value implying no association between the two variables. So, in our case we could say we have a moderate positive correlation between Gr_Liv_Area and Sale_Price. Let’s look at another relationship. In the following we look at the relationship between the unfinished basement square footage of homes (Bsmt_Unf_SF) and the Sale_Price. ggplot(ames_train, aes(Bsmt_Unf_SF, Sale_Price)) + geom_point(size = 1.5, alpha = .25) In this example, we don’t see much of a relationship. Basically, as Bsmt_Unf_SF gets larger or smaller, we really don’t see a strong pattern with Sale_Price. If we look at the correlation for this relationship, we see that the correlation coefficient is much closer to zero than to 1. This confirms our visual assessment that there does not seem to be much of a relationship between these two variables. ames_train %&gt;% summarize(correlation = cor(Bsmt_Unf_SF, Sale_Price)) ## # A tibble: 1 × 1 ## correlation ## &lt;dbl&gt; ## 1 0.186 5.3.1 Knowledge check Interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the 80s-style video game called, “Guess the Correlation”, at http://guessthecorrelation.com/ Using the ames_train data, visualize the relationship between Year_Built and Sale_Price. Guess what the correlation is between these two variables? Now compute the correlation between these two variables. Although a useful measure, correlation can be hard to imagine exactly what the association is between two variables based on this single statistic. Moreover, its important to realize that correlation assumes a linear relationship between two variables. For example, let’s check out the anscombe data, which is a built-in data set provided in R. If we look at each x and y relationship visually, we can see significant differences: p1 &lt;- qplot(x = x1, y = y1, data = anscombe) p2 &lt;- qplot(x = x2, y = y2, data = anscombe) p3 &lt;- qplot(x = x3, y = y3, data = anscombe) p4 &lt;- qplot(x = x4, y = y4, data = anscombe) gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2) However, if we compute the correlation between each of these relationships we see that they all have nearly equal correlation coefficients! Never take a correlation coefficient at face value! You should always compare the visual relationship with the computed correlation value. anscombe %&gt;% summarize( corr_x1_y1 = cor(x1, y1), corr_x2_y2 = cor(x2, y2), corr_x3_y3 = cor(x3, y3), corr_x4_y4 = cor(x4, y4) ) ## corr_x1_y1 corr_x2_y2 corr_x3_y3 corr_x4_y4 ## 1 0.8164205 0.8162365 0.8162867 0.8165214 There are actually several different ways to measure correlation. The most common, and the one we’ve been using here, is Pearson’s correlation. Alternative methods allow us to loosen some assumptions such as assuming a linear relationship. You can read more at http://uc-r.github.io/correlations. 5.4 Simple linear regression As discussed in the last section, correlation is often used to quantify the strength of the linear association between two continuous variables. However, this statistic alone does not provide us with a lot of actionable insights. But we can build on the concept of correlation to provide us with more useful information. In this section, we seek to fully characterize the linear relationship we measured with correlation using a method called simple linear regression (SLR). 5.4.1 Best fit line Let’s go back to our plot illustrating the relationship between Gr_Liv_Area and Sale_Price. We can characterize this relationship with a linear line that we consider is the “best-fitting” line (we’ll define “best-fitting” in a little bit). We do this by adding overplotting with geom_smooth(method = \"lm\", se = FALSE) ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) The line in the above plot is called a “regression line.” The regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable Sale_Price and the explanatory variable Gr_Liv_Area. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.71 suggesting that there is a positive relationship between these two variables. 5.4.2 Estimating “best fit” You may recall from secondary/high school algebra that the equation of a line is \\(y = a + b \\times x\\). Often, in formulas like these we illustrate multiplication without the \\(\\times\\) symbol like the following \\(y = a + bx\\). The equation of our line is defined by two coefficients \\(a\\) and \\(b\\). The intercept coefficient \\(a\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(b\\) for \\(x\\) is the increase in \\(y\\) for every increase of one in \\(x\\). This is also called the “rise over run.” However, when defining a regression line like the regression line in the previous plot, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 x\\). The intercept coefficient is \\(b_0\\), so \\(b_0\\) is the value of \\(\\widehat{y}\\) when \\(x = 0\\). The slope coefficient for \\(x\\) is \\(b_1\\), i.e., the increase in \\(\\widehat{y}\\) for every increase of one unit in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a “fitted value,” or the value of \\(y\\) on the regression line for a given \\(x\\) value. So what are the coefficients of our best fit line that characterizes the relationship between Gr_Liv_Area and Sale_Price? We can get that by fitting an SLR model where Sale_Price is our response variable and Gr_Liv_Area is our single predictor variable. Once our model is fit we can extract our fitted model results with tidy(): model1 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area, data = ames_train) tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 The estimated coefficients from our model are \\(b_0 =\\) 15938.17 and \\(b_1 =\\) 109.67. To interpret, we estimate that the mean selling price increases by 109.67 for each additional one square foot of above ground living space. With these coefficients, we can look at our scatter plot again (this time with the x &amp; y axes formatted) and compare the characterization of our linear line with the coefficients. This simple description of the relationship between the sale price and square footage using a single number (i.e., the slope) is what makes linear regression such an intuitive and popular modeling tool. ggplot(ames_train, aes(Gr_Liv_Area, Sale_Price)) + geom_point(size = 1.5, alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::dollar) This is great but you may still be asking how we are estimating the coefficients? Ideally, we want estimates of \\(b_0\\) and \\(b_1\\) that give us the “best fitting” line. But what is meant by “best fitting”? The most common approach is to use the method of least squares (LS) estimation; this form of linear regression is often referred to as ordinary least squares (OLS) regression. There are multiple ways to measure “best fitting”, but the LS criterion finds the “best fitting” line by minimizing the residual sum of squares (RSS). Before we define RSS, let’s first define what a residual is. Let’s look at a single home. This home has 3,608 square feet of living space and sold for $475,000. In other words, \\(x = 3608\\) and \\(y = 475000\\). ## # A tibble: 1 × 2 ## Gr_Liv_Area Sale_Price ## &lt;int&gt; &lt;int&gt; ## 1 3608 475000 Based on our linear regression model (or the intercept and slope we identified from our model) our best fit line estimates that this house’s sale price is \\[\\widehat{y} = b_0 + b_1 \\times x = 15938.1733 + 109.6675 \\times 3608 = 411618.5\\] We can visualize this in our plot where we have the actual Sale_Price (orange) and the estimated Sale_Price based on our fitted line. The difference between these two values (\\(y - \\widehat{y} = 475000 - 411618.5 = 63381.5\\)) is what we call our residual. It is considered the error for this observation, which we can visualize with the red line. Now, if we look across all our data points you will see that each one has a residual associated with it. In the right plot, the vertical lines represent the individual residuals/errors associated with each observation. Figure 5.1: The least squares fit from regressing sale price on living space for the the Ames housing data. Left: Fitted regression line. Right: Fitted regression line with vertical grey bars representing the residuals. The OLS criterion identifies the “best fitting” line that minimizes the sum of squares of these residuals. Mathematically, this is computed by taking the sum of the squared residuals (or as stated before the residual sum of squares –&gt; “RSS”). \\[\\begin{equation} RSS = \\sum_{i=1}^n\\left(y_i - \\widehat{y_i}\\right)^2 \\end{equation}\\] where \\(y_i\\) and \\(\\widehat{y_i}\\) just mean the actual and predicted response values for the ith observation. 5.4.3 Inference Let’s go back to our model1 results that show the \\(b_0\\) and \\(b_1\\) coefficient values: tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 Note that we call these coefficient values “estimates.” Due to various reasons we should always assume that there is some variability in our estimated coefficient values. The variability of an estimate is often measured by its standard error (SE). When we fit our linear regression model the SE for each coefficient was computed for us and are displayed in the column labeled std.error in the output from tidy(). From this, we can also derive simple \\(t\\)-tests to understand if the individual coefficients are statistically significant from zero. The t-statistics for such a test are nothing more than the estimated coefficients divided by their corresponding estimated standard errors (i.e., in the output from tidy(), t value (aka statistic) = estimate / std.error). The reported t-statistics measure the number of standard deviations each coefficient is away from 0. Thus, large t-statistics (greater than two in absolute value, say) roughly indicate statistical significance at the \\(\\alpha = 0.05\\) level. The p-values for these tests are also reported by tidy() in the column labeled p.value. This may seem quite complicated but don’t worry, R will do the heavy lifting for us. Just realize we can use these additional statistics provided in our model summary to tell us if the predictor variable (Gr_Liv_Area in our example) has a statistically significant relationship with our response variable. When the p.value for a given coefficient is quite small (i.e. p.value &lt; 0.005), that is a good indication that the estimate for that coefficient is statistically different than zero. For example, the p.value for the Gr_Liv_Area coefficient is 5.17e-311 (basically zero). This means that the estimated coefficient value of 109.6675 is statistically different than zero. Let’s look at this from another perspective. We can compute the 95% confidence intervals for the coefficients in our SLR example. confint(model1$fit, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 8384.213 23492.1336 ## Gr_Liv_Area 104.920 114.4149 To interpret, we estimate with 95% confidence that the mean selling price increases between 104.92 and 114.41 for each additional one square foot of above ground living space. We can also conclude that the slope \\(b_1\\) is significantly different from zero (or any other pre-specified value not included in the interval) at the \\(\\alpha = 0.05\\) level (\\(\\alpha = 0.05\\) because we just take 1 - confidence level we are computing so \\(1 - 0.95 = 0.05\\)). 5.4.4 Knowledge check Let’s revisit the relationship between Year_Built and Sale_Price. Using the ames_train data: Visualize the relationship between these two variables. Compute their correlation. Create a simple linear regression model where Sale_Price is a function of Year_Built. Interpret the coefficient for Year_Built. What is the 95% confidence interval for this coefficient and can we confidently say it is statistically different than zero? 5.5 Making predictions We’ve created a simple linear regression model to describe the relationship between Gr_Liv_Area and Sale_Price. As we saw in the last module, we can make predictions with this model. model1 %&gt;% predict(ames_train) ## # A tibble: 2,049 × 1 ## .pred ## &lt;dbl&gt; ## 1 135695. ## 2 135695. ## 3 107620. ## 4 98408. ## 5 126922. ## 6 224526. ## 7 114639. ## 8 129992. ## 9 205444. ## 10 132515. ## # … with 2,039 more rows ## # ℹ Use `print(n = ...)` to see more rows And we can always add these predictions back to our training data if we want to look at how the predicted values differ from the actual values. model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% select(Gr_Liv_Area, Sale_Price, .pred) ## # A tibble: 2,049 × 3 ## Gr_Liv_Area Sale_Price .pred ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1092 105500 135695. ## 2 1092 88000 135695. ## 3 836 120000 107620. ## 4 752 125000 98408. ## 5 1012 67500 126922. ## 6 1902 112000 224526. ## 7 900 122000 114639. ## 8 1040 127000 129992. ## 9 1728 84900 205444. ## 10 1063 128000 132515. ## # … with 2,039 more rows ## # ℹ Use `print(n = ...)` to see more rows 5.6 Assessing model accuracy This allows us to assess the accuracy of our model. Recall from the last module that for regression models we often use mean squared error (MSE) and root mean squared error (RMSE) to quantify the accuracy of our model. These two values are directly correlated to the RSS we discussed above, which determines the best fit line. Let’s illustrate. 5.6.1 Training data accuracy Recall that the residuals are the differences between the actual \\(y\\) and the estimated \\(\\widehat{y}\\) based on the best fit line. residuals &lt;- model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% select(Gr_Liv_Area, Sale_Price, .pred) %&gt;% mutate(residual = Sale_Price - .pred) head(residuals, 5) ## # A tibble: 5 × 4 ## Gr_Liv_Area Sale_Price .pred residual ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1092 105500 135695. -30195. ## 2 1092 88000 135695. -47695. ## 3 836 120000 107620. 12380. ## 4 752 125000 98408. 26592. ## 5 1012 67500 126922. -59422. The RSS squares these values and then sums them. residuals %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize(sum_of_squared_residuals = sum(squared_residuals)) ## # A tibble: 1 × 1 ## sum_of_squared_residuals ## &lt;dbl&gt; ## 1 6.60e12 Why do we square the residuals? So that both positive and negative deviations of the same amount are treated equally. While taking the absolute value of the residuals would also treat both positive and negative deviations of the same amount equally, squaring the residuals is used for reasons related to calculus: taking derivatives and minimizing functions. If you’d like to learn more we suggest you consult one of the textbooks referenced at the end of the lesson. However, when expressing the performance of a model we rarely state the RSS. Instead it is more common to state the average of the squared error, or the MSE as discussed here. Unfortunately, both the RSS and MSE are not very intuitive because the units the metrics are expressed in do have much meaning. So, we usually use the RMSE metric, which simply takes the square root of the MSE metric so that your error metric is in the same units as your response variable. We can manually compute this with the following, which tells us that on average, our linear regression model mispredicts the expected sale price of a home by about $56,760. residuals %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize( MSE = mean(squared_residuals), RMSE = sqrt(MSE) ) ## # A tibble: 1 × 2 ## MSE RMSE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3221722037. 56760. We could also compute this using the rmse() function we saw in the last module: model1 %&gt;% predict(ames_train) %&gt;% bind_cols(ames_train) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 56760. 5.6.2 Test data accuracy Recall that a major goal of the machine learning process is to find a model that most accurately predicts future values based on a set of features. In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. In the last module we called this our generalization error. So, ultimately, we want to understand how well our model will generalize to unseen data. To do this we need to compute the RMSE of our model on our test set. Here, we see that our test RMSE is right around the same as our training data. As we’ll see in later modules, this is not always the case. model1 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 55942. 5.6.3 Knowledge check Let’s revisit the simple linear regression model you created earlier with Year_Built and Sale_Price. Using this model, make predictions using the test data. What is the predicted value for the first home in the test data? Compute the generalization RMSE for this model. Interpret the generalization RMSE. How does this model compare to the model based on Gr_Liv_Area? 5.7 Exercises Using the Boston housing data set where the response feature is the median value of homes within a census tract (cmedv): Split the data into 70-30 training-test sets. Using the training data, pick a single feature variable and… Visualize the relationship between that feature and cmedv. Compute the correlation between that feature and cmedv. Create a simple linear regression model with cmedv as a function of that feature variable. Interpret the feature’s coefficient. What is the model’s generalization error? Now pick another feature variable and repeat the process in #2. 5.8 Other resources Some execellent resources to go deeper into linear regression: Kutner et al. (2005) for an excellent and comprehensive overview of linear regression. Faraway (2016) for a thorough discussion of linear regression in R. Lipovetsky (2020) for a great introduction to linear regression for explanation rather than prediction. References "],["lesson-2b-multiple-linear-regression.html", "6 Lesson 2b: Multiple linear regression 6.1 Learning objectives 6.2 Prerequisites 6.3 Adding additional predictors 6.4 Interactions 6.5 Qualitative predictors 6.6 Including many predictors 6.7 Feature importance 6.8 Exercises", " 6 Lesson 2b: Multiple linear regression In the last lesson we learned how to use one predictor variable to predict a numeric response. However, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (Gr_Liv_Area) and the year the house was built (Year_Built) are (linearly) related to sale price (Sale_Price). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the multiple linear regression (MLR) model and is the focus for this lesson. 6.1 Learning objectives By the end of this lesson you will know how to: Fit, interpret, and assess the performance of a multiple linear regression model. Include categorical features in a linear regression model and interpret their results. Asses the most influential predictor variables in a linear regression model. 6.2 Prerequisites This lesson leverages the following packages: # Data wrangling &amp; visualization packages library(tidyverse) # Modeling packages library(tidymodels) We’ll also continue working with the ames data set: # stratified sampling with the rsample package ames &lt;- AmesHousing::make_ames() set.seed(123) split &lt;- initial_split(ames, prop = 0.7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(split) ames_test &lt;- testing(split) 6.3 Adding additional predictors In the last lesson we saw how we could use the above ground square footage (Gr_Liv_Area) of a house to predict the sale price (Sale_Price). model1 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area, data = ames_train) tidy(model1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15938. 3852. 4.14 3.65e- 5 ## 2 Gr_Liv_Area 110. 2.42 45.3 5.17e-311 From our model we interpreted the results as that the mean selling price increases by 109.67 for each additional one square foot of above ground living space. We also determined that the Gr_Liv_Area coefficient is statistically different from zero based on the p.value. And, we saw that our model has a generalization RMSE value of 55942, which means that on average, our model’s predicted sales price differs from the actual sale price by $55,942. model1 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 55942. However, we are only using a single predictor variable to try predict the sale price. In reality, we likely can use other home attributes to do a better job at predicting sale price. For example, we may wish to understand if above ground square footage (Gr_Liv_Area) and the year the house was built (Year_Built) are (linearly) related to sale price (Sale_Price). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the multiple linear regression (MLR) model. With two predictors, the MLR model becomes: \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2, \\end{equation}\\] where \\(x_1\\) and \\(x_2\\) are features of interest. In our Ames housing example, \\(x_1\\) can represent Gr_Liv_Area and \\(x_2\\) can represent Year_Built. In R, multiple linear regression models can be fit by separating all the features of interest with a +: model2 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train) tidy(model2) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2102905. 69441. -30.3 9.17e-167 ## 2 Gr_Liv_Area 93.8 2.07 45.3 1.17e-310 ## 3 Year_Built 1087. 35.6 30.5 3.78e-169 The LS estimates of the regression coefficients are \\(\\widehat{b}_1 =\\) 93.828 and \\(\\widehat{b}_2 =\\) 1086.852 (the estimated intercept is -2.1029046^{6}. In other words, every one square foot increase to above ground square footage is associated with an additional $93.83 in mean selling price when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of $1,086.85 in selling price when holding the above ground square footage constant. As our model results show above, the p.values for our two coefficients suggest that both are stastically different than zero. This can also be confirmed by computing our confidence intervals around these coefficient estimates as we did before. confint(model2$fit) ## 2.5 % 97.5 % ## (Intercept) -2.239086e+06 -1.966723e+06 ## Gr_Liv_Area 8.976367e+01 9.789288e+01 ## Year_Built 1.017072e+03 1.156632e+03 Now, instead of modeling sale price with the the “best fitting” line that minimizes residuals, we are modeling sale price with the best fitting hyperplane that minimizes the residuals, which is illustrated below. Figure 6.1: Average home sales price as a function of year built and total square footage. 6.3.1 Knowledge check Using the ames_train data: Fit a MLR model where Sale_Price is a function of Gr_Liv_Area and Garage_Cars. Interpret the coefficients. Are they both statistically different from zero? Compute and interpret the generalization RMSE for this model. How does this model compare to the model based on just Gr_Liv_Area? 6.4 Interactions You may notice that the fitted plane in the above image is flat; there is no curvature. This is true for all linear models that include only main effects (i.e., terms involving only a single predictor). One way to model curvature is to include interaction effects. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. Suppose that when people buy older homes they care more about the historical nature and beauty of the home rather than the total square footage. However, as older historical homes grow larger in size we see a compounding impact to the value of the home. This is known as a synergy effect – as one feature changes there is a larger or smaller effect of the other feature. In linear regression, interactions can be captured via products of features (i.e., \\(x_1 \\times x_2\\)). A model with two main effects can also include a two-way interaction. For example, to include an interaction between \\(x_1 =\\) Gr_Liv_Area and \\(x_2 =\\) Year_Built, we introduce an additional product term: \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1 x_2. \\end{equation}\\] Note that in R, we use the : operator to include an interaction (technically, we could use * as well, but x1 * x2 is shorthand for x1 + x2 + x1:x2 so is slightly redundant): interaction_model &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train) tidy(interaction_model) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -810498. 210537. -3.85 1.22e- 4 ## 2 Gr_Liv_Area -729. 127. -5.75 1.01e- 8 ## 3 Year_Built 431. 107. 4.03 5.83e- 5 ## 4 Gr_Liv_Area:Year_Built 0.417 0.0642 6.49 1.04e-10 In this example, we see that the two main effects (Gr_Liv_Area &amp; Year_Built) are statistically significant and so is the interaction term (Gr_Liv_Area:Year_Built). So how do we interpret these results? Well, we can say that for every 1 additional square feet in Gr_Liv_Area, the Sale_Price of a home increases by \\(b_1 + b_3 \\times \\text{Year_Built}\\) = -728.5084 + 0.4168489 x Year_Built. Likewise, for each additional year that a home was built, the Sale_Price of a home increases by \\(b_2 + b_3 \\times \\text{Gr_Liv_Area}\\) = 430.8755 + 0.4168489 x Gr_Liv_Area. Adding an interaction term now makes the change in one variable non-linear because it includes an additional change based on another feature. This non-linearity (or curvature) that interactions capture can be illustrated in the contour plot below. The left plot illustrates a regression model with main effects only. Note how the fitted regression surface is flat (i.e., it does not twist or bend). While the fitted regression surface with interaction is displayed in the right side plot and you can see the curvature of the relationship induced. Figure 6.2: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The ‘best-fit’ plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane). Interaction effects are quite prevalent in predictive modeling. Since linear models are an example of parametric modeling, it is up to the analyst to decide if and when to include interaction effects. This becomes quite tedious and unrealistic for larger data sets. In later lessons, we’ll discuss algorithms that can automatically detect and incorporate interaction effects (albeit in different ways). For now, just realize that adding interactions is possible with MLR models. 6.5 Qualitative predictors In our discussion so far, we have assumed that all variables in our linear regression model are quantitative. But in practice, this is not necessarily the case; often some predictors are qualitative. For example, the Credit data set provided by the ISLR package records the balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating). credit &lt;- as_tibble(ISLR::Credit) credit ## # A tibble: 400 × 12 ## ID Income Limit Rating Cards Age Education Gender Student ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 14.9 3606 283 2 34 11 &quot; Male&quot; No ## 2 2 106. 6645 483 3 82 15 &quot;Female&quot; Yes ## 3 3 105. 7075 514 4 71 11 &quot; Male&quot; No ## 4 4 149. 9504 681 3 36 11 &quot;Female&quot; No ## 5 5 55.9 4897 357 2 68 16 &quot; Male&quot; No ## 6 6 80.2 8047 569 4 77 10 &quot; Male&quot; No ## 7 7 21.0 3388 259 2 37 12 &quot;Female&quot; No ## 8 8 71.4 7114 512 2 87 9 &quot; Male&quot; No ## 9 9 15.1 3300 266 5 66 13 &quot;Female&quot; No ## 10 10 71.1 6819 491 3 41 19 &quot;Female&quot; Yes ## # … with 390 more rows, and 3 more variables: Married &lt;fct&gt;, ## # Ethnicity &lt;fct&gt;, Balance &lt;int&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. For example, based on the gender, we can create a new variable that takes the form \\[ x_i = \\Bigg\\{ \\genfrac{}{}{0pt}{}{1 \\hspace{.5cm}\\text{ if }i\\text{th person is female}\\hspace{.25cm}}{0 \\hspace{.5cm}\\text{ if }i\\text{th person is male}} \\] and use this variable as a predictor in the regression equation. This results in the model \\[ y_i = b_0 + b_1x_i = \\Bigg\\{ \\genfrac{}{}{0pt}{}{b_0 + b_1 \\hspace{.5cm}\\text{ if }i\\text{th person is female}\\hspace{.3cm}}{b_0 \\hspace{1.5cm}\\text{ if }i\\text{th person is male}} \\] Now \\(b_0\\) can be interpreted as the average credit card balance among males, \\(b_0 + b_1\\) as the average credit card balance among females, and \\(b_1\\) as the average difference in credit card balance between females and males. We can produce this model in R using the same syntax as we saw earlier: qual_model &lt;- linear_reg() %&gt;% fit(Balance ~ Gender, data = credit) tidy(qual_model) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 510. 33.1 15.4 2.91e-42 ## 2 GenderFemale 19.7 46.1 0.429 6.69e- 1 The results above suggest that males are estimated to carry $509.80 in credit card debt where females carry $509.80 + $19.73 = $529.53. The decision to code males as 0 and females as 1 is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we want to change the reference variable (the variable coded as 0) we can change the factor levels. credit$Gender &lt;- factor(credit$Gender, levels = c(&quot;Female&quot;, &quot; Male&quot;)) qual_model &lt;- linear_reg() %&gt;% fit(Balance ~ Gender, data = credit) tidy(qual_model) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 530. 32.0 16.6 3.31e-47 ## 2 Gender Male -19.7 46.1 -0.429 6.69e- 1 A similar process ensues for qualitative predictor categories with more than two levels. For instance, if we go back to our Ames housing data we’ll see that there is a Neighborhood variable. In our data there are 28 different neighborhoods. ames_train %&gt;% count(Neighborhood) ## # A tibble: 28 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 North_Ames 306 ## 2 College_Creek 199 ## 3 Old_Town 164 ## 4 Edwards 131 ## 5 Somerset 122 ## 6 Northridge_Heights 116 ## 7 Gilbert 111 ## 8 Sawyer 108 ## 9 Northwest_Ames 82 ## 10 Sawyer_West 94 ## # … with 18 more rows ## # ℹ Use `print(n = ...)` to see more rows Most people are aware that different neighborhoods can generate significantly different home prices than other neighborhoods. In this data we can visualize this by looking at the distribution of Sale_Price across neighborhoods. We see that the Stone Brook neighborhood has the highest average sale price whereas Meadow Village has the lowest. This could be for many reasons (i.e. age of the neighborhood, amenities provided by the neighborhood, proximity to undesirable things such as manufacturing plants). ggplot(ames_train, aes(fct_reorder(Neighborhood, Sale_Price), Sale_Price)) + geom_boxplot() + xlab(NULL) + scale_y_continuous(&quot;Sale Price&quot;, labels = scales::dollar) + coord_flip() So, naturally, we can assume there is some relationship between Neighborhood and Sale_Price. We can assess this relationship by running the following model. Based on the results we see that the reference Neighborhood (which is North Ames based on levels(ames_train$Neighborhood)) has an average Sale_Price of $143,516.75 (based on the intercept). Whereas College Creek has a Sale_Price of $143,516.75 + $57,006.75 = $200,523.50. The p.value for College Creek is very small suggesting that this difference between North Ames and College Creek is statistically significant. However, look at the results for the Sawyer neighborhood. The coefficient suggests that the average Sale_Price for the Sawyer neighborhood is $143,516.75 - $4,591.68 = $148,108.40. However, the p.value is 0.43 which suggests that there is no statistical difference between the reference neighborhood (North Ames) and Sawyer. neighborhood_model &lt;- linear_reg() %&gt;% fit(Sale_Price ~ Neighborhood, data = ames_train) tidy(neighborhood_model) ## # A tibble: 28 × 5 ## term estimate std.error stati…¹ p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 143517. 2996. 47.9 0 ## 2 NeighborhoodCollege_Creek 57007. 4773. 11.9 8.04e- 32 ## 3 NeighborhoodOld_Town -19609. 5072. -3.87 1.14e- 4 ## 4 NeighborhoodEdwards -10196. 5472. -1.86 6.26e- 2 ## 5 NeighborhoodSomerset 87794. 5612. 15.6 3.68e- 52 ## 6 NeighborhoodNorthridge_Heights 177986. 5715. 31.1 2.85e-174 ## 7 NeighborhoodGilbert 44653. 5807. 7.69 2.30e- 14 ## 8 NeighborhoodSawyer -4592. 5866. -0.783 4.34e- 1 ## 9 NeighborhoodNorthwest_Ames 44635. 6518. 6.85 9.87e- 12 ## 10 NeighborhoodSawyer_West 40081. 6181. 6.48 1.11e- 10 ## # … with 18 more rows, and abbreviated variable name ¹​statistic ## # ℹ Use `print(n = ...)` to see more rows 6.5.1 Knowledge check The Ames housing data has an Overall_Qual variable that measures the overall quality of a home (Very Poor, Poor, …, Excellent, Very Excellent). Plot the relationship between Sale_Price and the Overall_Qual variable. Does there look to be a relationship between the quality of a home and its sale price? Model this relationship with a simple linear regression model. Interpret the coefficients. 6.6 Including many predictors In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with p distinct predictors is \\[\\begin{equation} \\widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_p x_p, \\end{equation}\\] where \\(x_i\\) for \\(i = 1, 2, \\dots, p\\) are the predictors of interest. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. The code below creates a model where we use all features in our data set as main effects (i.e., no interaction terms) to predict Sale_Price. However, note that we remove a few variables first. This is because these variables introduce some new problems that require us to do some feature engineering steps. We’ll discuss this in a future lesson but for now we’ll just put these feature variables to the side. # remove some trouble variables trbl_vars &lt;- c(&quot;MS_SubClass&quot;, &quot;Condition_2&quot;, &quot;Exterior_1st&quot;, &quot;Exterior_2nd&quot;, &quot;Misc_Feature&quot;) ames_train &lt;- ames_train %&gt;% select(-trbl_vars) # include all possible main effects model3 &lt;- linear_reg() %&gt;% fit(Sale_Price ~ ., data = ames_train) # print estimated coefficients in a tidy data frame tidy(model3) ## # A tibble: 249 × 5 ## term estimate std.e…¹ stati…² p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.76e+7 1.22e+7 -1.44 1.49e-1 ## 2 MS_ZoningResidential_High_Density 6.16e+3 8.84e+3 0.697 4.86e-1 ## 3 MS_ZoningResidential_Low_Density 3.69e+2 5.64e+3 0.0655 9.48e-1 ## 4 MS_ZoningResidential_Medium_Densi… 9.51e+2 6.40e+3 0.149 8.82e-1 ## 5 MS_ZoningA_agr -5.26e+4 5.44e+4 -0.966 3.34e-1 ## 6 MS_ZoningC_all -1.55e+4 1.01e+4 -1.53 1.25e-1 ## 7 MS_ZoningI_all -1.98e+4 2.70e+4 -0.736 4.62e-1 ## 8 Lot_Frontage -1.67e+1 2.07e+1 -0.807 4.20e-1 ## 9 Lot_Area 5.89e-1 1.08e-1 5.47 5.24e-8 ## 10 StreetPave 1.52e+3 1.05e+4 0.145 8.85e-1 ## # … with 239 more rows, and abbreviated variable names ¹​std.error, ## # ²​statistic ## # ℹ Use `print(n = ...)` to see more rows You’ll notice that our model’s results includes the intercept plus 248 predictor variable coefficients. However, our ames_train data only includes 75 predictor variables after removing those 5 troublesome variables! What gives? ames_train %&gt;% select(-Sale_Price) %&gt;% dim() ## [1] 2049 75 The reason is that 41 of our predictor variables are qualitative and many of these include several levels. So the dummy encoding procedure discussed in the last section causes us to have many more coefficients than initial predictor variables. ames_train %&gt;% select_if(is.factor) %&gt;% colnames() ## [1] &quot;MS_Zoning&quot; &quot;Street&quot; &quot;Alley&quot; ## [4] &quot;Lot_Shape&quot; &quot;Land_Contour&quot; &quot;Utilities&quot; ## [7] &quot;Lot_Config&quot; &quot;Land_Slope&quot; &quot;Neighborhood&quot; ## [10] &quot;Condition_1&quot; &quot;Bldg_Type&quot; &quot;House_Style&quot; ## [13] &quot;Overall_Qual&quot; &quot;Overall_Cond&quot; &quot;Roof_Style&quot; ## [16] &quot;Roof_Matl&quot; &quot;Mas_Vnr_Type&quot; &quot;Exter_Qual&quot; ## [19] &quot;Exter_Cond&quot; &quot;Foundation&quot; &quot;Bsmt_Qual&quot; ## [22] &quot;Bsmt_Cond&quot; &quot;Bsmt_Exposure&quot; &quot;BsmtFin_Type_1&quot; ## [25] &quot;BsmtFin_Type_2&quot; &quot;Heating&quot; &quot;Heating_QC&quot; ## [28] &quot;Central_Air&quot; &quot;Electrical&quot; &quot;Kitchen_Qual&quot; ## [31] &quot;Functional&quot; &quot;Fireplace_Qu&quot; &quot;Garage_Type&quot; ## [34] &quot;Garage_Finish&quot; &quot;Garage_Qual&quot; &quot;Garage_Cond&quot; ## [37] &quot;Paved_Drive&quot; &quot;Pool_QC&quot; &quot;Fence&quot; ## [40] &quot;Sale_Type&quot; &quot;Sale_Condition&quot; If we wanted to assess which features have a relationship we could easily filter our model results to find which coefficients have p.values less than 0.05. In this model we see that 67 (68 minus the intercept) features have a statistical relationship with Sale_Price. tidy(model3) %&gt;% filter(p.value &lt; 0.05) ## # A tibble: 68 × 5 ## term estimate std.error stati…¹ p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lot_Area 0.589 0.108 5.47 5.24e-8 ## 2 Lot_ShapeModerately_Irregular 10299. 3759. 2.74 6.21e-3 ## 3 Land_ContourHLS 16214. 4288. 3.78 1.61e-4 ## 4 Land_ContourLvl 10865. 3040. 3.57 3.61e-4 ## 5 Lot_ConfigCulDSac 6078. 2930. 2.07 3.82e-2 ## 6 Lot_ConfigFR2 -7998. 3563. -2.25 2.49e-2 ## 7 Land_SlopeSev -34202. 11287. -3.03 2.48e-3 ## 8 NeighborhoodCollege_Creek 26578. 10478. 2.54 1.13e-2 ## 9 NeighborhoodSomerset 23072. 6180. 3.73 1.95e-4 ## 10 NeighborhoodNorthridge_Heights 31949. 6159. 5.19 2.37e-7 ## # … with 58 more rows, and abbreviated variable name ¹​statistic ## # ℹ Use `print(n = ...)` to see more rows How does our model with all available predictors perform? We can compute the generalization error to assess. model3 %&gt;% predict(ames_test) %&gt;% bind_cols(ames_test) %&gt;% rmse(truth = Sale_Price, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 22959. Not too shabby! Using all the predictor variables in our model has drastically reduced our test RMSE! 6.7 Feature importance Ok, so we found a linear regression model that performs pretty good compared to the other linear regression models we trained. Our next goal is often to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the lesson, this constant rate of change is provided by the coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables? Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the t-statistic for each model parameter used. Rather than search through each of the variables to compare their t-statistic values, we can use vip::vip() to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). The plot below illustrates the top 20 most influential variables. We see that the top 4 most important variables have to do with roofing material followed by the total square footage on the second floor. # plot top 10 influential features model3 %&gt;% vip::vip(num_features = 20) This is basically saying that our model finds that these are the most influential features in our data set that have the largest impact on the predicted outcome. If we order our data based on the t-statistic we see similar results. tidy(model3) %&gt;% arrange(desc(statistic)) ## # A tibble: 249 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Roof_MatlWdShngl 663237. 43673. 15.2 4.08e-49 ## 2 Roof_MatlCompShg 591744. 41975. 14.1 6.74e-43 ## 3 Roof_MatlWdShake 580841. 43901. 13.2 3.28e-38 ## 4 Roof_MatlTar&amp;Grv 585872. 44342. 13.2 4.08e-38 ## 5 Second_Flr_SF 60.4 4.80 12.6 6.62e-35 ## 6 Roof_MatlRoll 603645. 48760. 12.4 7.56e-34 ## 7 Roof_MatlMembran 642865. 52863. 12.2 9.20e-33 ## 8 Roof_MatlMetal 636083. 52812. 12.0 3.42e-32 ## 9 First_Flr_SF 43.0 4.16 10.3 2.63e-24 ## 10 NeighborhoodGreen_Hills 159547. 19970. 7.99 2.39e-15 ## # … with 239 more rows ## # ℹ Use `print(n = ...)` to see more rows 6.8 Exercises Using the Boston housing data set where the response feature is the median value of homes within a census tract (cmedv): Split the data into 70-30 training-test sets. Train an MLR model that includes all the predictor variables. Assess and interpret the coefficients. Are all predictor variables statistically significant? Explain why or why not. What is the generalization error of this model? Which features are most influential in this model and which features are not? "],["computing-environment.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ───────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.4 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-10-07 ## rstudio 2022.07.1+554 Spotted Wakerobin (desktop) ## pandoc 2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown) ## ## ─ Packages ───────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## broom * 1.0.0 2022-07-01 [1] CRAN (R 4.2.0) ## DiagrammeR * 1.0.9 2022-03-05 [1] CRAN (R 4.2.0) ## dials * 1.0.0 2022-06-14 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## infer * 1.0.2 2022-06-10 [1] CRAN (R 4.2.0) ## kableExtra * 1.3.4 2021-02-20 [1] CRAN (R 4.2.0) ## modeldata * 1.0.0 2022-07-01 [1] CRAN (R 4.2.0) ## parsnip * 1.0.0 2022-06-16 [1] CRAN (R 4.2.0) ## plotly * 4.10.0 2021-10-09 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.2.0) ## recipes * 0.2.0 2022-02-18 [1] CRAN (R 4.2.0) ## reshape2 * 1.4.4 2020-04-09 [1] CRAN (R 4.2.0) ## rsample * 0.1.1 2021-11-08 [1] CRAN (R 4.2.0) ## scales * 1.2.0 2022-04-13 [1] CRAN (R 4.2.0) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.2.0) ## tibble * 3.1.8 2022-07-22 [1] CRAN (R 4.2.0) ## tidymodels * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tidyverse * 1.3.2 2022-07-18 [1] CRAN (R 4.2.0) ## tune * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## workflows * 0.2.6 2022-03-18 [1] CRAN (R 4.2.0) ## workflowsets * 0.2.1 2022-03-15 [1] CRAN (R 4.2.0) ## yardstick * 1.0.0 2022-06-06 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ─ Python configuration ───────────────────────────────────────────── ## python: /Users/b294776/Library/r-miniconda/envs/r-reticulate/bin/python ## libpython: /Users/b294776/Library/r-miniconda/envs/r-reticulate/lib/libpython3.8.dylib ## pythonhome: /Users/b294776/Library/r-miniconda/envs/r-reticulate:/Users/b294776/Library/r-miniconda/envs/r-reticulate ## version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:47) [Clang 12.0.1 ] ## numpy: /Users/b294776/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/numpy ## numpy_version: 1.22.4 ## ## ──────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
