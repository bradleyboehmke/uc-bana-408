[["index.html", "Data Mining with R Syllabus Learning Objectives Material Class Structure Schedule Conventions used in this book Feedback Acknowledgements", " Data Mining with R Bradley Boehmke Syllabus This is the primary “textbook” for the Machine Learning section of the UC BANA 4080 Data Mining course. The following is a truncated syllabus; for the full syllabus along with complete course content please visit the online course content in Canvas. Welcome to Data Mining with R! This course provides an intensive, hands-on introduction to data mining and analysis techniques. You will learn the fundamental skills required to extract informative attributes, relationships, and patterns from data sets. You will gain hands-on experience with exploratory data analysis, data visualization, unsupervised learning techniques such as clustering and dimension reduction, and supervised learning techniques such as linear regression, regularized regression, decision trees, random forests, and more! You will also be exposed to some more advanced topics such as ensembling techniques, deep learning, model stacking, and model interpretation. Together, this will provide you with a solid foundation of tools and techniques applied in organizations to aid modern day data-driven decision making. Check out the video in the “Overview of Course” module on Canvas for a quick introduction to this course. Learning Objectives Upon successfully completing this course, you will be able to: Apply data wrangling techniques to manipulate and prepare data for analysis. Use exploratory data analysis and visualization to provide descriptive insights of data. Apply common unsupervised learning algorithms to find common groupings of observations and features in a given dataset. Describe and apply a sound analytic modeling process. Apply, compare, and contrast various predictive modeling techniques. Have the resources and understanding to continue advancing your data mining and analysis capabilities. …all with R! This course assumes no prior knowledge of R. Experience with programming concepts or another programming language will help, but is not required to understand the material. Material This course is split into two main sections - Data Wrangling and Machine Learning. The data wrangling section will provide you the fundamental skills required to acquire, munge, transform, manipulate, and visualize data in a computing environment that fosters reproducibility. The primary course material for this section is provided via this free online book. The second section focused on machine learning section will expose you to several algorithms to identify hidden patterns and relationships within your data. The primary course material for this part of the course is provided via this free online book. There will also be recorded lectures and additional supplementary resources provided via Canvas. Class Structure Modules: For this class each module is covered over the course of week. In the “Overview” section for each module you will find overall learning objectives, a short description of the learning content covered in that module, along with all tasks that are required of you for that module (i.e. quizzes, lab). Each module will have two or more primary lessons and associated quizzes along with a lab. Lessons: For each lesson you will read and work through the tutorial. Short videos will be sprinkled throughout the lesson to further discuss and reinforce lesson concepts. Each lesson will have various “TODO” exercises throughout, along with end-of-lesson exercises. I highly recommend you work through these exercises as they will prepare you for the quizzes, labs, and project work. Quizzes: There will be a short quiz associated with each lesson. These quizzes will be hosted in the course website on Canvas. Please check Canvas for due dates for these quizzes. Labs: There will be a lab associated with each module. For these labs students will be guided through a case study step-by-step. The aim is to provide a detailed view on how to manage a variety of complex real-world data; how to convert real problems into data wrangling and analysis problems; and to apply R to address these problems and extract insights from the data. These labs will be provided via the course website on Canvas and the submission of these labs will also be done through the course website on Canvas. Please check Canvas for due dates for these labs. Projects: There will be two projects designed for you to put to work the tools and knowledge that you gain throughout this course. This provides you with multiple benefits. - It will provide you with more experience using data wrangling tools on real life data sets. - It helps you become a self-directed learner. As a data scientist, a large part of your job is to self-direct your learning and interests to find unique and creative ways to find insights in data. - It starts to build your data science portfolio. Establishing a data science portfolio is a great way to show potential employers your ability to work with data. Schedule See the Canvas course webpage for a detailed schedule with due dates for quizzes, labs, etc. Module Description DATA WRANGLING 1 Introduction R fundamentals &amp; the Rstudio IDE Deeper understanding of vectors 2 Reproducible Documents and Importing Data Managing your workflow and reproducibility Data structures &amp; importing data 3 Tidy Data and Data Manipulation Data manipulation &amp; summarization Tidy data 4 Relational Data and More Tidyverse Packages Relational data Leveraging the Tidyverse to text &amp; date-time data 5 Data Visualization &amp; Exploration Data visualization Exploratory data analysis 6 Creating Efficient Code in R Control statements &amp; iteration Writing functions MACHINE LEARNING 7 Introduction to Applied Modeling Introduction to tidymodels Feature engineering &amp; model evaluation/selection 8 First regression models Ordinary least squares (OLS) OLS cousins 9 First classification models Logistic regression Assessing classification models 10 More regression cousins Regularized regression Multi-adaptive Regression Splines (MARS) 11 Venturing away from linearity K-Nearest neighbor Decision trees 12 Ensembling trees Bagging Random forests 13 Ensembling trees continued Gradient boosting XGBoost and other variants 14 Deep learning Feedforward neural nets A survey of deep learning extensions 15 Unsupervised Learning Clustering. Dimension reductions Conventions used in this book The following typographical conventions are used in this book: strong italic: indicates new terms, bold: indicates package &amp; file names, inline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user, code chunk: indicates commands or other text that could be typed literally by the user 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following cells that provide additional context for improved learning: A video demonstrating this topic is available in Canvas. A tip or suggestion that will likely produce better results. A general note that could improve your understanding but is not required for the course requirements. Warning or caution to look out for. Knowledge check exercises to gauge your learning progress. Feedback To report errors or bugs that you find in this course material please post an issue at https://github.com/bradleyboehmke/uc-bana-4080/issues. For all other communication be sure to use Canvas or the university email. When communicating with me via email, please always include BANA4080 in the subject line. Acknowledgements This course and its materials have been influenced by the following resources: Jenny Bryan, STAT 545: Data wrangling, exploration, and analysis with R Garrett Grolemund &amp; Hadley Wickham, R for Data Science Stephanie Hicks, Statistical Computing Chester Ismay &amp; Albert Kim, ModernDive Alex Douglas et al., An Introduction to R Brandon Greenwell, Hands-on Machine Learning with R "],["introduction-to-machine-learning.html", "1 Introduction to Machine Learning 1.1 Learning objectives 1.2 Supervised learning 1.3 Unsupervised learning 1.4 Machine Learning in 1.5 The data sets 1.6 What You’ll Learn Next 1.7 Exercises", " 1 Introduction to Machine Learning Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include: Predicting the likelihood of a patient returning to the hospital (readmission) within 30 days of discharge. Segmenting customers based on common attributes or purchasing behavior for targeted marketing. Predicting coupon redemption rates for a given marketing campaign. Predicting customer churn so an organization can perform preventative intervention. And many more! In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of features to train an algorithm and extract insights. These algorithms, or learners, can be classified according to the amount and type of supervision needed during training. 1.1 Learning objectives This lesson will introduce you to some fundamental concepts around ML and this class. By the end of this lesson you will: Be able to explain the difference between supervised and unsupervised learning. Know when a problem is considered a regression or classification problem. Be able to import and explore the data sets we’ll use through various examples. 1.2 Supervised learning A predictive model is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. The learning algorithm in a predictive model attempts to discover and model the relationships among the target variable (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks; using home attributes to predict the sales price; using employee attributes to predict the likelihood of attrition; using patient attributes and symptoms to predict the risk of readmission; using production attributes to predict time to market. Each of these examples has a defined learning task; they each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this course we’ll use various terms interchangeably for \\(X\\): “predictor variable”, “independent variable”, “attribute”, “feature”, “predictor” \\(Y\\): “target variable”, “dependent variable”, “response”, “outcome measurement” The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the target values. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Most supervised learning problems can be bucketed into one of two categories, regression or classification, which we discuss next. 1.2.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between $80,000 and $755,000). The figure below illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane. Figure 1.1: Fig 1: Average home sales price as a function of year built and total square footage. 1.2.2 Classification problems When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: Did a customer redeem a coupon (coded as yes/no or 1/0)? Did a customer churn (coded as yes/no or 1/0)? Did a customer click on our online ad (coded as yes/no or 1/0)? Classifying customer reviews: Binary: positive vs. negative. Multinomial: extremely negative to extremely positive on a 0–5 Likert scale. Figure 1.2: Fig 2: Classification problem modeling ‘Yes’/‘No’ response based on three features. However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., “yes” or “no”), we often want to predict the probability of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, many of the supervised learning algorithms we cover in this class can be applied to both. These algorithms have become the most popular machine learning applications in recent years. 1.3 Unsupervised learning Unsupervised learning, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression). Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised! Despite its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to: Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. Identify products that have similar purchasing behavior so that managers can manage them as product groups. These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of unsupervised learning models can be used as inputs to downstream supervised learning models. 1.4 Machine Learning in Historically, the R ecosystem provides a wide variety of ML algorithm implementations. This has its benefits; however, this also has drawbacks as it requires the users to learn many different formula interfaces and syntax nuances. More recently, development on a group of packages called Tidymodels has helped to make implementation easier. The tidymodels collection allows you to perform discrete parts of the ML workflow with discrete packages: rsample for data splitting and resampling recipes for data pre-processing and feature engineering parsnip for applying algorithms tune for hyperparameter tuning yardstick for measuring model performance and several others! Throughout this course you’ll be exposed to several of these packages. Go ahead and make sure you have the following packages installed. Just like the tidyverse package, when you install tidymodels you are actually installing several packages that exist in the tidymodels ecosystem as discussed above. # common data wrangling and visualization install.packages(&quot;tidyverse&quot;) install.packages(&quot;vip&quot;) install.packages(&quot;here&quot;) # modeling install.packages(&quot;tidymodels&quot;) packageVersion(&quot;tidymodels&quot;) ## [1] &#39;0.2.0&#39; library(tidymodels) 1.5 The data sets The data sets chosen for this course allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this course is to demonstrate how to implement ML workflows, we make the assumption that you have already spent significant time wrangling, cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this course such as: Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process). Recoding variable names and values so that they are meaningful and more interpretable. Tidying data so that each column is a discrete variable and each row is an individual observation. Recoding, removing, or some other approach to handling missing values. Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. As mentioned above, these data sets are fairly common data sets that provide good benchmarks to compare and illustrate ML workflows. Although some of these data sets are available in R, we will import these data sets from a .csv file to ensure consistency over time. 1.5.1 Boston housing The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in Harrison Jr and Rubinfeld (1978) , it contains 13 attributes to predict the median property value. problem type: supervised regression response variable: medv median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5) features: 13 observations: 506 objective: use property attributes to predict the median value of owner-occupied homes # data file path library(here) data_path &lt;- here(&quot;data&quot;) # access data boston &lt;- readr::read_csv(here(data_path, &quot;boston.csv&quot;)) # initial dimension dim(boston) ## [1] 506 16 # features dplyr::select(boston, -cmedv) ## # A tibble: 506 × 15 ## lon lat crim zn indus chas nox rm age dis rad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.0 42.3 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 ## 2 -71.0 42.3 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 ## 3 -70.9 42.3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 ## 4 -70.9 42.3 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 ## 5 -70.9 42.3 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 ## 6 -70.9 42.3 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 ## 7 -70.9 42.3 0.0883 12.5 7.87 0 0.524 6.01 66.6 5.56 5 ## 8 -70.9 42.3 0.145 12.5 7.87 0 0.524 6.17 96.1 5.95 5 ## 9 -70.9 42.3 0.211 12.5 7.87 0 0.524 5.63 100 6.08 5 ## 10 -70.9 42.3 0.170 12.5 7.87 0 0.524 6.00 85.9 6.59 5 ## # … with 496 more rows, and 4 more variables: tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, ## # b &lt;dbl&gt;, lstat &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(boston$cmedv) ## [1] 24.0 21.6 34.7 33.4 36.2 28.7 1.5.2 Pima Indians Diabetes A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and published in smith1988using , it contains 8 attributes to predict the presence of diabetes. problem type: supervised binary classification response variable: diabetes positive or negative response (i.e. “pos”, “neg”) features: 8 observations: 768 objective: use biological attributes to predict the presence of diabetes # access data pima &lt;- readr::read_csv(here(data_path, &quot;pima.csv&quot;)) # initial dimension dim(pima) ## [1] 768 9 # features dplyr::select(pima, -diabetes) ## # A tibble: 768 × 8 ## pregnant glucose pressure triceps insulin mass pedigree age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 148 72 35 0 33.6 0.627 50 ## 2 1 85 66 29 0 26.6 0.351 31 ## 3 8 183 64 0 0 23.3 0.672 32 ## 4 1 89 66 23 94 28.1 0.167 21 ## 5 0 137 40 35 168 43.1 2.29 33 ## 6 5 116 74 0 0 25.6 0.201 30 ## 7 3 78 50 32 88 31 0.248 26 ## 8 10 115 0 0 0 35.3 0.134 29 ## 9 2 197 70 45 543 30.5 0.158 53 ## 10 8 125 96 0 0 0 0.232 54 ## # … with 758 more rows ## # ℹ Use `print(n = ...)` to see more rows # response variable head(pima$diabetes) ## [1] &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; 1.5.3 Iris flowers The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper (Fisher 1936) . It is sometimes called Anderson’s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. problem type: supervised multinomial classification response variable: species (i.e. “setosa”, “virginica”, “versicolor”) features: 4 observations: 150 objective: use plant leaf attributes to predict the type of flower # access data iris &lt;- readr::read_csv(here(data_path, &quot;iris.csv&quot;)) # initial dimension dim(iris) ## [1] 150 5 # features dplyr::select(iris, -Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows ## # ℹ Use `print(n = ...)` to see more rows # response variable head(iris$Species) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; 1.5.4 Ames housing The Ames housing data set is an alternative to the Boston housing data set and provides a more comprehensive set of home features to predict sales price. More information can be found in De Cock (2011) . problem type: supervised regression response variable: Sale_Price (i.e., $195,000, $215,000) features: 80 observations: 2,930 objective: use property attributes to predict the sale price of a home # access data ames &lt;- readr::read_csv(here(data_path, &quot;ames.csv&quot;)) # initial dimension dim(ames) ## [1] 2930 81 # features dplyr::select(ames, -Sale_Price) ## # A tibble: 2,930 × 80 ## MS_SubCl…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub ## 2 One_Story… Reside… 80 11622 Pave No_A… Regular Lvl AllPub ## 3 One_Story… Reside… 81 14267 Pave No_A… Slight… Lvl AllPub ## 4 One_Story… Reside… 93 11160 Pave No_A… Regular Lvl AllPub ## 5 Two_Story… Reside… 74 13830 Pave No_A… Slight… Lvl AllPub ## 6 Two_Story… Reside… 78 9978 Pave No_A… Slight… Lvl AllPub ## 7 One_Story… Reside… 41 4920 Pave No_A… Regular Lvl AllPub ## 8 One_Story… Reside… 43 5005 Pave No_A… Slight… HLS AllPub ## 9 One_Story… Reside… 39 5389 Pave No_A… Slight… Lvl AllPub ## 10 Two_Story… Reside… 60 7500 Pave No_A… Regular Lvl AllPub ## # … with 2,920 more rows, 71 more variables: Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, ## # Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Qual &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;chr&gt;, Exter_Cond &lt;chr&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(ames$Sale_Price) ## [1] 215000 105000 172000 244000 189900 195500 1.5.5 Attrition The employee attrition data set was originally provided by IBM Watson Analytics Lab and is a fictional data set created by IBM data scientists to explore what employee attributes influence attrition. problem type: supervised binomial classification response variable: Attrition (i.e., “Yes”, “No”) features: 30 observations: 1,470 objective: use employee attributes to predict if they will attrit (leave the company) # access data attrition &lt;- readr::read_csv(here(data_path, &quot;attrition.csv&quot;)) # initial dimension dim(attrition) ## [1] 1470 31 # features dplyr::select(attrition, -Attrition) ## # A tibble: 1,470 × 30 ## Age Business…¹ Daily…² Depar…³ Dista…⁴ Educa…⁵ Educa…⁶ Envir…⁷ Gender ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 41 Travel_Ra… 1102 Sales 1 College Life_S… Medium Female ## 2 49 Travel_Fr… 279 Resear… 8 Below_… Life_S… High Male ## 3 37 Travel_Ra… 1373 Resear… 2 College Other Very_H… Male ## 4 33 Travel_Fr… 1392 Resear… 3 Master Life_S… Very_H… Female ## 5 27 Travel_Ra… 591 Resear… 2 Below_… Medical Low Male ## 6 32 Travel_Fr… 1005 Resear… 2 College Life_S… Very_H… Male ## 7 59 Travel_Ra… 1324 Resear… 3 Bachel… Medical High Female ## 8 30 Travel_Ra… 1358 Resear… 24 Below_… Life_S… Very_H… Male ## 9 38 Travel_Fr… 216 Resear… 23 Bachel… Life_S… Very_H… Male ## 10 36 Travel_Ra… 1299 Resear… 27 Bachel… Medical High Male ## # … with 1,460 more rows, 21 more variables: HourlyRate &lt;dbl&gt;, ## # JobInvolvement &lt;chr&gt;, JobLevel &lt;dbl&gt;, JobRole &lt;chr&gt;, ## # JobSatisfaction &lt;chr&gt;, MaritalStatus &lt;chr&gt;, MonthlyIncome &lt;dbl&gt;, ## # MonthlyRate &lt;dbl&gt;, NumCompaniesWorked &lt;dbl&gt;, OverTime &lt;chr&gt;, ## # PercentSalaryHike &lt;dbl&gt;, PerformanceRating &lt;chr&gt;, ## # RelationshipSatisfaction &lt;chr&gt;, StockOptionLevel &lt;dbl&gt;, ## # TotalWorkingYears &lt;dbl&gt;, TrainingTimesLastYear &lt;dbl&gt;, … ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(attrition$Attrition) ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 1.5.6 Hitters This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. The idea was to illustrate if and how major league baseball player’s batting performance could predict their salary. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. Note that the data does contain the players name but this should be removed during analysis and is not a valid feature. problem type: supervised regression response variable: Salary features: 19 observations: 322 objective: use baseball player’s batting attributes to predict their salary. # access data hitters &lt;- readr::read_csv(here(data_path, &quot;hitters.csv&quot;)) # initial dimension dim(hitters) ## [1] 322 21 # features dplyr::select(hitters, -Salary, -Player) ## # A tibble: 322 × 19 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 293 66 1 30 29 14 1 293 66 1 30 ## 2 315 81 7 24 38 39 14 3449 835 69 321 ## 3 479 130 18 66 72 76 3 1624 457 63 224 ## 4 496 141 20 65 78 37 11 5628 1575 225 828 ## 5 321 87 10 39 42 30 2 396 101 12 48 ## 6 594 169 4 74 51 35 11 4408 1133 19 501 ## 7 185 37 1 23 8 21 2 214 42 1 30 ## 8 298 73 0 24 24 7 3 509 108 0 41 ## 9 323 81 6 26 32 8 2 341 86 6 32 ## 10 401 92 17 49 66 65 13 5206 1332 253 784 ## # … with 312 more rows, and 8 more variables: CRBI &lt;dbl&gt;, CWalks &lt;dbl&gt;, ## # League &lt;chr&gt;, Division &lt;chr&gt;, PutOuts &lt;dbl&gt;, Assists &lt;dbl&gt;, ## # Errors &lt;dbl&gt;, NewLeague &lt;chr&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names # response variable head(hitters$Salary) ## [1] NA 475.0 480.0 500.0 91.5 750.0 1.6 What You’ll Learn Next The lessons that follow are designed to help you understand the individual sub-tasks of an ML project. The focus is to have an intuitive understanding of each discrete sub-task and algorithm. Once you understand when, where, and why these sub-tasks are performed you will be able to transfer this knowledge to other projects. The concepts you will learn include: Provide an overview of the ML modeling process: data splitting model fitting model validation and tuning performance measurement feature engineering Cover common supervised learners: linear regression regularized regression K-nearest neighbors decision trees bagging &amp; random forests gradient boosting Cover common unsupervised learners: K-means clustering Principal component analysis Along the way you’ll learn about: each algorithm’s hyperparameters model interpretation feature importance and more! 1.7 Exercises Identify four real-life applications of supervised and unsupervised problems. Explain what makes these problems supervised versus unsupervised. For each problem identify the target variable (if applicable) and potential features. Identify and contrast a regression problem with a classification problem. What is the target variable in each problem and why would being able to accurately predict this target be beneficial to society? What are potential features and where could you collect this information? What is determining if the problem is a regression or a classification problem? Identify three open source data sets suitable for machine learning (e.g., https://bit.ly/35wKu5c). Explain the type of machine learning models that could be constructed from the data (e.g., supervised versus unsupervised and regression versus classification). What are the dimensions of the data? Is there a code book that explains who collected the data, why it was originally collected, and what each variable represents? If the data set is suitable for supervised learning, which variable(s) could be considered as a useful target? Which variable(s) could be considered as features? Identify examples of misuse of machine learning in society. What was the ethical concern? References "],["computing-environment.html", "Computing Environment", " Computing Environment This book was built with the following computing environment and packages: sessioninfo::session_info(pkgs = &#39;attached&#39;) ## ─ Session info ───────────────────────────────────────────────────────── ## setting value ## version R version 4.2.0 (2022-04-22) ## os macOS Monterey 12.4 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-08-11 ## rstudio 2022.07.1+554 Spotted Wakerobin (desktop) ## pandoc 2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown) ## ## ─ Packages ───────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## broom * 1.0.0 2022-07-01 [1] CRAN (R 4.2.0) ## DiagrammeR * 1.0.9 2022-03-05 [1] CRAN (R 4.2.0) ## dials * 1.0.0 2022-06-14 [1] CRAN (R 4.2.0) ## dplyr * 1.0.9 2022-04-28 [1] CRAN (R 4.2.0) ## ggplot2 * 3.3.6 2022-05-03 [1] CRAN (R 4.2.0) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.2.0) ## infer * 1.0.2 2022-06-10 [1] CRAN (R 4.2.0) ## modeldata * 0.1.1 2021-07-14 [1] CRAN (R 4.2.0) ## parsnip * 1.0.0 2022-06-16 [1] CRAN (R 4.2.0) ## plotly * 4.10.0 2021-10-09 [1] CRAN (R 4.2.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.2.0) ## recipes * 0.2.0 2022-02-18 [1] CRAN (R 4.2.0) ## rsample * 0.1.1 2021-11-08 [1] CRAN (R 4.2.0) ## scales * 1.2.0 2022-04-13 [1] CRAN (R 4.2.0) ## tibble * 3.1.8 2022-07-22 [1] CRAN (R 4.2.0) ## tidymodels * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.2.0) ## tune * 0.2.0 2022-03-19 [1] CRAN (R 4.2.0) ## workflows * 0.2.6 2022-03-18 [1] CRAN (R 4.2.0) ## workflowsets * 0.2.1 2022-03-15 [1] CRAN (R 4.2.0) ## yardstick * 1.0.0 2022-06-06 [1] CRAN (R 4.2.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## ## ──────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
