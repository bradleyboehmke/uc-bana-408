<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Lesson 2b: Multiple linear regression | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Lesson 2b: Multiple linear regression | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Lesson 2b: Multiple linear regression | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lesson-2a-simple-linear-regression.html"/>
<link rel="next" href="overview-2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 5</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>15</b> Lesson 5b: Multivariate Adaptive Regression Splines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#learning-objectives-15"><i class="fa fa-check"></i><b>15.1</b> Learning objectives</a></li>
<li class="chapter" data-level="15.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#prerequisites-8"><i class="fa fa-check"></i><b>15.2</b> Prerequisites</a></li>
<li class="chapter" data-level="15.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#nonlinearity"><i class="fa fa-check"></i><b>15.3</b> Nonlinearity</a></li>
<li class="chapter" data-level="15.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>15.4</b> Multivariate adaptive regression splines</a></li>
<li class="chapter" data-level="15.5" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-mars-model"><i class="fa fa-check"></i><b>15.5</b> Fitting a MARS model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-basic-model"><i class="fa fa-check"></i><b>15.5.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="15.5.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model"><i class="fa fa-check"></i><b>15.5.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="15.5.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model-with-interactions"><i class="fa fa-check"></i><b>15.5.3</b> Fitting a full model with interactions</a></li>
<li class="chapter" data-level="15.5.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-25"><i class="fa fa-check"></i><b>15.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#tuning-1"><i class="fa fa-check"></i><b>15.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-26"><i class="fa fa-check"></i><b>15.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#feature-interpretation-1"><i class="fa fa-check"></i><b>15.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="15.8" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#final-thoughts-2"><i class="fa fa-check"></i><b>15.8</b> Final thoughts</a></li>
<li class="chapter" data-level="15.9" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#exercises-9"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Module 6</b></span></li>
<li class="chapter" data-level="16" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i><b>16</b> Overview</a>
<ul>
<li class="chapter" data-level="16.1" data-path="overview-5.html"><a href="overview-5.html#learning-objectives-16"><i class="fa fa-check"></i><b>16.1</b> Learning objectives</a></li>
<li class="chapter" data-level="16.2" data-path="overview-5.html"><a href="overview-5.html#estimated-time-requirement-5"><i class="fa fa-check"></i><b>16.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="16.3" data-path="overview-5.html"><a href="overview-5.html#tasks-5"><i class="fa fa-check"></i><b>16.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html"><i class="fa fa-check"></i><b>17</b> Lesson 6a: Decision Trees</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#learning-objectives-17"><i class="fa fa-check"></i><b>17.1</b> Learning objectives</a></li>
<li class="chapter" data-level="17.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#prerequisites-9"><i class="fa fa-check"></i><b>17.2</b> Prerequisites</a></li>
<li class="chapter" data-level="17.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#structure"><i class="fa fa-check"></i><b>17.3</b> Structure</a></li>
<li class="chapter" data-level="17.4" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#partitioning"><i class="fa fa-check"></i><b>17.4</b> Partitioning</a></li>
<li class="chapter" data-level="17.5" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#how-deep"><i class="fa fa-check"></i><b>17.5</b> How deep?</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#early-stopping"><i class="fa fa-check"></i><b>17.5.1</b> Early stopping</a></li>
<li class="chapter" data-level="17.5.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#pruning"><i class="fa fa-check"></i><b>17.5.2</b> Pruning</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-decision-tree"><i class="fa fa-check"></i><b>17.6</b> Fitting a decision tree</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1"><i class="fa fa-check"></i><b>17.6.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="17.6.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-full-model-1"><i class="fa fa-check"></i><b>17.6.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="17.6.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-27"><i class="fa fa-check"></i><b>17.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#tuning-2"><i class="fa fa-check"></i><b>17.7</b> Tuning</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-28"><i class="fa fa-check"></i><b>17.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#feature-interpretation-2"><i class="fa fa-check"></i><b>17.8</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-29"><i class="fa fa-check"></i><b>17.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#final-thoughts-3"><i class="fa fa-check"></i><b>17.9</b> Final thoughts</a></li>
<li class="chapter" data-level="17.10" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>17.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html"><i class="fa fa-check"></i><b>18</b> Lesson 6b: Bagging</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#learning-objectives-18"><i class="fa fa-check"></i><b>18.1</b> Learning objectives</a></li>
<li class="chapter" data-level="18.2" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#prerequisites-10"><i class="fa fa-check"></i><b>18.2</b> Prerequisites</a></li>
<li class="chapter" data-level="18.3" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#why-and-when-bagging-works"><i class="fa fa-check"></i><b>18.3</b> Why and when bagging works</a></li>
<li class="chapter" data-level="18.4" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#fitting-a-bagged-decision-tree-model"><i class="fa fa-check"></i><b>18.4</b> Fitting a bagged decision tree model</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-30"><i class="fa fa-check"></i><b>18.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#tuning-3"><i class="fa fa-check"></i><b>18.5</b> Tuning</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-31"><i class="fa fa-check"></i><b>18.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#feature-interpretation-3"><i class="fa fa-check"></i><b>18.6</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-32"><i class="fa fa-check"></i><b>18.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#final-thoughts-4"><i class="fa fa-check"></i><b>18.7</b> Final thoughts</a></li>
<li class="chapter" data-level="18.8" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#exercises-11"><i class="fa fa-check"></i><b>18.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html"><i class="fa fa-check"></i><b>19</b> Lesson 6c: Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#learning-objectives-19"><i class="fa fa-check"></i><b>19.1</b> Learning objectives</a></li>
<li class="chapter" data-level="19.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#prerequisites-11"><i class="fa fa-check"></i><b>19.2</b> Prerequisites</a></li>
<li class="chapter" data-level="19.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#extending-bagging"><i class="fa fa-check"></i><b>19.3</b> Extending bagging</a></li>
<li class="chapter" data-level="19.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#out-of-the-box-performance"><i class="fa fa-check"></i><b>19.4</b> Out-of-the-box performance</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-33"><i class="fa fa-check"></i><b>19.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#hyperparameters"><i class="fa fa-check"></i><b>19.5</b> Hyperparameters</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#number-of-trees"><i class="fa fa-check"></i><b>19.5.1</b> Number of trees</a></li>
<li class="chapter" data-level="19.5.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#mtry"><i class="fa fa-check"></i><b>19.5.2</b> <span class="math inline">\(m_{try}\)</span></a></li>
<li class="chapter" data-level="19.5.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tree-complexity"><i class="fa fa-check"></i><b>19.5.3</b> Tree complexity</a></li>
<li class="chapter" data-level="19.5.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#others"><i class="fa fa-check"></i><b>19.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tuning-4"><i class="fa fa-check"></i><b>19.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="19.6.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-34"><i class="fa fa-check"></i><b>19.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#feature-interpretation-4"><i class="fa fa-check"></i><b>19.7</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="19.7.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-35"><i class="fa fa-check"></i><b>19.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#final-thoughts-5"><i class="fa fa-check"></i><b>19.8</b> Final thoughts</a></li>
<li class="chapter" data-level="19.9" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#exercises-12"><i class="fa fa-check"></i><b>19.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VII Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-2b-multiple-linear-regression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Lesson 2b: Multiple linear regression<a href="lesson-2b-multiple-linear-regression.html#lesson-2b-multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the last lesson we learned how to use one predictor variable to predict a numeric response. However, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (<code>Gr_Liv_Area</code>) and the year the house was built (<code>Year_Built</code>) are (linearly) related to sale price (<code>Sale_Price</code>). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the <em>multiple linear regression</em> (MLR) model and is the focus for this lesson.</p>
<div id="learning-objectives-6" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Learning objectives<a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this lesson you will know how to:</p>
<ul>
<li>Fit, interpret, and assess the performance of a multiple linear regression model.</li>
<li>Include categorical features in a linear regression model and interpret their results.</li>
<li>Asses the most influential predictor variables in a linear regression model.</li>
</ul>
</div>
<div id="prerequisites-2" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Prerequisites<a href="lesson-2b-multiple-linear-regression.html#prerequisites-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This lesson leverages the following packages:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="lesson-2b-multiple-linear-regression.html#cb45-1" tabindex="-1"></a><span class="co"># Data wrangling &amp; visualization packages</span></span>
<span id="cb45-2"><a href="lesson-2b-multiple-linear-regression.html#cb45-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb45-3"><a href="lesson-2b-multiple-linear-regression.html#cb45-3" tabindex="-1"></a></span>
<span id="cb45-4"><a href="lesson-2b-multiple-linear-regression.html#cb45-4" tabindex="-1"></a><span class="co"># Modeling packages</span></span>
<span id="cb45-5"><a href="lesson-2b-multiple-linear-regression.html#cb45-5" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span></code></pre></div>
<p>We’ll also continue working with the <code>ames</code> data set:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="lesson-2b-multiple-linear-regression.html#cb46-1" tabindex="-1"></a><span class="co"># stratified sampling with the rsample package</span></span>
<span id="cb46-2"><a href="lesson-2b-multiple-linear-regression.html#cb46-2" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span>
<span id="cb46-3"><a href="lesson-2b-multiple-linear-regression.html#cb46-3" tabindex="-1"></a></span>
<span id="cb46-4"><a href="lesson-2b-multiple-linear-regression.html#cb46-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb46-5"><a href="lesson-2b-multiple-linear-regression.html#cb46-5" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)</span>
<span id="cb46-6"><a href="lesson-2b-multiple-linear-regression.html#cb46-6" tabindex="-1"></a>ames_train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb46-7"><a href="lesson-2b-multiple-linear-regression.html#cb46-7" tabindex="-1"></a>ames_test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="adding-additional-predictors" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Adding additional predictors<a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/til7QprCrQo?si=W4TAxfYluSMgpYmH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
<p>In the last lesson we saw how we could use the above ground square footage (<code>Gr_Liv_Area</code>) of a house to predict the sale price (<code>Sale_Price</code>).</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="lesson-2b-multiple-linear-regression.html#cb47-1" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb47-2"><a href="lesson-2b-multiple-linear-regression.html#cb47-2" tabindex="-1"></a>   <span class="fu">fit</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area, <span class="at">data =</span> ames_train)</span>
<span id="cb47-3"><a href="lesson-2b-multiple-linear-regression.html#cb47-3" tabindex="-1"></a></span>
<span id="cb47-4"><a href="lesson-2b-multiple-linear-regression.html#cb47-4" tabindex="-1"></a><span class="fu">tidy</span>(model1)</span>
<span id="cb47-5"><a href="lesson-2b-multiple-linear-regression.html#cb47-5" tabindex="-1"></a><span class="do">## # A tibble: 2 × 5</span></span>
<span id="cb47-6"><a href="lesson-2b-multiple-linear-regression.html#cb47-6" tabindex="-1"></a><span class="do">##   term        estimate std.error statistic   p.value</span></span>
<span id="cb47-7"><a href="lesson-2b-multiple-linear-regression.html#cb47-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb47-8"><a href="lesson-2b-multiple-linear-regression.html#cb47-8" tabindex="-1"></a><span class="do">## 1 (Intercept)   15938.   3852.        4.14 3.65e-  5</span></span>
<span id="cb47-9"><a href="lesson-2b-multiple-linear-regression.html#cb47-9" tabindex="-1"></a><span class="do">## 2 Gr_Liv_Area     110.      2.42     45.3  5.17e-311</span></span></code></pre></div>
<p>From our model we interpreted the results as that the mean selling price increases by 109.67 for each additional one square foot of above ground living space. We also determined that the <code>Gr_Liv_Area</code> coefficient is statistically different from zero based on the <code>p.value</code>.</p>
<p>And, we saw that our model has a generalization RMSE value of 55942, which means that on average, our model’s predicted sales price differs from the actual sale price by $55,942.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="lesson-2b-multiple-linear-regression.html#cb48-1" tabindex="-1"></a>model1 <span class="sc">%&gt;%</span></span>
<span id="cb48-2"><a href="lesson-2b-multiple-linear-regression.html#cb48-2" tabindex="-1"></a>   <span class="fu">predict</span>(ames_test) <span class="sc">%&gt;%</span></span>
<span id="cb48-3"><a href="lesson-2b-multiple-linear-regression.html#cb48-3" tabindex="-1"></a>   <span class="fu">bind_cols</span>(ames_test) <span class="sc">%&gt;%</span></span>
<span id="cb48-4"><a href="lesson-2b-multiple-linear-regression.html#cb48-4" tabindex="-1"></a>   <span class="fu">rmse</span>(<span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb48-5"><a href="lesson-2b-multiple-linear-regression.html#cb48-5" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb48-6"><a href="lesson-2b-multiple-linear-regression.html#cb48-6" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb48-7"><a href="lesson-2b-multiple-linear-regression.html#cb48-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb48-8"><a href="lesson-2b-multiple-linear-regression.html#cb48-8" tabindex="-1"></a><span class="do">## 1 rmse    standard      55942.</span></span></code></pre></div>
<p>However, we are only using a single predictor variable to try predict the sale price. In reality, we likely can use other home attributes to do a better job at predicting sale price. For example, we may wish to understand if above ground square footage (<code>Gr_Liv_Area</code>) and the year the house was built (<code>Year_Built</code>) are (linearly) related to sale price (<code>Sale_Price</code>). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the <em>multiple linear regression</em> (MLR) model.</p>
<p>With two predictors, the MLR model becomes:</p>
<p><span class="math display">\[\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are features of interest. In our Ames housing example, <span class="math inline">\(x_1\)</span> can represent <code>Gr_Liv_Area</code> and <span class="math inline">\(x_2\)</span> can represent <code>Year_Built</code>.</p>
<p>In R, multiple linear regression models can be fit by separating all the features of interest with a <code>+</code>:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="lesson-2b-multiple-linear-regression.html#cb49-1" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb49-2"><a href="lesson-2b-multiple-linear-regression.html#cb49-2" tabindex="-1"></a>   <span class="fu">fit</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> Year_Built, <span class="at">data =</span> ames_train)</span>
<span id="cb49-3"><a href="lesson-2b-multiple-linear-regression.html#cb49-3" tabindex="-1"></a></span>
<span id="cb49-4"><a href="lesson-2b-multiple-linear-regression.html#cb49-4" tabindex="-1"></a><span class="fu">tidy</span>(model2)</span>
<span id="cb49-5"><a href="lesson-2b-multiple-linear-regression.html#cb49-5" tabindex="-1"></a><span class="do">## # A tibble: 3 × 5</span></span>
<span id="cb49-6"><a href="lesson-2b-multiple-linear-regression.html#cb49-6" tabindex="-1"></a><span class="do">##   term          estimate std.error statistic   p.value</span></span>
<span id="cb49-7"><a href="lesson-2b-multiple-linear-regression.html#cb49-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb49-8"><a href="lesson-2b-multiple-linear-regression.html#cb49-8" tabindex="-1"></a><span class="do">## 1 (Intercept) -2102905.   69441.       -30.3 9.17e-167</span></span>
<span id="cb49-9"><a href="lesson-2b-multiple-linear-regression.html#cb49-9" tabindex="-1"></a><span class="do">## 2 Gr_Liv_Area       93.8      2.07      45.3 1.17e-310</span></span>
<span id="cb49-10"><a href="lesson-2b-multiple-linear-regression.html#cb49-10" tabindex="-1"></a><span class="do">## 3 Year_Built      1087.      35.6       30.5 3.78e-169</span></span></code></pre></div>
<p>The LS estimates of the regression coefficients are <span class="math inline">\(\widehat{b}_1 =\)</span> 93.828 and <span class="math inline">\(\widehat{b}_2 =\)</span> 1086.852 (the estimated intercept is -2.1029046^{6}. In other words, every one square foot increase to above ground square footage is associated with an additional $93.83 in mean selling price <strong><em>when holding the year the house was built constant</em></strong>. Likewise, for every year newer a home is there is approximately an increase of $1,086.85 in selling price <strong><em>when holding the above ground square footage constant</em></strong>.</p>
<p>As our model results show above, the <code>p.value</code>s for our two coefficients suggest that both are stastically different than zero. This can also be confirmed by computing our confidence intervals around these coefficient estimates as we did before.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="lesson-2b-multiple-linear-regression.html#cb50-1" tabindex="-1"></a><span class="fu">confint</span>(model2<span class="sc">$</span>fit)</span>
<span id="cb50-2"><a href="lesson-2b-multiple-linear-regression.html#cb50-2" tabindex="-1"></a><span class="do">##                     2.5 %        97.5 %</span></span>
<span id="cb50-3"><a href="lesson-2b-multiple-linear-regression.html#cb50-3" tabindex="-1"></a><span class="do">## (Intercept) -2.239086e+06 -1.966723e+06</span></span>
<span id="cb50-4"><a href="lesson-2b-multiple-linear-regression.html#cb50-4" tabindex="-1"></a><span class="do">## Gr_Liv_Area  8.976367e+01  9.789288e+01</span></span>
<span id="cb50-5"><a href="lesson-2b-multiple-linear-regression.html#cb50-5" tabindex="-1"></a><span class="do">## Year_Built   1.017072e+03  1.156632e+03</span></span></code></pre></div>
<p>Now, instead of modeling sale price with the the “best fitting” line that minimizes residuals, we are modeling sale price with the best fitting hyperplane that minimizes the residuals, which is illustrated below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lm-mls-plane"></span>
<div class="plotly html-widget html-fill-item" id="htmlwidget-50473ff1116f738e10c7" style="width:576px;height:336px;"></div>
<script type="application/json" data-for="htmlwidget-50473ff1116f738e10c7">{"x":{"visdat":{"dd2596b7682":["function () ","plotlyVisDat"]},"cur_data":"dd2596b7682","attrs":{"dd2596b7682":{"x":{},"y":{},"z":{},"mode":"markers","marker":{"size":5,"opacity":0.25},"showlegend":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"},"dd2596b7682.1":{"x":[334,384,434,484,534,584,634,684,734,784,834,884,934,984,1034,1084,1134,1184,1234,1284,1334,1384,1434,1484,1534,1584,1634,1684,1734,1784,1834,1884,1934,1984,2034,2084,2134,2184,2234,2284,2334,2384,2434,2484,2534,2584,2634,2684,2734,2784,2834,2884,2934,2984,3034,3084,3134,3184,3234,3284,3334,3384,3434,3484,3534,3584,3634,3684,3734,3784,3834,3884,3934,3984,4034,4084,4134,4184,4234,4284,4334,4384,4434,4484,4534,4584,4634,4684,4734,4784,4834,4884,4934,4984,5034,5084,5134,5184,5234,5284,5334,5384,5434,5484,5534,5584,5634],"y":[1872,1882,1892,1902,1912,1922,1932,1942,1952,1962,1972,1982,1992,2002],"z":[[-36978.325543317478,-32286.911792061524,-27595.498040805571,-22904.084289549617,-18212.670538293663,-13521.256787037943,-8829.8430357819889,-4138.4292845260352,552.98446672991849,5244.3982179858722,9935.8119692418259,14627.22572049778,19318.6394717535,24010.053223009454,28701.466974265408,33392.880725521361,38084.294476777315,42775.708228033269,47467.12197928899,52158.535730544943,56849.949481800897,61541.363233056851,66232.776984312804,70924.190735568758,75615.604486824479,80307.018238080433,84998.431989336386,89689.84574059234,94381.259491848294,99072.673243104247,103764.0869943602,108455.50074561592,113146.91449687188,117838.32824812783,122529.74199938378,127221.15575063974,131912.56950189569,136603.98325315164,141295.39700440736,145986.81075566332,150678.22450691927,155369.63825817523,160061.05200943118,164752.4657606869,169443.87951194285,174135.29326319881,178826.70701445476,183518.12076571072,188209.53451696667,192900.94826822262,197592.36201947834,202283.7757707343,206975.18952199025,211666.6032732462,216358.01702450216,221049.43077575811,225740.84452701407,230432.25827826979,235123.67202952574,239815.08578078169,244506.49953203765,249197.9132832936,253889.32703454932,258580.74078580551,263272.15453706123,267963.56828831718,272654.98203957314,277346.39579082909,282037.80954208504,286729.22329334076,291420.63704459672,296112.05079585267,300803.46454710863,305494.87829836458,310186.29204962053,314877.70580087649,319569.11955213221,324260.53330338816,328951.94705464412,333643.36080590007,338334.77455715602,343026.18830841174,347717.60205966793,352409.01581092365,357100.4295621796,361791.84331343556,366483.25706469151,371174.67081594747,375866.08456720319,380557.49831845914,385248.91206971509,389940.32582097105,394631.739572227,399323.15332348295,404014.56707473891,408705.98082599463,413397.39457725058,418088.80832850654,422780.22207976249,427471.63583101844,432163.04958227417,436854.46333353035,441545.87708478607,446237.29083604203,450928.70458729798,455620.11833855393,460311.53208980989],[-26109.80202381569,-21418.388272559736,-16726.974521303782,-12035.560770047829,-7344.1470187918749,-2652.733267536154,2038.6804837197997,6730.0942349757534,11421.507986231707,16112.921737487661,20804.335488743614,25495.749239999568,30187.162991255289,34878.576742511243,39569.990493767196,44261.40424502315,48952.817996279104,53644.231747535057,58335.645498790778,63027.059250046732,67718.473001302686,72409.886752558639,77101.300503814593,81792.714255070547,86484.128006326267,91175.541757582221,95866.955508838175,100558.36926009413,105249.78301135008,109941.19676260604,114632.61051386199,119324.02426511771,124015.43801637366,128706.85176762962,133398.26551888557,138089.67927014153,142781.09302139748,147472.50677265343,152163.92052390915,156855.33427516511,161546.74802642106,166238.16177767701,170929.57552893297,175620.98928018869,180312.40303144464,185003.8167827006,189695.23053395655,194386.6442852125,199078.05803646846,203769.47178772441,208460.88553898013,213152.29929023609,217843.71304149204,222535.12679274799,227226.54054400395,231917.9542952599,236609.36804651585,241300.78179777157,245992.19554902753,250683.60930028348,255375.02305153944,260066.43680279539,264757.85055405111,269449.2643053073,274140.67805656302,278832.09180781897,283523.50555907493,288214.91931033088,292906.33306158683,297597.74681284255,302289.16056409851,306980.57431535446,311671.98806661041,316363.40181786637,321054.81556912232,325746.22932037828,330437.643071634,335129.05682288995,339820.4705741459,344511.88432540186,349203.29807665781,353894.71182791353,358586.12557916972,363277.53933042544,367968.95308168139,372660.36683293735,377351.7805841933,382043.19433544925,386734.60808670497,391426.02183796093,396117.43558921688,400808.84934047284,405500.26309172879,410191.67684298474,414883.0905942407,419574.50434549642,424265.91809675237,428957.33184800833,433648.74559926428,438340.15935052023,443031.57310177595,447722.98685303214,452414.40060428786,457105.81435554381,461797.22810679977,466488.64185805572,471180.05560931168],[-15241.278504314134,-10549.86475305818,-5858.4510018022265,-1167.0372505462728,3524.3765007096808,8215.7902519654017,12907.204003221355,17598.617754477309,22290.031505733263,26981.445256989216,31672.85900824517,36364.272759501124,41055.686510756845,45747.100262012798,50438.514013268752,55129.927764524706,59821.341515780659,64512.755267036613,69204.169018292334,73895.582769548288,78586.996520804241,83278.410272060195,87969.824023316149,92661.237774572102,97352.651525827823,102044.06527708378,106735.47902833973,111426.89277959568,116118.30653085164,120809.72028210759,125501.13403336355,130192.54778461927,134883.96153587522,139575.37528713117,144266.78903838713,148958.20278964308,153649.61654089903,158341.03029215499,163032.44404341071,167723.85779466666,172415.27154592262,177106.68529717857,181798.09904843452,186489.51279969024,191180.9265509462,195872.34030220215,200563.75405345811,205255.16780471406,209946.58155597001,214637.99530722597,219329.40905848169,224020.82280973764,228712.2365609936,233403.65031224955,238095.0640635055,242786.47781476146,247477.89156601741,252169.30531727313,256860.71906852908,261552.13281978504,266243.54657104099,270934.96032229695,275626.37407355267,280317.78782480885,285009.20157606457,289700.61532732053,294392.02907857648,299083.44282983243,303774.85658108839,308466.27033234411,313157.68408360006,317849.09783485602,322540.51158611197,327231.92533736792,331923.33908862388,336614.75283987983,341306.16659113555,345997.58034239151,350688.99409364746,355380.40784490341,360071.82159615937,364763.23534741509,369454.64909867127,374146.062849927,378837.47660118295,383528.8903524389,388220.30410369486,392911.71785495081,397603.13160620653,402294.54535746248,406985.95910871844,411677.37285997439,416368.78661123035,421060.2003624863,425751.61411374225,430443.02786499797,435134.44161625393,439825.85536750988,444517.26911876583,449208.68287002179,453900.09662127751,458591.5103725337,463282.92412378942,467974.33787504537,472665.75162630132,477357.16537755728,482048.57912881323],[-4372.7549848123454,318.65876644360833,5010.072517699562,9701.4862689555157,14392.900020211469,19084.31377146719,23775.727522723144,28467.141273979098,33158.555025235051,37849.968776491005,42541.382527746959,47232.796279002912,51924.210030258633,56615.623781514587,61307.037532770541,65998.451284026494,70689.865035282448,75381.278786538402,80072.692537794122,84764.106289050076,89455.52004030603,94146.933791561984,98838.347542817937,103529.76129407389,108221.17504532961,112912.58879658557,117604.00254784152,122295.41629909747,126986.83005035343,131678.24380160938,136369.65755286533,141061.07130412105,145752.48505537701,150443.89880663296,155135.31255788892,159826.72630914487,164518.14006040082,169209.55381165678,173900.9675629125,178592.38131416845,183283.79506542441,187975.20881668036,192666.62256793631,197358.03631919203,202049.45007044799,206740.86382170394,211432.27757295989,216123.69132421585,220815.1050754718,225506.51882672776,230197.93257798348,234889.34632923943,239580.76008049538,244272.17383175134,248963.58758300729,253655.00133426324,258346.4150855192,263037.82883677492,267729.24258803087,272420.65633928683,277112.07009054278,281803.48384179873,286494.89759305445,291186.31134431064,295877.72509556636,300569.13884682232,305260.55259807827,309951.96634933422,314643.38010059018,319334.7938518459,324026.20760310185,328717.62135435781,333409.03510561376,338100.44885686971,342791.86260812567,347483.27635938162,352174.69011063734,356866.10386189329,361557.51761314925,366248.9313644052,370940.34511566116,375631.75886691688,380323.17261817306,385014.58636942878,389706.00012068474,394397.41387194069,399088.82762319664,403780.2413744526,408471.65512570832,413163.06887696427,417854.48262822023,422545.89637947618,427237.31013073213,431928.72388198809,436620.13763324404,441311.55138449976,446002.96513575572,450694.37888701167,455385.79263826762,460077.20638952358,464768.6201407793,469460.03389203548,474151.44764329121,478842.86139454716,483534.27514580311,488225.68889705907,492917.10264831502],[6495.7685346892104,11187.182285945164,15878.596037201118,20570.009788457071,25261.423539713025,29952.837290968746,34644.2510422247,39335.664793480653,44027.078544736607,48718.492295992561,53409.906047248514,58101.319798504468,62792.733549760189,67484.147301016143,72175.561052272096,76866.97480352805,81558.388554784004,86249.802306039957,90941.216057295678,95632.629808551632,100324.04355980759,105015.45731106354,109706.87106231949,114398.28481357545,119089.69856483117,123781.11231608712,128472.52606734307,133163.93981859903,137855.35356985498,142546.76732111094,147238.18107236689,151929.59482362261,156621.00857487856,161312.42232613452,166003.83607739047,170695.24982864643,175386.66357990238,180078.07733115833,184769.49108241405,189460.90483367001,194152.31858492596,198843.73233618191,203535.14608743787,208226.55983869359,212917.97358994954,217609.3873412055,222300.80109246145,226992.2148437174,231683.62859497336,236375.04234622931,241066.45609748503,245757.86984874099,250449.28359999694,255140.69735125289,259832.11110250885,264523.5248537648,269214.93860502075,273906.35235627647,278597.76610753243,283289.17985878838,287980.59361004434,292672.00736130029,297363.42111255601,302054.8348638122,306746.24861506792,311437.66236632387,316129.07611757983,320820.48986883578,325511.90362009173,330203.31737134745,334894.73112260341,339586.14487385936,344277.55862511531,348968.97237637127,353660.38612762722,358351.79987888318,363043.2136301389,367734.62738139485,372426.0411326508,377117.45488390676,381808.86863516271,386500.28238641843,391191.69613767462,395883.10988893034,400574.52364018629,405265.93739144225,409957.3511426982,414648.76489395415,419340.17864520987,424031.59239646583,428723.00614772178,433414.41989897774,438105.83365023369,442797.24740148964,447488.6611527456,452180.07490400132,456871.48865525727,461562.90240651323,466254.31615776918,470945.72990902513,475637.14366028085,480328.55741153704,485019.97116279276,489711.38491404871,494402.79866530467,499094.21241656062,503785.62616781658],[17364.292054190999,22055.705805446953,26747.119556702906,31438.53330795886,36129.947059214814,40821.360810470534,45512.774561726488,50204.188312982442,54895.602064238396,59587.015815494349,64278.429566750303,68969.843318006257,73661.257069261977,78352.670820517931,83044.084571773885,87735.498323029839,92426.912074285792,97118.325825541746,101809.73957679747,106501.15332805342,111192.56707930937,115883.98083056533,120575.39458182128,125266.80833307724,129958.22208433296,134649.63583558891,139341.04958684486,144032.46333810082,148723.87708935677,153415.29084061272,158106.70459186868,162798.1183431244,167489.53209438035,172180.94584563631,176872.35959689226,181563.77334814821,186255.18709940417,190946.60085066012,195638.01460191584,200329.4283531718,205020.84210442775,209712.2558556837,214403.66960693966,219095.08335819538,223786.49710945133,228477.91086070728,233169.32461196324,237860.73836321919,242552.15211447515,247243.5658657311,251934.97961698682,256626.39336824277,261317.80711949873,266009.22087075468,270700.63462201064,275392.04837326659,280083.46212452254,284774.87587577826,289466.28962703422,294157.70337829017,298849.11712954612,303540.53088080208,308231.9446320578,312923.35838331399,317614.77213456971,322306.18588582566,326997.59963708161,331689.01338833757,336380.42713959352,341071.84089084924,345763.2546421052,350454.66839336115,355146.0821446171,359837.49589587306,364528.90964712901,369220.32339838496,373911.73714964068,378603.15090089664,383294.56465215259,387985.97840340855,392677.3921546645,397368.80590592022,402060.21965717641,406751.63340843213,411443.04715968808,416134.46091094404,420825.87466219999,425517.28841345594,430208.70216471166,434900.11591596762,439591.52966722357,444282.94341847952,448974.35716973548,453665.77092099143,458357.18467224739,463048.59842350311,467740.01217475906,472431.42592601501,477122.83967727097,481814.25342852692,486505.66717978264,491197.08093103883,495888.49468229455,500579.9084335505,505271.32218480646,509962.73593606241,514654.14968731836],[28232.815573692787,32924.229324948741,37615.643076204695,42307.056827460648,46998.470578716602,51689.884329972323,56381.298081228277,61072.71183248423,65764.125583740184,70455.539334996138,75146.953086252091,79838.366837508045,84529.780588763766,89221.19434001972,93912.608091275673,98604.021842531627,103295.43559378758,107986.84934504353,112678.26309629926,117369.67684755521,122061.09059881116,126752.50435006712,131443.91810132307,136135.33185257902,140826.74560383474,145518.1593550907,150209.57310634665,154900.98685760261,159592.40060885856,164283.81436011451,168975.22811137047,173666.64186262619,178358.05561388214,183049.46936513809,187740.88311639405,192432.29686765,197123.71061890596,201815.12437016191,206506.53812141763,211197.95187267358,215889.36562392954,220580.77937518549,225272.19312644145,229963.60687769717,234655.02062895312,239346.43438020907,244037.84813146503,248729.26188272098,253420.67563397693,258112.08938523289,262803.50313648861,267494.91688774456,272186.33063900052,276877.74439025647,281569.15814151242,286260.57189276838,290951.98564402433,295643.39939528005,300334.81314653601,305026.22689779196,309717.64064904791,314409.05440030387,319100.46815155959,323791.88190281577,328483.29565407149,333174.70940532745,337866.1231565834,342557.53690783936,347248.95065909531,351940.36441035103,356631.77816160698,361323.19191286294,366014.60566411889,370706.01941537485,375397.4331666308,380088.84691788675,384780.26066914247,389471.67442039843,394163.08817165438,398854.50192291033,403545.91567416629,408237.32942542201,412928.7431766782,417620.15692793392,422311.57067918987,427002.98443044582,431694.39818170178,436385.81193295773,441077.22568421345,445768.63943546941,450460.05318672536,455151.46693798131,459842.88068923727,464534.29444049322,469225.70819174917,473917.1219430049,478608.53569426085,483299.9494455168,487991.36319677276,492682.77694802871,497374.19069928443,502065.60445054062,506757.01820179634,511448.43195305229,516139.84570430825,520831.2594555642,525522.67320682015],[39101.339093194343,43792.752844450297,48484.166595706251,53175.580346962204,57866.994098218158,62558.407849473879,67249.821600729832,71941.235351985786,76632.64910324174,81324.062854497693,86015.476605753647,90706.890357009601,95398.304108265322,100089.71785952128,104781.13161077723,109472.54536203318,114163.95911328914,118855.37286454509,123546.78661580081,128238.20036705676,132929.61411831272,137621.02786956867,142312.44162082463,147003.85537208058,151695.2691233363,156386.68287459225,161078.09662584821,165769.51037710416,170460.92412836011,175152.33787961607,179843.75163087202,184535.16538212774,189226.5791333837,193917.99288463965,198609.4066358956,203300.82038715156,207992.23413840751,212683.64788966347,217375.06164091919,222066.47539217514,226757.88914343109,231449.30289468705,236140.716645943,240832.13039719872,245523.54414845468,250214.95789971063,254906.37165096658,259597.78540222254,264289.19915347849,268980.61290473444,273672.02665599016,278363.44040724612,283054.85415850207,287746.26790975803,292437.68166101398,297129.09541226993,301820.50916352589,306511.92291478161,311203.33666603756,315894.75041729352,320586.16416854947,325277.57791980542,329968.99167106114,334660.40542231733,339351.81917357305,344043.232924829,348734.64667608496,353426.06042734091,358117.47417859687,362808.88792985259,367500.30168110854,372191.71543236449,376883.12918362045,381574.5429348764,386265.95668613235,390957.37043738831,395648.78418864403,400340.19793989998,405031.61169115594,409723.02544241189,414414.43919366784,419105.85294492356,423797.26669617975,428488.68044743547,433180.09419869143,437871.50794994738,442562.92170120333,447254.33545245929,451945.74920371501,456637.16295497096,461328.57670622692,466019.99045748287,470711.40420873882,475402.81795999478,480094.23171125073,484785.64546250645,489477.0592137624,494168.47296501836,498859.88671627431,503551.30046753027,508242.71421878599,512934.12797004217,517625.54172129789,522316.95547255385,527008.3692238098,531699.78297506575,536391.19672632171],[49969.862612695899,54661.276363951853,59352.690115207806,64044.10386646376,68735.517617719714,73426.931368975434,78118.345120231388,82809.758871487342,87501.172622743296,92192.586373999249,96884.000125255203,101575.41387651116,106266.82762776688,110958.24137902283,115649.65513027878,120341.06888153474,125032.48263279069,129723.89638404665,134415.31013530237,139106.72388655832,143798.13763781427,148489.55138907023,153180.96514032618,157872.37889158214,162563.79264283786,167255.20639409381,171946.62014534976,176638.03389660572,181329.44764786167,186020.86139911762,190712.27515037358,195403.6889016293,200095.10265288525,204786.51640414121,209477.93015539716,214169.34390665311,218860.75765790907,223552.17140916502,228243.58516042074,232934.9989116767,237626.41266293265,242317.8264141886,247009.24016544456,251700.65391670028,256392.06766795623,261083.48141921218,265774.89517046814,270466.30892172409,275157.72267298005,279849.136424236,284540.55017549172,289231.96392674767,293923.37767800363,298614.79142925958,303306.20518051554,307997.61893177149,312689.03268302744,317380.44643428316,322071.86018553912,326763.27393679507,331454.68768805102,336146.10143930698,340837.5151905627,345528.92894181889,350220.34269307461,354911.75644433056,359603.17019558651,364294.58394684247,368985.99769809842,373677.41144935414,378368.8252006101,383060.23895186605,387751.652703122,392443.06645437796,397134.48020563391,401825.89395688986,406517.30770814558,411208.72145940154,415900.13521065749,420591.54896191345,425282.9627131694,429974.37646442512,434665.79021568131,439357.20396693703,444048.61771819298,448740.03146944894,453431.44522070489,458122.85897196084,462814.27272321656,467505.68647447252,472197.10022572847,476888.51397698442,481579.92772824038,486271.34147949633,490962.75523075229,495654.16898200801,500345.58273326396,505036.99648451991,509728.41023577587,514419.82398703182,519111.23773828754,523802.65148954373,528494.06524079945,533185.4789920554,537876.89274331136,542568.30649456731,547259.72024582326],[60838.386132197455,65529.799883453408,70221.213634709362,74912.627385965316,79604.041137221269,84295.45488847699,88986.868639732944,93678.282390988898,98369.696142244851,103061.1098935008,107752.52364475676,112443.93739601271,117135.35114726843,121826.76489852439,126518.17864978034,131209.59240103629,135901.00615229225,140592.4199035482,145283.83365480392,149975.24740605988,154666.66115731583,159358.07490857178,164049.48865982774,168740.90241108369,173432.31616233941,178123.72991359537,182815.14366485132,187506.55741610727,192197.97116736323,196889.38491861918,201580.79866987513,206272.21242113085,210963.62617238681,215655.03992364276,220346.45367489872,225037.86742615467,229729.28117741062,234420.69492866658,239112.1086799223,243803.52243117825,248494.9361824342,253186.34993369016,257877.76368494611,262569.17743620183,267260.59118745779,271952.00493871374,276643.41868996969,281334.83244122565,286026.2461924816,290717.65994373756,295409.07369499328,300100.48744624923,304791.90119750518,309483.31494876114,314174.72870001709,318866.14245127304,323557.556202529,328248.96995378472,332940.38370504067,337631.79745629663,342323.21120755258,347014.62495880853,351706.03871006425,356397.45246132044,361088.86621257616,365780.27996383212,370471.69371508807,375163.10746634402,379854.52121759998,384545.9349688557,389237.34872011165,393928.76247136761,398620.17622262356,403311.58997387951,408003.00372513547,412694.41747639142,417385.83122764714,422077.24497890309,426768.65873015905,431460.072481415,436151.48623267096,440842.89998392668,445534.31373518286,450225.72748643858,454917.14123769454,459608.55498895049,464299.96874020644,468991.3824914624,473682.79624271812,478374.20999397407,483065.62374523003,487757.03749648598,492448.45124774193,497139.86499899789,501831.27875025384,506522.69250150956,511214.10625276552,515905.52000402147,520596.93375527742,525288.34750653338,529979.7612577891,534671.17500904528,539362.58876030101,544054.00251155696,548745.41626281291,553436.83001406887,558128.24376532482],[71706.90965169901,76398.323402954964,81089.737154210918,85781.150905466871,90472.564656722825,95163.978407978546,99855.3921592345,104546.80591049045,109238.21966174641,113929.63341300236,118621.04716425831,123312.46091551427,128003.87466676999,132695.28841802594,137386.7021692819,142078.11592053785,146769.5296717938,151460.94342304976,156152.35717430548,160843.77092556143,165535.18467681739,170226.59842807334,174918.01217932929,179609.42593058525,184300.83968184097,188992.25343309692,193683.66718435287,198375.08093560883,203066.49468686478,207757.90843812074,212449.32218937669,217140.73594063241,221832.14969188836,226523.56344314432,231214.97719440027,235906.39094565623,240597.80469691218,245289.21844816813,249980.63219942385,254672.04595067981,259363.45970193576,264054.87345319171,268746.28720444767,273437.70095570339,278129.11470695934,282820.5284582153,287511.94220947125,292203.3559607272,296894.76971198316,301586.18346323911,306277.59721449483,310969.01096575079,315660.42471700674,320351.83846826269,325043.25221951865,329734.6659707746,334426.07972203055,339117.49347328627,343808.90722454223,348500.32097579818,353191.73472705414,357883.14847831009,362574.56222956581,367265.975980822,371957.38973207772,376648.80348333367,381340.21723458963,386031.63098584558,390723.04473710153,395414.45848835725,400105.87223961321,404797.28599086916,409488.69974212511,414180.11349338107,418871.52724463702,423562.94099589298,428254.3547471487,432945.76849840465,437637.1822496606,442328.59600091656,447020.00975217251,451711.42350342823,456402.83725468442,461094.25100594014,465785.66475719609,470477.07850845205,475168.492259708,479859.90601096395,484551.31976221967,489242.73351347563,493934.14726473158,498625.56101598754,503316.97476724349,508008.38851849944,512699.8022697554,517391.21602101112,522082.62977226707,526774.04352352303,531465.45727477898,536156.87102603493,540848.28477729065,545539.69852854684,550231.11227980256,554922.52603105851,559613.93978231447,564305.35353357042,568996.76728482638],[82575.433171201032,87266.846922456985,91958.260673712939,96649.674424968893,101341.08817622485,106032.50192748057,110723.91567873652,115415.32942999247,120106.74318124843,124798.15693250438,129489.57068376034,134180.98443501629,138872.39818627201,143563.81193752796,148255.22568878392,152946.63944003987,157638.05319129582,162329.46694255178,167020.8806938075,171712.29444506345,176403.70819631941,181095.12194757536,185786.53569883131,190477.94945008727,195169.36320134299,199860.77695259894,204552.1907038549,209243.60445511085,213935.0182063668,218626.43195762276,223317.84570887871,228009.25946013443,232700.67321139039,237392.08696264634,242083.50071390229,246774.91446515825,251466.3282164142,256157.74196767015,260849.15571892587,265540.56947018183,270231.98322143778,274923.39697269374,279614.81072394969,284306.22447520541,288997.63822646136,293689.05197771732,298380.46572897327,303071.87948022923,307763.29323148518,312454.70698274113,317146.12073399685,321837.53448525281,326528.94823650876,331220.36198776471,335911.77573902067,340603.18949027662,345294.60324153258,349986.0169927883,354677.43074404425,359368.8444953002,364060.25824655616,368751.67199781211,373443.08574906783,378134.49950032402,382825.91325157974,387517.32700283569,392208.74075409165,396900.1545053476,401591.56825660355,406282.98200785927,410974.39575911523,415665.80951037118,420357.22326162714,425048.63701288309,429740.05076413904,434431.464515395,439122.87826665072,443814.29201790667,448505.70576916263,453197.11952041858,457888.53327167453,462579.94702293025,467271.36077418644,471962.77452544216,476654.18827669811,481345.60202795407,486037.01577921002,490728.42953046598,495419.8432817217,500111.25703297765,504802.6707842336,509494.08453548956,514185.49828674551,518876.91203800146,523568.32578925742,528259.73954051314,532951.15329176909,537642.56704302505,542333.980794281,547025.39454553695,551716.80829679267,556408.22204804886,561099.63579930458,565791.04955056054,570482.46330181649,575173.87705307244,579865.2908043284],[93443.956690702587,98135.370441958541,102826.78419321449,107518.19794447045,112209.6116957264,116901.02544698212,121592.43919823808,126283.85294949403,130975.26670074998,135666.68045200594,140358.09420326189,145049.50795451785,149740.92170577357,154432.33545702952,159123.74920828547,163815.16295954143,168506.57671079738,173197.99046205333,177889.40421330906,182580.81796456501,187272.23171582096,191963.64546707692,196655.05921833287,201346.47296958882,206037.88672084454,210729.3004721005,215420.71422335645,220112.12797461241,224803.54172586836,229494.95547712431,234186.36922838027,238877.78297963599,243569.19673089194,248260.61048214789,252952.02423340385,257643.4379846598,262334.85173591576,267026.26548717171,271717.67923842743,276409.09298968338,281100.50674093934,285791.92049219529,290483.33424345125,295174.74799470697,299866.16174596292,304557.57549721887,309248.98924847483,313940.40299973078,318631.81675098673,323323.23050224269,328014.64425349841,332706.05800475436,337397.47175601032,342088.88550726627,346780.29925852222,351471.71300977818,356163.12676103413,360854.54051228985,365545.95426354581,370237.36801480176,374928.78176605771,379620.19551731367,384311.60926856939,389003.02301982557,393694.43677108129,398385.85052233725,403077.2642735932,407768.67802484916,412460.09177610511,417151.50552736083,421842.91927861678,426534.33302987274,431225.74678112869,435917.16053238465,440608.5742836406,445299.98803489655,449991.40178615227,454682.81553740823,459374.22928866418,464065.64303992013,468757.05679117609,473448.47054243181,478139.884293688,482831.29804494372,487522.71179619967,492214.12554745562,496905.53929871158,501596.95304996753,506288.36680122325,510979.78055247921,515671.19430373516,520362.60805499111,525054.02180624707,529745.43555750302,534436.84930875897,539128.26306001469,543819.67681127065,548511.0905625266,553202.50431378256,557893.91806503851,562585.33181629423,567276.74556755042,571968.15931880614,576659.57307006209,581350.98682131805,586042.400572574,590733.81432382995],[104312.48021020414,109003.8939614601,113695.30771271605,118386.721463972,123078.13521522796,127769.54896648368,132460.96271773963,137152.37646899559,141843.79022025154,146535.20397150749,151226.61772276345,155918.0314740194,160609.44522527512,165300.85897653108,169992.27272778703,174683.68647904298,179375.10023029894,184066.51398155489,188757.92773281061,193449.34148406656,198140.75523532252,202832.16898657847,207523.58273783443,212214.99648909038,216906.4102403461,221597.82399160205,226289.23774285801,230980.65149411396,235672.06524536991,240363.47899662587,245054.89274788182,249746.30649913754,254437.7202503935,259129.13400164945,263820.5477529054,268511.96150416136,273203.37525541731,277894.78900667327,282586.20275792899,287277.61650918494,291969.03026044089,296660.44401169685,301351.8577629528,306043.27151420852,310734.68526546448,315426.09901672043,320117.51276797638,324808.92651923234,329500.34027048829,334191.75402174424,338883.16777299996,343574.58152425592,348265.99527551187,352957.40902676783,357648.82277802378,362340.23652927973,367031.65028053569,371723.06403179141,376414.47778304736,381105.89153430331,385797.30528555927,390488.71903681522,395180.13278807094,399871.54653932713,404562.96029058285,409254.3740418388,413945.78779309476,418637.20154435071,423328.61529560667,428020.02904686239,432711.44279811834,437402.85654937429,442094.27030063025,446785.6840518862,451477.09780314215,456168.51155439811,460859.92530565383,465551.33905690978,470242.75280816574,474934.16655942169,479625.58031067764,484316.99406193336,489008.40781318955,493699.82156444527,498391.23531570123,503082.64906695718,507774.06281821313,512465.47656946909,517156.89032072481,521848.30407198076,526539.71782323672,531231.13157449267,535922.54532574862,540613.95907700458,545305.37282826053,549996.78657951625,554688.2003307722,559379.61408202816,564071.02783328411,568762.44158454007,573453.85533579579,578145.26908705197,582836.68283830769,587528.09658956365,592219.5103408196,596910.92409207555,601602.33784333151]],"mode":"markers","marker":{"size":5,"opacity":0.25},"showlegend":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Gr_Liv_Area"},"yaxis":{"title":"Year_Built"},"zaxis":{"title":"Sale_Price"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[1092,1092,836,752,1012,1902,900,1040,1728,1063,1206,1337,988,1057,827,1458,1844,1140,936,1605,747,1251,1633,1245,832,720,2256,765,1040,1068,1324,816,1092,1224,919,1884,1020,912,1067,1144,1172,1298,894,1664,1120,1096,960,924,1296,1647,864,856,1666,1154,1107,1128,1178,1044,952,945,1112,1092,630,1411,864,864,864,864,987,1365,1302,987,987,1302,1055,836,892,1217,841,784,980,864,1020,912,864,925,864,1776,1041,960,708,1203,1584,1392,1738,1202,1319,1112,1005,793,1699,599,725,1142,1040,1040,1041,1045,894,1224,1230,1378,1944,1131,1464,2372,1701,1960,936,1328,1387,612,968,792,1452,1510,1284,2138,1022,1869,1480,720,797,952,1324,1348,904,1301,800,875,995,2229,2128,1427,1864,892,1403,904,1112,1052,660,861,672,1089,845,1510,1535,1535,1535,1513,1040,894,1126,816,1149,698,1368,1529,864,944,1678,1560,960,1196,1298,1604,438,1032,1229,2168,1192,1526,572,810,1771,630,1344,833,1474,864,987,987,987,1014,907,720,729,1077,906,1252,882,773,1764,1216,869,1159,672,910,876,1507,984,1278,825,764,1323,1360,672,1656,1740,1027,899,928,901,1588,1344,1017,2358,984,935,784,1158,1342,1013,1216,896,1136,1362,520,1285,1609,768,1308,1040,1236,1406,825,1198,759,1344,1355,1040,1073,1258,1140,1006,672,1079,1367,1164,924,864,780,796,1092,996,1152,1152,1137,1204,864,790,1639,1374,1091,1090,796,816,778,1200,1656,912,733,1361,894,864,1872,1475,988,988,1092,988,816,1836,950,872,858,914,864,1092,1218,1218,1302,1365,958,894,1052,1728,912,1074,882,894,1224,932,334,666,808,1560,912,2380,1144,922,1216,1560,948,925,924,1034,882,691,952,1082,1238,864,925,864,960,1296,1022,1183,866,1472,1232,1358,968,1382,1435,1409,1367,1316,1214,1355,1315,1111,1196,1382,1889,840,1484,1005,1029,1040,913,894,948,868,897,955,894,943,605,1342,1362,1340,990,990,882,864,875,960,1351,1179,816,1152,948,1195,774,894,1346,768,864,948,789,954,1045,1517,803,1123,816,1077,845,1991,1001,1092,992,630,768,1608,816,974,864,858,874,1092,1365,987,1030,948,1092,987,965,988,1728,984,909,1024,912,941,1051,869,1252,936,1235,1145,1312,1310,713,833,720,1130,1214,864,1040,1040,1040,1040,1556,1328,1657,1004,813,1032,844,1750,1376,492,1163,840,1440,1368,1200,1248,960,1020,788,1118,1176,1344,1162,1015,1324,616,960,693,1430,1330,819,1376,1077,984,879,1150,1148,1124,1456,971,864,864,1436,1220,1012,1176,914,936,1669,1184,864,1482,1414,2230,1273,1340,1288,976,672,951,1336,480,936,1140,1475,874,630,1092,630,1092,1092,1092,1004,1078,1056,1456,1456,923,1097,1155,894,1040,1236,1416,1080,1479,1209,1775,1054,1078,1520,1268,2274,1604,1314,1188,1580,1064,1086,1224,1246,900,1116,1839,1347,1316,1652,1236,1132,1453,1710,1962,1812,1352,1074,996,1445,965,1026,848,992,1196,1121,1526,1687,1824,1188,833,1274,1434,2050,1601,1193,1792,1002,1026,1050,958,1548,1456,1322,1456,1548,1456,1120,1141,1114,1114,1882,980,1452,1620,1556,864,1568,894,1313,1292,1154,1445,1505,1394,1632,1215,1382,1866,1062,1031,1527,1200,1656,1690,1380,951,1105,1306,1478,1268,1904,1339,1183,1148,1461,1495,1806,1306,1558,1389,2007,1521,1432,1654,1176,1412,1539,1826,1930,1666,1121,899,1034,858,860,1138,1473,1733,1639,1049,1338,1224,2228,2240,1229,990,1097,990,907,914,1040,864,1268,768,848,848,1426,1072,1520,1981,1232,1608,1135,1228,1273,1343,1523,1229,926,1595,1344,1008,874,1404,796,1091,883,1488,1308,1178,1054,1055,1626,1266,1175,1114,1199,912,864,912,1392,1392,1252,2944,1740,1625,1464,925,1728,1728,1656,952,1128,1092,1114,1053,1252,1100,1118,1496,1337,1144,1098,1283,1196,1080,1104,1152,950,1558,1436,1256,1320,1588,1796,1117,1133,1539,1436,1122,1499,1627,1450,2350,1143,1738,1497,2009,1774,1505,838,1536,1969,1211,1588,1075,1096,894,1034,1126,1228,960,1601,1721,1350,904,1094,1269,990,990,1174,1026,848,848,848,848,1256,1181,5642,1072,1200,1470,1482,1694,1316,1264,1502,1556,1495,1092,1160,816,816,845,1750,1694,1187,1472,1424,1032,1620,981,914,767,1337,1548,1055,1412,1795,1100,1143,1094,2020,970,1057,999,1696,1055,1102,1093,1073,958,1138,964,1302,1374,1327,908,1150,1920,1644,1254,1008,1056,1056,1411,1154,1040,1008,928,1647,893,1728,2080,1846,1134,1040,1170,1242,1377,1800,1432,936,1072,1798,1642,2291,1616,1559,2554,1060,1274,1218,1322,1426,1513,1344,1108,1375,999,1452,1285,2654,1299,1176,1630,1626,1480,1022,990,894,1025,1009,1040,848,1214,1113,1125,1090,1072,2592,1422,1298,1436,1250,1355,1215,1601,1558,912,1080,1026,1120,2034,936,1040,1060,1008,1432,1178,816,1293,1204,1056,1074,1103,1096,1069,1142,1131,1141,1158,1217,1320,1212,1236,1223,1223,1193,1164,980,1051,976,1052,1200,1050,864,1712,1095,1053,816,1043,1624,1216,2136,1578,1008,1144,1364,1383,1370,1236,943,1050,1375,1008,1575,1005,1384,1595,2414,1086,1207,1194,816,1521,1150,1348,999,1416,1428,960,1566,1928,2104,1690,1577,1921,1640,1382,1064,934,1059,1159,1400,1967,960,810,894,1350,1488,1057,1855,1609,1388,914,1100,990,1388,894,1320,848,848,1072,1709,1708,1928,1595,1551,1479,1784,1510,1465,1717,1336,1285,1664,1210,2521,1652,1360,1728,1224,1329,1338,1465,1341,1502,1752,1844,1370,1626,1324,1269,1430,2110,1845,1744,1494,1923,1680,1573,1226,1610,1891,1714,1381,1645,1152,1556,1128,2207,1700,1264,1287,1385,1395,1675,1373,1440,1732,1208,1226,1682,1392,1484,2447,1652,1034,1191,1285,1034,2048,1214,1444,1908,1430,1344,1479,1608,1628,1382,1718,1636,1470,1516,1593,1481,1167,1442,1360,1934,1586,1671,1707,1868,1621,1116,1358,1306,1490,1484,1560,1555,1220,1145,1432,2267,1374,1419,1594,1342,1314,1800,1430,1590,1218,1221,1653,1812,1512,1200,1434,1730,1540,1224,1256,1471,1512,1739,2499,1442,1489,2090,1336,1334,1680,2061,1320,1342,1728,1644,1453,1986,2142,1382,2544,2634,1742,1548,2019,1944,1200,1200,1464,1683,1774,2071,1775,1768,1290,1232,1422,1661,1463,1779,1431,1660,1430,1444,1492,1725,1504,1417,1716,1212,1630,1500,1476,1396,1922,3082,1913,1567,1494,964,2480,1564,1561,1982,1287,1217,1412,1479,2014,1224,1110,1567,1414,2112,1470,1604,1636,1384,1337,1280,1604,1658,1895,1228,1337,1126,2044,1523,1964,1337,1358,1455,1504,1246,1515,1593,1166,1456,1720,1986,1490,1414,1651,1428,1696,1663,1640,1162,1609,1680,1679,1657,1737,1294,1430,1391,1250,1245,1501,1549,1404,1607,1812,1682,1876,999,1978,1425,1165,1265,1350,1661,1820,1362,1537,1431,1440,1261,1570,1392,1611,2082,2495,1573,1820,1716,1666,1188,1383,2022,1357,1674,1643,1652,1790,1724,1479,1558,1640,1764,1493,1838,1710,1582,1234,1361,1456,1656,1398,1132,1424,1920,2555,1680,1955,1820,1118,1279,1442,1394,1256,1675,1344,1344,1200,1968,1277,1470,1600,1384,1714,1253,2299,1642,2090,1179,1321,1440,1868,1664,1930,1969,2030,1353,1220,1484,1743,1877,1337,1302,1302,1554,1405,1496,1536,1484,1458,1326,1456,1340,1504,1456,1258,1258,1402,1266,1720,1948,1690,1647,1430,1320,1690,1644,1335,1721,1349,1575,1701,1151,1565,1352,1550,1241,1250,1501,1512,1573,1525,1441,1432,1855,1262,2620,1082,1295,1610,1242,1042,1280,1651,1812,1772,1635,2264,1824,2294,1952,1302,1522,2060,1316,1274,1240,1208,1500,1873,1200,1308,1796,1786,1690,1668,1932,1337,1476,1422,1190,1330,1491,1536,1408,5095,4676,3395,1502,1768,1426,1646,1755,1560,1680,1020,2515,1299,1478,1328,1240,1717,1756,1146,1622,1356,1851,1776,1511,1169,1501,1632,2320,1621,1574,1552,1083,1875,1309,1364,1368,1496,1326,1456,1492,1414,1248,1506,1838,1509,1414,1598,1509,1886,1671,1504,1278,1716,1576,1262,1266,1686,1585,1837,1603,1398,1143,1524,1208,1226,1565,1433,1246,1336,2640,1466,1146,1773,1600,2014,1641,1827,1320,1458,1721,1040,1920,1779,1437,1000,1453,2559,1208,1220,1294,1809,1596,988,1499,1861,1776,1541,1282,2049,1390,1948,1700,1737,1552,1720,1336,1456,1310,1636,1802,1609,1491,1339,1403,1216,1389,2000,1656,2110,3279,1856,1674,2334,1704,1940,1544,1541,1698,1822,1535,1358,1752,1960,2035,2599,2475,2622,2270,3238,1595,1547,1566,1468,2084,2349,1488,1418,1848,1492,2495,2978,2268,1981,1508,1949,1694,3222,1601,1829,2098,1661,1668,2133,2177,1902,1801,1646,1916,1346,1968,1894,2018,1803,2704,2283,2522,1659,1478,1734,2234,2046,1530,2353,1602,2673,1884,1718,2772,3140,2402,1600,2042,2184,2674,1717,1713,1531,1808,2224,1760,1502,2392,2452,2400,1606,1314,2492,1884,1456,1712,1324,2054,2060,2365,2794,2687,2450,2612,2344,2646,2061,2448,1468,1696,1658,1578,1702,1432,1852,1530,1448,2007,1795,1953,1550,1836,1662,1450,2122,1844,1755,3005,1694,2640,2288,1567,2640,1576,2134,2196,1879,2020,2787,2787,1630,1720,1838,1800,1604,1254,1644,1764,1573,1840,1611,2184,2153,1513,1828,2034,1602,1698,1906,2365,1648,2052,1710,2036,1620,1296,1838,1978,2117,1593,1682,2504,1884,1710,1688,2840,2340,1932,2332,2713,1582,2464,2748,2790,2331,2088,2332,2470,2668,2046,1575,2649,2690,1866,1702,1684,1800,1342,1342,1358,1892,2322,2199,2630,2198,2501,2113,2418,1580,1933,1574,1506,1563,1428,1874,1460,1768,1958,2078,1218,2683,2786,1638,1310,1419,1557,1709,1789,2393,1288,1620,2614,2715,1988,2775,1524,1363,1663,2358,2108,2329,1626,2200,2037,2276,2097,1511,1771,1494,1823,1786,1795,2031,1865,2031,2263,1947,1786,2290,2327,1536,1564,3820,1512,3447,1954,2161,1898,1640,1652,1648,1646,1646,2032,1689,1689,1501,2898,1537,1440,1964,2223,1652,1922,2541,1612,1419,2234,2042,1828,1694,2550,2046,2172,2552,1973,2643,1634,2758,2290,2152,2473,2161,1802,2956,2468,2385,2374,1614,1828,1577,1561,1324,1589,2285,3078,2582,2385,4316,2013,2398,2520,2531,3228,2202,2538,1542,1905,1464,1534,1966,1636,1528,1726,1689,1588,1538,1506,1694,1977,1830,1792,1761,1685,2443,2466,1494,2167,2872,1950,2048,1494,2362,1437,2497,2154,2267,1824,1325,2422,1822,2243,2127,1982,1958,2696,2828,1651,2140,1651,2574,1546,2062,2144,1953,1743,1842,2169,2452,1668,1341,1928,1718,2624,3194,1991,2727,2320,2259,1488,2726,1625,1392,1409,1501,1502,2279,1689,1564,1571,1646,1922,1629,2486,1776,1824,2482,2236,2462,1670,1801,2212,2798,3390,2473,2868,2795,2945,1714,2000,2183,2263,2589,2596,1740,1868,1850,2452,2206,2098,2091,2324,2253,2389,1944,1970,1792,1780,1792,2728,1914,1565,1686,1863,2373,1557,1511,1548,1936,1889,2291,1976,2234,3627,2855,1924,2726,3500,2494,2799,1964,1670,2097,2640,1731,2646,2417,2826,2520,1624,1235,2156,3608,2287,2008,2201,2009,3672,1876,1614,1962,1846,1734,1614,1915,1533,1525,1494,1588,1779,1989,1762,1755,1358,1787,2214,1995,2097,1919,2314,1517,2057,1671,2345,1811,1647,2519,1295,1650,1800,1960,2028,1838,1368],"y":[1971,1971,1975,1984,1920,1978,1967,1962,1962,1954,1959,1956,1958,1953,1955,1948,1900,1910,1920,1915,1945,1938,1948,1920,1923,1928,1910,1925,1936,1923,1920,1963,1930,1917,1940,1907,1875,1967,1950,1956,1954,1964,1976,1965,1948,1975,1958,1941,1924,1915,1914,1939,1931,1922,1921,1920,1900,1952,1896,1973,1976,1970,1972,1977,1970,1970,1970,1971,1973,1970,1971,1972,1972,1972,1975,1976,1967,1980,1950,1928,1962,1961,1965,1963,1965,1965,1959,1958,1959,1955,1940,1950,1967,1968,1920,1956,1953,1951,1955,1945,1900,1940,1890,1958,1950,1949,1958,1963,1962,1926,1910,1935,1910,1941,1910,1892,1920,1952,1953,1950,1954,1940,1910,1923,1920,1910,1947,1898,1910,1890,1920,1920,1900,1916,1936,1925,1926,1924,1936,1931,1925,1912,1922,1915,1902,1923,1915,1922,1934,1967,1947,1940,1941,1954,1957,1940,1979,1979,1979,1979,1978,1972,1949,1938,1960,1947,1950,1945,1922,1952,1959,1956,1956,1920,1941,1941,1920,1980,1980,1900,1920,1915,1925,1910,1960,1970,1970,1985,1952,1972,1973,1971,1971,1956,1957,1949,1935,1939,1937,1950,1952,1953,1957,1947,1948,1925,1940,1953,1951,1960,1954,1954,1954,1925,1948,1910,1940,1940,1920,1920,1910,1957,1954,1949,1916,1900,1872,1959,1930,1924,1929,1939,1926,1920,1946,1929,1920,1927,1948,1910,1930,1915,1925,1925,1920,1955,1920,1912,1915,1940,1965,1967,1969,1920,1959,1940,1948,1900,1950,1975,1972,1972,1910,1966,1940,1955,1955,1947,1954,1914,1930,1936,1938,1951,1940,1922,1930,1940,1930,1967,1930,1952,1895,1935,1920,1976,1976,1973,1973,1970,1954,1983,1900,1971,1971,1971,1961,1971,1973,1973,1973,1970,1971,1976,1977,1963,1964,1958,1956,1959,1960,1962,1950,1946,1946,1958,1968,1958,1965,1960,1957,1955,1958,1952,1956,1952,1940,1956,1920,1968,1948,1950,1950,1961,1962,1926,1927,1922,1948,1945,1925,1910,1930,1956,1949,1950,1938,1931,1926,1901,1914,1958,1920,1916,1941,1910,1930,1910,1964,1961,1966,1966,1966,1968,1971,1968,1966,1968,1958,1920,1951,1920,1957,1994,1994,1972,1972,1972,1965,1925,1960,1990,1955,1954,1946,1931,1954,1910,1955,1955,1954,1926,1938,1980,1919,1941,1939,1922,1920,1939,1895,1930,1972,1970,1970,1970,1961,1982,1991,1970,1971,1971,1973,1973,1973,1972,1972,1972,1972,1965,1962,1963,1965,1968,1965,1964,1967,1966,1923,1908,1954,1961,1956,1947,1924,1930,1954,1900,1920,1915,1959,1950,1950,1949,1950,1960,1925,1920,1930,1930,1924,1925,1880,1905,1921,1955,1920,1900,1910,1923,1920,1958,1918,1941,1939,1934,1936,1930,1925,1930,1921,1970,1941,1924,1910,1919,1937,1922,1930,1936,1942,1952,1954,1948,1969,1972,1972,1910,1951,1975,1969,2002,1948,1924,1910,1955,1954,1922,1912,1910,1945,1941,1942,1925,1936,1920,1949,1916,2006,1977,1968,1970,1972,1970,1970,1970,1970,1970,1971,1968,1975,1978,1980,1984,1961,1965,1963,2005,1999,2003,1976,1966,1964,1963,1950,1955,1954,1960,1958,1957,1959,1959,1952,1954,1948,1959,1951,1962,1966,1957,1929,1915,1957,1939,1942,1921,1915,1900,1969,1950,1992,1900,1996,1977,2004,1954,1968,2003,1957,1948,1971,1962,1984,1991,1974,1978,1978,1975,1974,1970,1969,1971,1976,1976,1976,1974,1977,1978,1978,1966,2006,2004,2004,1981,1977,1999,1976,1968,1967,1967,1960,1961,1964,1966,1964,1953,1963,1958,1958,1955,1958,1954,1952,1948,1950,1916,1910,1963,1958,1957,1957,1960,1949,1950,1958,1959,1959,1949,1948,1964,1910,1939,1946,1900,1910,1923,1930,1937,1939,1936,1915,1927,1927,1963,1967,1978,1982,1982,1960,1968,1955,1946,1984,1983,1979,1979,1979,1979,1994,1994,1994,1978,1979,1977,1978,1977,1972,2003,2003,1977,2005,1932,1928,1916,1925,1936,1918,1960,1921,1949,1980,1977,1991,1958,1968,1968,1969,1972,1993,1992,1950,1969,1973,1969,1977,2003,2007,2005,2003,1993,1962,1961,1961,2007,2007,1999,1977,1968,1965,1968,1965,1965,1965,1973,1956,1963,1964,1961,1959,1959,1960,1960,1960,1950,1961,1955,1962,1956,1958,1956,1953,1953,1945,1922,1959,1948,1955,1963,1963,2008,1904,1940,1948,1946,1900,1920,1920,1928,1935,1937,1939,1931,1940,1918,1952,1938,1968,1959,1963,1978,1967,1978,1984,1966,1963,1958,1960,1941,1970,1953,1962,1994,1994,1978,1978,2003,2004,2004,2004,1965,1972,2008,2005,1946,1954,1925,1925,1926,1914,1931,1980,1989,2007,1968,1982,1982,1982,1940,1994,1992,1991,1969,1978,1969,1971,1970,1998,2001,1976,1978,2006,2006,2006,2006,2003,1977,1963,1962,1962,1962,1973,2004,1966,1965,1960,1958,1961,1964,1964,1959,1940,1955,1948,1942,1961,1956,1958,1954,1955,1953,1953,1953,1956,1953,1949,1953,1969,1949,1960,1959,1958,1960,1960,1961,1910,1980,1940,1900,1927,1910,1926,1948,1890,1925,1939,1941,1939,1930,1920,1929,1930,1937,1963,1964,1977,1920,1954,1958,1953,1954,1951,1994,1994,1974,1976,1977,1977,2003,1965,1976,2006,2005,2005,1923,1926,1921,1921,1910,1941,1940,1925,1950,1978,1983,1974,2007,1953,1996,1963,1976,1977,1977,1976,1983,1954,1987,1955,1975,1974,1969,1975,2005,2005,2005,2004,1991,1994,1977,1961,2005,2005,1972,1969,1967,1968,1968,1968,1965,1961,1958,1959,1958,1958,1934,1947,1952,1951,1951,1941,1963,1959,1957,1960,1956,1957,1979,1954,1954,1952,1951,1956,1956,1915,1900,1966,1962,1959,1957,1950,1961,1966,1962,1900,1900,1955,1924,1910,1910,1916,1939,1964,1950,1963,1967,1964,1966,1968,1978,1977,1965,1971,1974,1954,1952,1955,1954,1953,1968,1996,1996,1995,1977,1972,1993,2003,2004,2005,2004,2004,1916,1940,1938,1926,1914,1920,1939,1915,1953,1926,1978,1975,1973,1970,1969,1976,1960,1958,2001,1998,1990,1985,1988,1977,2006,2003,2005,2004,2000,1994,1992,1993,1974,1992,2004,2000,1978,1977,1978,1988,1972,1971,1964,1949,1966,1970,1959,1960,1959,1952,1963,1959,1978,1968,1985,2006,2009,1998,1977,2004,1919,1994,1989,1989,1977,1976,1976,2009,2006,2002,2004,1999,1969,1997,1996,1998,1998,1998,1993,1994,1992,1994,1992,1990,1984,1977,1979,1978,1978,1976,1975,1974,2005,2005,2005,2005,2005,2005,2005,2005,2004,2002,2004,2002,2004,2003,2007,2000,1999,2008,2009,2004,2003,2004,2004,2003,2001,1992,1994,2009,2008,1999,2000,1999,1976,1970,1968,1973,1969,1977,1998,1974,1971,1967,1956,1953,1961,1965,1968,1962,1969,1969,1882,1925,1910,1915,1987,1987,1963,1978,1961,1956,1993,2007,2007,2007,2005,1996,1998,1998,1999,2002,2001,2002,2001,2002,1996,1997,1996,2001,2000,2004,2005,2007,2003,1920,1928,1940,1942,1921,1940,1941,1960,1986,1977,1990,1988,1987,2003,1999,1978,1977,1958,1969,1995,1998,1996,1996,1993,1992,1997,1995,1994,1988,1989,1977,1980,1976,1976,1998,2004,2003,2005,2007,2004,2004,2005,2005,2002,2003,2004,2002,1999,2007,2003,2003,2004,2007,1995,1993,1994,1994,1992,1991,1974,2004,2006,2004,1999,1999,1999,1999,2002,1970,1974,1961,1968,1964,1966,1965,1964,1955,1965,1961,1962,1962,1959,1958,1958,1957,1964,2003,1890,1930,1925,1915,1925,1956,1954,1977,1959,1918,1966,2000,1999,1998,1999,1975,2003,1999,1997,1997,2003,2004,2007,2007,2005,2003,1976,2005,1921,1930,1922,1915,1958,1996,1977,1977,2006,2006,1986,1999,1997,1997,1964,1969,1970,1997,1997,1997,1999,1995,1999,1993,1984,1988,1990,1981,1984,1978,1979,1977,1976,1979,1971,1971,1968,1974,1997,2005,2005,2006,2003,2007,2005,2005,2006,2005,2007,2007,2007,2007,2007,2006,2006,2007,2004,2003,2000,1999,2000,2003,2000,1999,2006,1994,2000,1992,1975,2006,2006,2006,2006,2006,2006,1999,2000,2000,2000,1999,1978,1976,1976,1977,1969,1968,1972,1973,1965,1966,1963,1962,1910,1923,1918,1919,1963,1950,1940,1984,1988,2007,2005,2006,2007,2006,1988,1995,1997,1998,1997,2001,2003,2003,2003,2007,2004,2005,2006,2006,2005,2005,2008,2007,1914,1923,1937,1918,1941,1930,1960,1995,1988,1957,1979,1991,1959,1985,1990,1996,2003,1961,1977,1969,1996,1997,1993,1993,1998,1948,1984,1976,1975,1976,1975,1974,2005,2005,2006,2006,2006,2006,2004,2005,2005,2005,2002,2004,2004,2003,1998,2000,2006,2006,2005,2006,2006,2005,2005,2005,1997,1992,1990,2004,2000,1977,1978,1965,1961,1964,1958,1925,1936,1963,1961,1957,1900,1925,1924,1966,1967,1968,1978,1958,1955,1956,1963,1986,1989,2005,2006,2006,2005,2005,1969,1995,2002,2002,2001,1978,1996,2003,2004,2003,2006,2005,2004,2005,2005,2005,1919,1920,1932,2001,1990,2000,1998,1974,1993,1960,1968,2003,2010,2000,2009,2007,2009,2009,2005,2005,2005,2004,2009,2005,2002,2001,1999,1998,1994,1999,1995,2005,2009,2008,2005,2004,1993,2004,2010,2000,2002,1993,1967,1890,1978,2009,1993,2008,1990,2001,2003,1997,1998,2003,1941,1950,1959,1959,2004,1998,2008,2009,2002,2008,1994,1972,2003,2004,2000,1998,1995,2008,2008,2005,1980,1977,1980,2003,2006,2007,2008,2008,2007,2008,2008,2007,2006,2004,2006,2008,2004,2003,2009,2003,2003,2003,2006,2007,2004,2008,2008,2002,2006,2000,1999,1999,1995,2000,1998,1998,1996,1993,1995,1994,2008,2008,2008,2008,2007,2006,2009,2007,2006,2008,2006,2005,2006,2008,2006,2008,1996,2005,1991,2004,1977,1967,1977,1966,1880,1923,1916,1990,1993,1995,2000,2000,2008,2008,2007,2007,2006,2008,1989,1997,2002,2001,2002,2003,2000,1999,2002,1997,2007,2007,1954,1971,2007,2000,2004,1965,2007,2001,2006,2007,1970,2001,1997,2005,2004,2007,1988,1978,1996,2008,2007,2008,2007,2007,2006,2005,2005,2007,2004,2003,2003,2003,2003,2001,2003,2008,2008,2005,2007,2006,2006,2007,2000,1999,1997,1998,2000,1995,1995,1993,2007,2007,2006,2008,2007,2008,2007,2006,2005,2005,2007,2007,1990,1993,2000,2001,2001,1999,1999,2001,1998,1996,1967,1974,1976,1966,1893,1979,1954,1954,1908,1935,1990,1994,1993,1996,2008,1993,1995,1997,2002,2000,2001,2002,2002,2000,1998,2001,2001,2002,2002,2003,2002,2007,1959,1925,1935,1939,1950,1956,1954,2008,2007,2004,2004,1996,2002,1991,1986,1976,2006,2003,2005,1973,2000,2005,2005,2006,2007,2006,2006,2007,2007,2006,2005,2006,2007,2006,2006,2006,2005,2007,2007,2004,2007,2006,2004,2004,2003,2003,2005,2006,2005,2005,2006,2003,2000,1998,1998,1998,1994,1996,1996,1995,1995,1992,1992,1996,2007,2006,2007,2006,2007,2006,2007,2007,2007,2006,2005,2005,2004,2006,2006,2007,2007,2006,2004,2004,1998,1974,1992,1997,2003,1994,1980,1976,1981,1964,1937,1971,1948,1954,1984,1990,1991,1989,1994,2007,2006,2007,2006,2007,2007,2005,1988,1958,1992,1976,2000,2002,1998,2002,1999,2003,1927,1918,1934,1932,1932,1945,1953,1968,1986,2006,2002,2002,1987,1998,1958,2006,2006,2007,2006,2002,2002,2003,2006,2006,2005,1997,2005,2006,2005,2005,2005,2006,2006,2005,2005,2006,2005,2005,1988,1977,2006,2006,2006,2005,2004,2004,2005,2005,2005,2005,2005,2006,2003,2006,2005,2004,2003,2005,2004,2005,2006,2006,2004,2003,2002,2004,2000,2000,1999,1998,2000,1995,2000,1998,1995,1993,1994,1993,2005,2006,2005,2006,1992,1974,1991,1992,1993,1994,1980,1968,1892,1953,1948,1966,1928,1935,2005,2005,2006,2006,2005,2005,1992,1975,2000,1997,1999,1999,2002,2002,1999,2002,2001,1999,1995,1998,2006,2003,1929,1936,1929,1932,1956,1977,1989,1986,1958,2002,2005,2005,2005,1997],"z":[105500,88000,120000,125000,67500,112000,122000,127000,84900,128000,108538,108000,119000,109000,107500,97500,80400,96500,109500,107400,80000,119000,129000,100000,12789,105900,76500,87000,123900,109500,122000,110000,55000,107500,95000,93369,94000,125000,128000,129000,128000,114000,128200,100000,64000,125200,90000,80000,104000,128000,58500,126000,100000,125500,110000,68400,102776,55993,50138,119500,85000,76000,75500,82500,76500,120500,124500,97000,111000,125000,112000,97000,118000,119500,123000,128500,100000,120000,82000,76000,110000,122000,124100,129000,62383,117500,127500,110000,124500,122000,82500,110000,124000,110000,104900,120000,123000,126000,115000,113000,95000,59000,78500,113500,92900,90000,128000,127000,109500,115000,110000,128900,103500,66500,129000,107500,94550,124500,93000,129500,93000,45000,37900,99500,113000,87500,110000,106000,85000,124900,119000,34900,44000,121000,128000,117000,119000,100000,60000,105000,115000,104000,62500,123000,97500,70000,116000,61000,63000,113500,63900,82500,113000,125000,84000,108000,118964,118858,118858,113722,124000,123000,108000,100000,127000,118000,120000,98000,99900,82000,119900,103500,112000,110000,122000,117000,60000,115000,124000,123000,78000,85000,75000,57625,115000,80000,97000,112000,115000,111250,100000,85400,89500,98000,109008,72500,52000,118000,87000,119000,116000,110000,100000,118500,89900,114000,86900,116000,94000,127000,112500,127500,109500,73000,122600,111000,64000,119164,122250,120000,78000,112500,107900,65000,98000,114000,122000,110000,79500,109500,120000,105000,113000,115000,79000,123000,128000,68500,127000,127500,93850,120000,127500,127000,89500,125000,79900,85000,82375,127500,129500,120000,108959,95541,80000,108000,125600,119000,125000,110000,127000,124900,85000,123000,108000,82000,82500,96000,100000,67000,91000,115000,110000,104000,120000,55000,120500,120500,119000,126000,99500,13100,89000,81000,65000,100000,129500,105000,94900,98000,113000,128500,103000,119500,123000,110000,128000,120500,113500,113000,113700,106000,122500,128000,120750,127000,119000,120000,124000,106500,117600,101800,124000,39300,64500,112000,126000,114500,129000,120000,116000,129500,128600,125000,127000,112500,109000,103200,86000,128500,119200,102000,129000,99000,125500,103000,127500,89500,120000,110500,125000,114000,124500,104500,110000,128250,126000,127500,90000,117000,127000,99500,101000,109900,128500,122000,87000,50000,115400,118500,123000,129000,125000,108000,119900,115000,127000,129000,80000,86000,125000,96000,120000,126000,123600,112500,127000,117000,94750,122000,110000,118500,61500,111000,98500,79000,110000,112000,125000,79275,119000,96500,119750,123900,115000,87000,100000,75200,67000,68104,119600,124900,85500,106500,84500,129500,80000,124500,93500,116050,88000,97500,105500,125500,83000,116000,89000,108000,94500,119900,129500,125000,126000,116000,120875,124000,129200,124400,85500,127000,121000,98600,127000,127000,112000,99800,117000,86900,120000,105000,120000,109500,81400,87500,93500,119000,107000,111500,105000,110000,108000,111500,124000,96900,61500,126175,64000,79000,114504,125000,93500,125000,64500,100000,119900,114500,115000,75000,88000,110000,89000,129000,52500,107000,116500,72000,119500,91000,90000,80000,128000,123500,93000,91500,128500,128500,119900,98000,102000,127500,119900,117250,97900,107000,81000,115000,125000,83000,118500,121000,128000,112000,102000,72000,106500,108000,35311,78000,84500,127000,126500,73000,79400,92000,87550,90500,71000,149000,149900,142000,149900,146000,143000,152000,148000,138500,133000,152000,155000,147110,130500,159000,136300,137500,142125,153000,132000,154300,135000,145000,142000,159500,135000,144900,132000,154000,134800,160000,155000,143000,130000,139000,149700,134000,132000,140000,149500,140000,158000,144500,147000,138500,159000,150000,143000,155891,144000,140000,144500,160000,139000,157900,149900,136000,157000,154000,157000,158000,136000,148500,156000,143000,143750,146000,148500,147000,137900,147000,138000,148800,152000,159000,142000,160000,135000,156000,150000,154000,130000,143000,141500,153000,145100,154000,158000,149500,159000,139000,155000,153000,135000,131000,145500,152500,129900,135000,156500,154000,143500,135000,153000,157500,133000,138000,139000,138000,132500,133500,135000,144750,130000,150000,135000,160000,146500,149000,137000,134900,132500,141500,155000,149000,135000,139000,130000,149900,134900,137000,148000,143000,139600,154400,139500,156500,153337,147983,142953,148325,143000,145000,138000,140000,134900,150500,136500,143500,133000,144000,140000,159500,145500,159434,135000,147500,137450,155000,137000,155000,154900,158000,137000,156450,152000,137500,139500,130000,160000,131900,131250,149000,133000,146900,157000,151000,148500,155000,159895,146000,155000,153900,144000,135000,142000,146000,146000,160000,150000,150000,140500,141000,135000,145000,142600,135000,135750,155000,145000,140000,145500,142000,141000,153000,141500,141500,139950,135000,140000,139000,136000,133000,137500,130000,140000,139000,159000,138000,150000,140000,136870,143000,157500,155000,147000,140000,139500,132000,130000,138000,153000,157000,150000,129900,145000,130000,136000,141000,134000,134432,148000,157500,130000,153000,152000,149350,144900,140000,145000,152400,144000,155000,134500,155000,145000,158000,147000,136500,145000,149300,141000,147500,137500,160000,148000,130500,135000,140000,136500,145000,147500,149000,151000,150000,146000,152500,130500,138000,134500,160000,160000,160000,160000,155000,148000,152000,143250,133000,133000,155000,155000,142500,156932,147000,146000,147000,149000,144000,132500,132000,132500,143900,147000,148800,158500,156000,151500,145000,137500,158000,147000,150750,138500,144000,137000,146500,158500,131000,139000,139400,130000,142000,136000,134500,132000,153000,135000,156000,135000,143000,135000,152000,140000,152000,156500,141000,132000,140000,130000,137000,145000,133900,152000,153500,159500,136500,135000,131000,132000,131750,137500,140000,139400,141000,130000,158450,143500,158000,150000,137000,130000,154000,136500,139500,146000,131000,153500,160000,134900,131500,140000,148000,155000,157000,140000,150000,136500,146500,141500,139000,140000,141000,129850,152000,156000,134000,143000,147000,130000,140000,132250,129900,153500,157000,135000,139000,149900,130000,145000,153500,155000,138800,146300,139500,132000,142500,158000,151000,150000,151000,135000,147400,149900,146500,140000,135500,142000,157500,138500,146000,136500,129900,145000,140000,150000,129800,137000,146500,135000,137900,133000,135000,143000,132000,145250,148000,134450,135960,156000,140000,139500,155000,133500,159000,155500,160000,159950,147000,159500,129900,147500,146000,155000,134500,156500,139900,135500,139000,137000,155000,144000,140200,150000,131000,148000,153600,132000,142500,158000,149500,156500,145000,130000,142500,146000,135000,137000,136900,149900,158900,148000,145000,156000,157500,149900,137000,133000,155900,140000,130000,130000,160000,140000,137000,160000,138887,149000,131000,140000,157000,145400,145000,148000,151400,146500,140000,150900,131000,172000,213500,180400,171500,212000,164000,190000,205000,175500,199500,192000,180000,205000,189000,194500,169000,190000,190000,177500,206000,205000,212500,196500,197500,171000,180500,172500,178000,180000,165500,167500,162000,166000,192000,173000,160250,163000,178000,206000,198900,200500,161750,213000,169000,203135,185000,162500,190000,169900,170000,179781,174000,202900,173500,166500,161000,189500,189000,177500,185000,191000,185000,181316,178000,174000,173000,180500,187500,185000,181000,200000,184000,197500,213000,167900,213000,184500,167800,174000,174000,192500,180000,160200,188500,200000,170000,184100,195500,178000,207500,179000,168000,210000,208300,185000,209000,193000,203000,184900,189000,171500,184000,164500,195000,172500,180000,172500,180500,185000,185750,200000,163000,167900,213250,168500,172500,161500,177625,167000,187500,205000,193500,176000,190000,200000,168000,205000,167500,179900,179000,179000,175000,172500,165000,165000,213000,212000,200000,194500,173000,212000,207000,197900,192000,195000,187100,203000,201000,198500,203000,210000,191000,178000,213000,190000,173500,170000,198500,197000,177000,205000,177500,163500,205000,174500,193000,208000,173000,173000,175000,213500,170000,165000,165500,196000,176500,206900,165000,195500,173000,177900,180000,180000,181000,188000,194500,189000,188500,175000,185000,180000,205000,181000,207000,195000,191000,187687,179000,193000,190000,176000,188000,213000,184000,176000,185900,182000,188000,192500,173000,170000,182000,163000,193500,189000,175000,179200,192000,196000,171900,178000,179400,172900,170000,200000,205000,174000,185000,178400,176000,180500,174900,179900,168500,165500,201800,163000,174000,165000,185000,163000,166800,166000,167900,200000,184000,162900,184000,162500,161000,185000,165000,192000,177000,185000,195000,207500,193000,188000,191000,163000,183000,207000,208900,185000,208500,185900,179900,193000,189000,165400,160500,176000,168000,161900,191000,177000,165500,213000,171000,171500,172500,167500,180000,195000,209200,206300,173000,172000,167000,177000,169000,161500,180000,176000,204000,168000,207500,182000,212000,182900,178000,213000,192000,190000,195000,192350,170000,175000,175000,178900,177000,204000,200000,209500,200000,181755,176000,178900,168165,203000,179540,181134,166000,177594,173500,172500,194201,185485,183500,211000,181000,175000,170000,162000,187000,185000,210400,201000,179000,179200,171000,166000,193800,208900,207500,199900,164700,176500,168500,177000,177000,174000,194000,187000,181500,190000,162500,164000,185000,175500,161000,175000,172000,169500,184900,210000,200000,180500,202500,212300,165250,182000,211000,200141,203000,194000,195400,212999,207000,177500,175000,211000,204750,190550,180000,213000,185000,179400,179600,173500,167840,168675,167000,171500,183850,184750,200000,165000,175000,189950,164000,188000,201000,170000,200000,200624,183500,210000,170000,174000,187500,204000,187500,168000,188000,192100,202000,185000,183500,165600,176500,169000,202500,190000,179900,172000,184500,181000,185850,202665,179665,174190,172785,185101,178740,167240,213490,192140,183200,178000,201490,180000,199900,173000,199000,198600,200825,197000,185000,176000,184000,184900,187000,175900,172000,176400,166000,165000,170000,185900,161000,172000,162000,180000,170000,165000,183000,207500,169000,178000,202500,164500,165000,180000,160500,172500,170000,168000,164000,190000,209000,183600,194000,193879,209700,181000,180000,187000,200000,192500,205000,174000,210900,207000,195000,200000,203160,212900,196500,198000,163990,164990,167000,200100,199500,209000,180000,202000,164000,170000,188000,215000,244000,538000,394432,216000,376162,306000,395192,290941,220000,275000,259000,214000,224000,319900,216500,222500,333168,355000,325000,290000,410000,221500,215200,262500,271500,233000,362500,260000,267916,254000,230000,243500,242000,240000,230000,308030,236000,245350,320000,272000,237000,240000,224900,232000,257500,215000,335000,289000,246000,254900,220000,269500,214900,378500,285000,345000,270000,375000,278000,240000,239500,501837,372500,260000,260000,215000,249000,240900,337500,462000,485000,555000,256300,253293,335000,610000,255500,274900,300000,324000,350000,269500,233170,386250,445000,290000,255900,230000,552000,320000,248500,286500,254000,244000,214000,337000,403000,327000,340000,336000,265000,260000,260000,402000,244600,275000,257500,287090,275500,245000,252678,250000,238500,310000,270000,252000,241000,264500,291000,224243,220000,257000,235000,280750,318750,256900,230000,256000,265979,223500,214500,271900,239900,239500,269500,269500,295493,332000,272500,239000,220000,221800,275000,231500,227000,230000,250000,254750,313000,220000,279500,219500,293200,320000,224000,315000,230000,251000,240000,375000,315500,220000,315000,425000,237500,286000,247900,325000,387000,334000,226500,292500,306000,394617,426000,446261,317500,417500,390000,460000,379000,250000,316000,615000,412500,284000,284000,421250,370000,336860,367294,266000,370967,234250,219500,217300,226750,225000,284500,315000,250000,290000,350000,341000,235128,214000,232000,264132,245000,227680,297900,247000,226700,250000,339750,230348,290000,372500,300000,285000,290000,305000,328900,223000,345000,245500,215000,240000,299800,227000,325000,245000,241500,256000,240000,235000,262280,226500,225000,229000,475000,228000,225000,215700,221000,235000,223500,230000,237000,235000,222500,287000,274000,300000,255900,240000,214000,218836,284700,231000,381000,311500,263400,236500,224000,392500,294464,250000,275000,300000,271000,228500,241500,287000,294000,232500,370000,244000,251000,377426,349265,319900,392000,441929,455000,314813,318000,338931,450000,280000,479069,395000,380000,250000,418000,500067,260116,317000,325624,372000,342000,354000,350000,285000,245700,250000,280000,277500,221370,226500,248000,410000,322500,252000,755000,269790,315750,290000,290000,430000,263000,330000,260261,320000,282922,215000,419005,255000,235876,311872,248328,227000,260000,249000,307000,250000,221300,255000,227875,253000,265000,340000,215000,294000,285000,239000,301000,279700,255000,218000,373000,224500,219000,302000,216000,255000,228500,225000,238000,228500,220000,383970,424870,250000,360000,246500,276000,217000,277000,277000,240000,279000,217500,228500,267000,285000,246900,219500,238000,239000,359100,220000,315000,259500,217000,215000,315000,235000,215000,245000,244000,330000,257000,231713,267300,239000,248900,282000,270000,279900,283463,392000,281213,222000,320000,240000,239900,309000,457347,545224,356383,556581,401179,438780,318000,470000,229000,235000,412083,342643,293000,415000,326000,309000,300000,275500,338500,345000,344133,298236,360000,281000,372397,378000,374000,437154,279000,282500,332200,219210,258000,234000,246990,225000,230500,215000,230000,286000,300000,625000,405000,280000,350000,584500,321000,315000,252000,336820,423000,405749,248500,260000,260000,334000,310000,262000,214000,262500,475000,225000,225000,274970,235000,415000,229800,221500,239799,233555,260000,225000,220000,280000,235000,217000,214000,217000,255000,222000,214000,220000,275000,268500,237000,274300,233230,229000,230000,250000,218000,239000,230000,217500,335000,230000,250764,312500,320000,369900,359900,215000],"mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":5,"opacity":0.25,"line":{"color":"rgba(31,119,180,1)"}},"showlegend":false,"type":"scatter3d","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Sale_Price","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[334,384,434,484,534,584,634,684,734,784,834,884,934,984,1034,1084,1134,1184,1234,1284,1334,1384,1434,1484,1534,1584,1634,1684,1734,1784,1834,1884,1934,1984,2034,2084,2134,2184,2234,2284,2334,2384,2434,2484,2534,2584,2634,2684,2734,2784,2834,2884,2934,2984,3034,3084,3134,3184,3234,3284,3334,3384,3434,3484,3534,3584,3634,3684,3734,3784,3834,3884,3934,3984,4034,4084,4134,4184,4234,4284,4334,4384,4434,4484,4534,4584,4634,4684,4734,4784,4834,4884,4934,4984,5034,5084,5134,5184,5234,5284,5334,5384,5434,5484,5534,5584,5634],"y":[1872,1882,1892,1902,1912,1922,1932,1942,1952,1962,1972,1982,1992,2002],"z":[[-36978.325543317478,-32286.911792061524,-27595.498040805571,-22904.084289549617,-18212.670538293663,-13521.256787037943,-8829.8430357819889,-4138.4292845260352,552.98446672991849,5244.3982179858722,9935.8119692418259,14627.22572049778,19318.6394717535,24010.053223009454,28701.466974265408,33392.880725521361,38084.294476777315,42775.708228033269,47467.12197928899,52158.535730544943,56849.949481800897,61541.363233056851,66232.776984312804,70924.190735568758,75615.604486824479,80307.018238080433,84998.431989336386,89689.84574059234,94381.259491848294,99072.673243104247,103764.0869943602,108455.50074561592,113146.91449687188,117838.32824812783,122529.74199938378,127221.15575063974,131912.56950189569,136603.98325315164,141295.39700440736,145986.81075566332,150678.22450691927,155369.63825817523,160061.05200943118,164752.4657606869,169443.87951194285,174135.29326319881,178826.70701445476,183518.12076571072,188209.53451696667,192900.94826822262,197592.36201947834,202283.7757707343,206975.18952199025,211666.6032732462,216358.01702450216,221049.43077575811,225740.84452701407,230432.25827826979,235123.67202952574,239815.08578078169,244506.49953203765,249197.9132832936,253889.32703454932,258580.74078580551,263272.15453706123,267963.56828831718,272654.98203957314,277346.39579082909,282037.80954208504,286729.22329334076,291420.63704459672,296112.05079585267,300803.46454710863,305494.87829836458,310186.29204962053,314877.70580087649,319569.11955213221,324260.53330338816,328951.94705464412,333643.36080590007,338334.77455715602,343026.18830841174,347717.60205966793,352409.01581092365,357100.4295621796,361791.84331343556,366483.25706469151,371174.67081594747,375866.08456720319,380557.49831845914,385248.91206971509,389940.32582097105,394631.739572227,399323.15332348295,404014.56707473891,408705.98082599463,413397.39457725058,418088.80832850654,422780.22207976249,427471.63583101844,432163.04958227417,436854.46333353035,441545.87708478607,446237.29083604203,450928.70458729798,455620.11833855393,460311.53208980989],[-26109.80202381569,-21418.388272559736,-16726.974521303782,-12035.560770047829,-7344.1470187918749,-2652.733267536154,2038.6804837197997,6730.0942349757534,11421.507986231707,16112.921737487661,20804.335488743614,25495.749239999568,30187.162991255289,34878.576742511243,39569.990493767196,44261.40424502315,48952.817996279104,53644.231747535057,58335.645498790778,63027.059250046732,67718.473001302686,72409.886752558639,77101.300503814593,81792.714255070547,86484.128006326267,91175.541757582221,95866.955508838175,100558.36926009413,105249.78301135008,109941.19676260604,114632.61051386199,119324.02426511771,124015.43801637366,128706.85176762962,133398.26551888557,138089.67927014153,142781.09302139748,147472.50677265343,152163.92052390915,156855.33427516511,161546.74802642106,166238.16177767701,170929.57552893297,175620.98928018869,180312.40303144464,185003.8167827006,189695.23053395655,194386.6442852125,199078.05803646846,203769.47178772441,208460.88553898013,213152.29929023609,217843.71304149204,222535.12679274799,227226.54054400395,231917.9542952599,236609.36804651585,241300.78179777157,245992.19554902753,250683.60930028348,255375.02305153944,260066.43680279539,264757.85055405111,269449.2643053073,274140.67805656302,278832.09180781897,283523.50555907493,288214.91931033088,292906.33306158683,297597.74681284255,302289.16056409851,306980.57431535446,311671.98806661041,316363.40181786637,321054.81556912232,325746.22932037828,330437.643071634,335129.05682288995,339820.4705741459,344511.88432540186,349203.29807665781,353894.71182791353,358586.12557916972,363277.53933042544,367968.95308168139,372660.36683293735,377351.7805841933,382043.19433544925,386734.60808670497,391426.02183796093,396117.43558921688,400808.84934047284,405500.26309172879,410191.67684298474,414883.0905942407,419574.50434549642,424265.91809675237,428957.33184800833,433648.74559926428,438340.15935052023,443031.57310177595,447722.98685303214,452414.40060428786,457105.81435554381,461797.22810679977,466488.64185805572,471180.05560931168],[-15241.278504314134,-10549.86475305818,-5858.4510018022265,-1167.0372505462728,3524.3765007096808,8215.7902519654017,12907.204003221355,17598.617754477309,22290.031505733263,26981.445256989216,31672.85900824517,36364.272759501124,41055.686510756845,45747.100262012798,50438.514013268752,55129.927764524706,59821.341515780659,64512.755267036613,69204.169018292334,73895.582769548288,78586.996520804241,83278.410272060195,87969.824023316149,92661.237774572102,97352.651525827823,102044.06527708378,106735.47902833973,111426.89277959568,116118.30653085164,120809.72028210759,125501.13403336355,130192.54778461927,134883.96153587522,139575.37528713117,144266.78903838713,148958.20278964308,153649.61654089903,158341.03029215499,163032.44404341071,167723.85779466666,172415.27154592262,177106.68529717857,181798.09904843452,186489.51279969024,191180.9265509462,195872.34030220215,200563.75405345811,205255.16780471406,209946.58155597001,214637.99530722597,219329.40905848169,224020.82280973764,228712.2365609936,233403.65031224955,238095.0640635055,242786.47781476146,247477.89156601741,252169.30531727313,256860.71906852908,261552.13281978504,266243.54657104099,270934.96032229695,275626.37407355267,280317.78782480885,285009.20157606457,289700.61532732053,294392.02907857648,299083.44282983243,303774.85658108839,308466.27033234411,313157.68408360006,317849.09783485602,322540.51158611197,327231.92533736792,331923.33908862388,336614.75283987983,341306.16659113555,345997.58034239151,350688.99409364746,355380.40784490341,360071.82159615937,364763.23534741509,369454.64909867127,374146.062849927,378837.47660118295,383528.8903524389,388220.30410369486,392911.71785495081,397603.13160620653,402294.54535746248,406985.95910871844,411677.37285997439,416368.78661123035,421060.2003624863,425751.61411374225,430443.02786499797,435134.44161625393,439825.85536750988,444517.26911876583,449208.68287002179,453900.09662127751,458591.5103725337,463282.92412378942,467974.33787504537,472665.75162630132,477357.16537755728,482048.57912881323],[-4372.7549848123454,318.65876644360833,5010.072517699562,9701.4862689555157,14392.900020211469,19084.31377146719,23775.727522723144,28467.141273979098,33158.555025235051,37849.968776491005,42541.382527746959,47232.796279002912,51924.210030258633,56615.623781514587,61307.037532770541,65998.451284026494,70689.865035282448,75381.278786538402,80072.692537794122,84764.106289050076,89455.52004030603,94146.933791561984,98838.347542817937,103529.76129407389,108221.17504532961,112912.58879658557,117604.00254784152,122295.41629909747,126986.83005035343,131678.24380160938,136369.65755286533,141061.07130412105,145752.48505537701,150443.89880663296,155135.31255788892,159826.72630914487,164518.14006040082,169209.55381165678,173900.9675629125,178592.38131416845,183283.79506542441,187975.20881668036,192666.62256793631,197358.03631919203,202049.45007044799,206740.86382170394,211432.27757295989,216123.69132421585,220815.1050754718,225506.51882672776,230197.93257798348,234889.34632923943,239580.76008049538,244272.17383175134,248963.58758300729,253655.00133426324,258346.4150855192,263037.82883677492,267729.24258803087,272420.65633928683,277112.07009054278,281803.48384179873,286494.89759305445,291186.31134431064,295877.72509556636,300569.13884682232,305260.55259807827,309951.96634933422,314643.38010059018,319334.7938518459,324026.20760310185,328717.62135435781,333409.03510561376,338100.44885686971,342791.86260812567,347483.27635938162,352174.69011063734,356866.10386189329,361557.51761314925,366248.9313644052,370940.34511566116,375631.75886691688,380323.17261817306,385014.58636942878,389706.00012068474,394397.41387194069,399088.82762319664,403780.2413744526,408471.65512570832,413163.06887696427,417854.48262822023,422545.89637947618,427237.31013073213,431928.72388198809,436620.13763324404,441311.55138449976,446002.96513575572,450694.37888701167,455385.79263826762,460077.20638952358,464768.6201407793,469460.03389203548,474151.44764329121,478842.86139454716,483534.27514580311,488225.68889705907,492917.10264831502],[6495.7685346892104,11187.182285945164,15878.596037201118,20570.009788457071,25261.423539713025,29952.837290968746,34644.2510422247,39335.664793480653,44027.078544736607,48718.492295992561,53409.906047248514,58101.319798504468,62792.733549760189,67484.147301016143,72175.561052272096,76866.97480352805,81558.388554784004,86249.802306039957,90941.216057295678,95632.629808551632,100324.04355980759,105015.45731106354,109706.87106231949,114398.28481357545,119089.69856483117,123781.11231608712,128472.52606734307,133163.93981859903,137855.35356985498,142546.76732111094,147238.18107236689,151929.59482362261,156621.00857487856,161312.42232613452,166003.83607739047,170695.24982864643,175386.66357990238,180078.07733115833,184769.49108241405,189460.90483367001,194152.31858492596,198843.73233618191,203535.14608743787,208226.55983869359,212917.97358994954,217609.3873412055,222300.80109246145,226992.2148437174,231683.62859497336,236375.04234622931,241066.45609748503,245757.86984874099,250449.28359999694,255140.69735125289,259832.11110250885,264523.5248537648,269214.93860502075,273906.35235627647,278597.76610753243,283289.17985878838,287980.59361004434,292672.00736130029,297363.42111255601,302054.8348638122,306746.24861506792,311437.66236632387,316129.07611757983,320820.48986883578,325511.90362009173,330203.31737134745,334894.73112260341,339586.14487385936,344277.55862511531,348968.97237637127,353660.38612762722,358351.79987888318,363043.2136301389,367734.62738139485,372426.0411326508,377117.45488390676,381808.86863516271,386500.28238641843,391191.69613767462,395883.10988893034,400574.52364018629,405265.93739144225,409957.3511426982,414648.76489395415,419340.17864520987,424031.59239646583,428723.00614772178,433414.41989897774,438105.83365023369,442797.24740148964,447488.6611527456,452180.07490400132,456871.48865525727,461562.90240651323,466254.31615776918,470945.72990902513,475637.14366028085,480328.55741153704,485019.97116279276,489711.38491404871,494402.79866530467,499094.21241656062,503785.62616781658],[17364.292054190999,22055.705805446953,26747.119556702906,31438.53330795886,36129.947059214814,40821.360810470534,45512.774561726488,50204.188312982442,54895.602064238396,59587.015815494349,64278.429566750303,68969.843318006257,73661.257069261977,78352.670820517931,83044.084571773885,87735.498323029839,92426.912074285792,97118.325825541746,101809.73957679747,106501.15332805342,111192.56707930937,115883.98083056533,120575.39458182128,125266.80833307724,129958.22208433296,134649.63583558891,139341.04958684486,144032.46333810082,148723.87708935677,153415.29084061272,158106.70459186868,162798.1183431244,167489.53209438035,172180.94584563631,176872.35959689226,181563.77334814821,186255.18709940417,190946.60085066012,195638.01460191584,200329.4283531718,205020.84210442775,209712.2558556837,214403.66960693966,219095.08335819538,223786.49710945133,228477.91086070728,233169.32461196324,237860.73836321919,242552.15211447515,247243.5658657311,251934.97961698682,256626.39336824277,261317.80711949873,266009.22087075468,270700.63462201064,275392.04837326659,280083.46212452254,284774.87587577826,289466.28962703422,294157.70337829017,298849.11712954612,303540.53088080208,308231.9446320578,312923.35838331399,317614.77213456971,322306.18588582566,326997.59963708161,331689.01338833757,336380.42713959352,341071.84089084924,345763.2546421052,350454.66839336115,355146.0821446171,359837.49589587306,364528.90964712901,369220.32339838496,373911.73714964068,378603.15090089664,383294.56465215259,387985.97840340855,392677.3921546645,397368.80590592022,402060.21965717641,406751.63340843213,411443.04715968808,416134.46091094404,420825.87466219999,425517.28841345594,430208.70216471166,434900.11591596762,439591.52966722357,444282.94341847952,448974.35716973548,453665.77092099143,458357.18467224739,463048.59842350311,467740.01217475906,472431.42592601501,477122.83967727097,481814.25342852692,486505.66717978264,491197.08093103883,495888.49468229455,500579.9084335505,505271.32218480646,509962.73593606241,514654.14968731836],[28232.815573692787,32924.229324948741,37615.643076204695,42307.056827460648,46998.470578716602,51689.884329972323,56381.298081228277,61072.71183248423,65764.125583740184,70455.539334996138,75146.953086252091,79838.366837508045,84529.780588763766,89221.19434001972,93912.608091275673,98604.021842531627,103295.43559378758,107986.84934504353,112678.26309629926,117369.67684755521,122061.09059881116,126752.50435006712,131443.91810132307,136135.33185257902,140826.74560383474,145518.1593550907,150209.57310634665,154900.98685760261,159592.40060885856,164283.81436011451,168975.22811137047,173666.64186262619,178358.05561388214,183049.46936513809,187740.88311639405,192432.29686765,197123.71061890596,201815.12437016191,206506.53812141763,211197.95187267358,215889.36562392954,220580.77937518549,225272.19312644145,229963.60687769717,234655.02062895312,239346.43438020907,244037.84813146503,248729.26188272098,253420.67563397693,258112.08938523289,262803.50313648861,267494.91688774456,272186.33063900052,276877.74439025647,281569.15814151242,286260.57189276838,290951.98564402433,295643.39939528005,300334.81314653601,305026.22689779196,309717.64064904791,314409.05440030387,319100.46815155959,323791.88190281577,328483.29565407149,333174.70940532745,337866.1231565834,342557.53690783936,347248.95065909531,351940.36441035103,356631.77816160698,361323.19191286294,366014.60566411889,370706.01941537485,375397.4331666308,380088.84691788675,384780.26066914247,389471.67442039843,394163.08817165438,398854.50192291033,403545.91567416629,408237.32942542201,412928.7431766782,417620.15692793392,422311.57067918987,427002.98443044582,431694.39818170178,436385.81193295773,441077.22568421345,445768.63943546941,450460.05318672536,455151.46693798131,459842.88068923727,464534.29444049322,469225.70819174917,473917.1219430049,478608.53569426085,483299.9494455168,487991.36319677276,492682.77694802871,497374.19069928443,502065.60445054062,506757.01820179634,511448.43195305229,516139.84570430825,520831.2594555642,525522.67320682015],[39101.339093194343,43792.752844450297,48484.166595706251,53175.580346962204,57866.994098218158,62558.407849473879,67249.821600729832,71941.235351985786,76632.64910324174,81324.062854497693,86015.476605753647,90706.890357009601,95398.304108265322,100089.71785952128,104781.13161077723,109472.54536203318,114163.95911328914,118855.37286454509,123546.78661580081,128238.20036705676,132929.61411831272,137621.02786956867,142312.44162082463,147003.85537208058,151695.2691233363,156386.68287459225,161078.09662584821,165769.51037710416,170460.92412836011,175152.33787961607,179843.75163087202,184535.16538212774,189226.5791333837,193917.99288463965,198609.4066358956,203300.82038715156,207992.23413840751,212683.64788966347,217375.06164091919,222066.47539217514,226757.88914343109,231449.30289468705,236140.716645943,240832.13039719872,245523.54414845468,250214.95789971063,254906.37165096658,259597.78540222254,264289.19915347849,268980.61290473444,273672.02665599016,278363.44040724612,283054.85415850207,287746.26790975803,292437.68166101398,297129.09541226993,301820.50916352589,306511.92291478161,311203.33666603756,315894.75041729352,320586.16416854947,325277.57791980542,329968.99167106114,334660.40542231733,339351.81917357305,344043.232924829,348734.64667608496,353426.06042734091,358117.47417859687,362808.88792985259,367500.30168110854,372191.71543236449,376883.12918362045,381574.5429348764,386265.95668613235,390957.37043738831,395648.78418864403,400340.19793989998,405031.61169115594,409723.02544241189,414414.43919366784,419105.85294492356,423797.26669617975,428488.68044743547,433180.09419869143,437871.50794994738,442562.92170120333,447254.33545245929,451945.74920371501,456637.16295497096,461328.57670622692,466019.99045748287,470711.40420873882,475402.81795999478,480094.23171125073,484785.64546250645,489477.0592137624,494168.47296501836,498859.88671627431,503551.30046753027,508242.71421878599,512934.12797004217,517625.54172129789,522316.95547255385,527008.3692238098,531699.78297506575,536391.19672632171],[49969.862612695899,54661.276363951853,59352.690115207806,64044.10386646376,68735.517617719714,73426.931368975434,78118.345120231388,82809.758871487342,87501.172622743296,92192.586373999249,96884.000125255203,101575.41387651116,106266.82762776688,110958.24137902283,115649.65513027878,120341.06888153474,125032.48263279069,129723.89638404665,134415.31013530237,139106.72388655832,143798.13763781427,148489.55138907023,153180.96514032618,157872.37889158214,162563.79264283786,167255.20639409381,171946.62014534976,176638.03389660572,181329.44764786167,186020.86139911762,190712.27515037358,195403.6889016293,200095.10265288525,204786.51640414121,209477.93015539716,214169.34390665311,218860.75765790907,223552.17140916502,228243.58516042074,232934.9989116767,237626.41266293265,242317.8264141886,247009.24016544456,251700.65391670028,256392.06766795623,261083.48141921218,265774.89517046814,270466.30892172409,275157.72267298005,279849.136424236,284540.55017549172,289231.96392674767,293923.37767800363,298614.79142925958,303306.20518051554,307997.61893177149,312689.03268302744,317380.44643428316,322071.86018553912,326763.27393679507,331454.68768805102,336146.10143930698,340837.5151905627,345528.92894181889,350220.34269307461,354911.75644433056,359603.17019558651,364294.58394684247,368985.99769809842,373677.41144935414,378368.8252006101,383060.23895186605,387751.652703122,392443.06645437796,397134.48020563391,401825.89395688986,406517.30770814558,411208.72145940154,415900.13521065749,420591.54896191345,425282.9627131694,429974.37646442512,434665.79021568131,439357.20396693703,444048.61771819298,448740.03146944894,453431.44522070489,458122.85897196084,462814.27272321656,467505.68647447252,472197.10022572847,476888.51397698442,481579.92772824038,486271.34147949633,490962.75523075229,495654.16898200801,500345.58273326396,505036.99648451991,509728.41023577587,514419.82398703182,519111.23773828754,523802.65148954373,528494.06524079945,533185.4789920554,537876.89274331136,542568.30649456731,547259.72024582326],[60838.386132197455,65529.799883453408,70221.213634709362,74912.627385965316,79604.041137221269,84295.45488847699,88986.868639732944,93678.282390988898,98369.696142244851,103061.1098935008,107752.52364475676,112443.93739601271,117135.35114726843,121826.76489852439,126518.17864978034,131209.59240103629,135901.00615229225,140592.4199035482,145283.83365480392,149975.24740605988,154666.66115731583,159358.07490857178,164049.48865982774,168740.90241108369,173432.31616233941,178123.72991359537,182815.14366485132,187506.55741610727,192197.97116736323,196889.38491861918,201580.79866987513,206272.21242113085,210963.62617238681,215655.03992364276,220346.45367489872,225037.86742615467,229729.28117741062,234420.69492866658,239112.1086799223,243803.52243117825,248494.9361824342,253186.34993369016,257877.76368494611,262569.17743620183,267260.59118745779,271952.00493871374,276643.41868996969,281334.83244122565,286026.2461924816,290717.65994373756,295409.07369499328,300100.48744624923,304791.90119750518,309483.31494876114,314174.72870001709,318866.14245127304,323557.556202529,328248.96995378472,332940.38370504067,337631.79745629663,342323.21120755258,347014.62495880853,351706.03871006425,356397.45246132044,361088.86621257616,365780.27996383212,370471.69371508807,375163.10746634402,379854.52121759998,384545.9349688557,389237.34872011165,393928.76247136761,398620.17622262356,403311.58997387951,408003.00372513547,412694.41747639142,417385.83122764714,422077.24497890309,426768.65873015905,431460.072481415,436151.48623267096,440842.89998392668,445534.31373518286,450225.72748643858,454917.14123769454,459608.55498895049,464299.96874020644,468991.3824914624,473682.79624271812,478374.20999397407,483065.62374523003,487757.03749648598,492448.45124774193,497139.86499899789,501831.27875025384,506522.69250150956,511214.10625276552,515905.52000402147,520596.93375527742,525288.34750653338,529979.7612577891,534671.17500904528,539362.58876030101,544054.00251155696,548745.41626281291,553436.83001406887,558128.24376532482],[71706.90965169901,76398.323402954964,81089.737154210918,85781.150905466871,90472.564656722825,95163.978407978546,99855.3921592345,104546.80591049045,109238.21966174641,113929.63341300236,118621.04716425831,123312.46091551427,128003.87466676999,132695.28841802594,137386.7021692819,142078.11592053785,146769.5296717938,151460.94342304976,156152.35717430548,160843.77092556143,165535.18467681739,170226.59842807334,174918.01217932929,179609.42593058525,184300.83968184097,188992.25343309692,193683.66718435287,198375.08093560883,203066.49468686478,207757.90843812074,212449.32218937669,217140.73594063241,221832.14969188836,226523.56344314432,231214.97719440027,235906.39094565623,240597.80469691218,245289.21844816813,249980.63219942385,254672.04595067981,259363.45970193576,264054.87345319171,268746.28720444767,273437.70095570339,278129.11470695934,282820.5284582153,287511.94220947125,292203.3559607272,296894.76971198316,301586.18346323911,306277.59721449483,310969.01096575079,315660.42471700674,320351.83846826269,325043.25221951865,329734.6659707746,334426.07972203055,339117.49347328627,343808.90722454223,348500.32097579818,353191.73472705414,357883.14847831009,362574.56222956581,367265.975980822,371957.38973207772,376648.80348333367,381340.21723458963,386031.63098584558,390723.04473710153,395414.45848835725,400105.87223961321,404797.28599086916,409488.69974212511,414180.11349338107,418871.52724463702,423562.94099589298,428254.3547471487,432945.76849840465,437637.1822496606,442328.59600091656,447020.00975217251,451711.42350342823,456402.83725468442,461094.25100594014,465785.66475719609,470477.07850845205,475168.492259708,479859.90601096395,484551.31976221967,489242.73351347563,493934.14726473158,498625.56101598754,503316.97476724349,508008.38851849944,512699.8022697554,517391.21602101112,522082.62977226707,526774.04352352303,531465.45727477898,536156.87102603493,540848.28477729065,545539.69852854684,550231.11227980256,554922.52603105851,559613.93978231447,564305.35353357042,568996.76728482638],[82575.433171201032,87266.846922456985,91958.260673712939,96649.674424968893,101341.08817622485,106032.50192748057,110723.91567873652,115415.32942999247,120106.74318124843,124798.15693250438,129489.57068376034,134180.98443501629,138872.39818627201,143563.81193752796,148255.22568878392,152946.63944003987,157638.05319129582,162329.46694255178,167020.8806938075,171712.29444506345,176403.70819631941,181095.12194757536,185786.53569883131,190477.94945008727,195169.36320134299,199860.77695259894,204552.1907038549,209243.60445511085,213935.0182063668,218626.43195762276,223317.84570887871,228009.25946013443,232700.67321139039,237392.08696264634,242083.50071390229,246774.91446515825,251466.3282164142,256157.74196767015,260849.15571892587,265540.56947018183,270231.98322143778,274923.39697269374,279614.81072394969,284306.22447520541,288997.63822646136,293689.05197771732,298380.46572897327,303071.87948022923,307763.29323148518,312454.70698274113,317146.12073399685,321837.53448525281,326528.94823650876,331220.36198776471,335911.77573902067,340603.18949027662,345294.60324153258,349986.0169927883,354677.43074404425,359368.8444953002,364060.25824655616,368751.67199781211,373443.08574906783,378134.49950032402,382825.91325157974,387517.32700283569,392208.74075409165,396900.1545053476,401591.56825660355,406282.98200785927,410974.39575911523,415665.80951037118,420357.22326162714,425048.63701288309,429740.05076413904,434431.464515395,439122.87826665072,443814.29201790667,448505.70576916263,453197.11952041858,457888.53327167453,462579.94702293025,467271.36077418644,471962.77452544216,476654.18827669811,481345.60202795407,486037.01577921002,490728.42953046598,495419.8432817217,500111.25703297765,504802.6707842336,509494.08453548956,514185.49828674551,518876.91203800146,523568.32578925742,528259.73954051314,532951.15329176909,537642.56704302505,542333.980794281,547025.39454553695,551716.80829679267,556408.22204804886,561099.63579930458,565791.04955056054,570482.46330181649,575173.87705307244,579865.2908043284],[93443.956690702587,98135.370441958541,102826.78419321449,107518.19794447045,112209.6116957264,116901.02544698212,121592.43919823808,126283.85294949403,130975.26670074998,135666.68045200594,140358.09420326189,145049.50795451785,149740.92170577357,154432.33545702952,159123.74920828547,163815.16295954143,168506.57671079738,173197.99046205333,177889.40421330906,182580.81796456501,187272.23171582096,191963.64546707692,196655.05921833287,201346.47296958882,206037.88672084454,210729.3004721005,215420.71422335645,220112.12797461241,224803.54172586836,229494.95547712431,234186.36922838027,238877.78297963599,243569.19673089194,248260.61048214789,252952.02423340385,257643.4379846598,262334.85173591576,267026.26548717171,271717.67923842743,276409.09298968338,281100.50674093934,285791.92049219529,290483.33424345125,295174.74799470697,299866.16174596292,304557.57549721887,309248.98924847483,313940.40299973078,318631.81675098673,323323.23050224269,328014.64425349841,332706.05800475436,337397.47175601032,342088.88550726627,346780.29925852222,351471.71300977818,356163.12676103413,360854.54051228985,365545.95426354581,370237.36801480176,374928.78176605771,379620.19551731367,384311.60926856939,389003.02301982557,393694.43677108129,398385.85052233725,403077.2642735932,407768.67802484916,412460.09177610511,417151.50552736083,421842.91927861678,426534.33302987274,431225.74678112869,435917.16053238465,440608.5742836406,445299.98803489655,449991.40178615227,454682.81553740823,459374.22928866418,464065.64303992013,468757.05679117609,473448.47054243181,478139.884293688,482831.29804494372,487522.71179619967,492214.12554745562,496905.53929871158,501596.95304996753,506288.36680122325,510979.78055247921,515671.19430373516,520362.60805499111,525054.02180624707,529745.43555750302,534436.84930875897,539128.26306001469,543819.67681127065,548511.0905625266,553202.50431378256,557893.91806503851,562585.33181629423,567276.74556755042,571968.15931880614,576659.57307006209,581350.98682131805,586042.400572574,590733.81432382995],[104312.48021020414,109003.8939614601,113695.30771271605,118386.721463972,123078.13521522796,127769.54896648368,132460.96271773963,137152.37646899559,141843.79022025154,146535.20397150749,151226.61772276345,155918.0314740194,160609.44522527512,165300.85897653108,169992.27272778703,174683.68647904298,179375.10023029894,184066.51398155489,188757.92773281061,193449.34148406656,198140.75523532252,202832.16898657847,207523.58273783443,212214.99648909038,216906.4102403461,221597.82399160205,226289.23774285801,230980.65149411396,235672.06524536991,240363.47899662587,245054.89274788182,249746.30649913754,254437.7202503935,259129.13400164945,263820.5477529054,268511.96150416136,273203.37525541731,277894.78900667327,282586.20275792899,287277.61650918494,291969.03026044089,296660.44401169685,301351.8577629528,306043.27151420852,310734.68526546448,315426.09901672043,320117.51276797638,324808.92651923234,329500.34027048829,334191.75402174424,338883.16777299996,343574.58152425592,348265.99527551187,352957.40902676783,357648.82277802378,362340.23652927973,367031.65028053569,371723.06403179141,376414.47778304736,381105.89153430331,385797.30528555927,390488.71903681522,395180.13278807094,399871.54653932713,404562.96029058285,409254.3740418388,413945.78779309476,418637.20154435071,423328.61529560667,428020.02904686239,432711.44279811834,437402.85654937429,442094.27030063025,446785.6840518862,451477.09780314215,456168.51155439811,460859.92530565383,465551.33905690978,470242.75280816574,474934.16655942169,479625.58031067764,484316.99406193336,489008.40781318955,493699.82156444527,498391.23531570123,503082.64906695718,507774.06281821313,512465.47656946909,517156.89032072481,521848.30407198076,526539.71782323672,531231.13157449267,535922.54532574862,540613.95907700458,545305.37282826053,549996.78657951625,554688.2003307722,559379.61408202816,564071.02783328411,568762.44158454007,573453.85533579579,578145.26908705197,582836.68283830769,587528.09658956365,592219.5103408196,596910.92409207555,601602.33784333151]],"mode":"markers","marker":{"size":5,"opacity":0.25},"showlegend":false,"type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 6.1: Average home sales price as a function of year built and total square footage.
</p>
</div>
<div id="knowledge-check-10" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Knowledge check<a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>ames_train</code> data:
</p>
<ol style="list-style-type: decimal">
<li>
Fit a MLR model where <code>Sale_Price</code> is a function of
<code>Gr_Liv_Area</code> and <code>Garage_Cars</code>.
</li>
<li>
Interpret the coefficients. Are they both statistically different
from zero?
</li>
<li>
Compute and interpret the <strong><em>generalization</em></strong>
RMSE for this model.
</li>
<li>
How does this model compare to the model based on just
<code>Gr_Liv_Area</code>?
</li>
</ol>
</div>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/U5_shWsY7Hs?si=kLgT_xL8P9zOiwQr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
</div>
</div>
<div id="interactions" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Interactions<a href="lesson-2b-multiple-linear-regression.html#interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may notice that the fitted plane in the above image is flat; there is no curvature. This is true for all linear models that include only <em>main effects</em> (i.e., terms involving only a single predictor). One way to model curvature is to include <em>interaction effects</em>. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors.</p>
<p>Suppose that when people buy older homes they care more about the historical nature and beauty of the home rather than the total square footage. However, as older historical homes grow larger in size we see a compounding impact to the value of the home. This is known as a synergy effect – as one feature changes there is a larger or smaller effect of the other feature.</p>
<p>In linear regression, interactions can be captured via products of features (i.e., <span class="math inline">\(x_1 \times x_2\)</span>). A model with two main effects can also include a two-way interaction. For example, to include an interaction between <span class="math inline">\(x_1 =\)</span> <code>Gr_Liv_Area</code> and <span class="math inline">\(x_2 =\)</span> <code>Year_Built</code>, we introduce an additional product term:</p>
<p><span class="math display">\[\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1 x_2.
\end{equation}\]</span></p>
<p>Note that in R, we use the <code>:</code> operator to include an interaction (technically, we could use <code>*</code> as well, but <code>x1 * x2</code> is shorthand for <code>x1 + x2 + x1:x2</code> so is slightly redundant):</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="lesson-2b-multiple-linear-regression.html#cb51-1" tabindex="-1"></a>interaction_model <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb51-2"><a href="lesson-2b-multiple-linear-regression.html#cb51-2" tabindex="-1"></a>   <span class="fu">fit</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> Year_Built <span class="sc">+</span> Gr_Liv_Area<span class="sc">:</span>Year_Built, <span class="at">data =</span> ames_train)</span>
<span id="cb51-3"><a href="lesson-2b-multiple-linear-regression.html#cb51-3" tabindex="-1"></a></span>
<span id="cb51-4"><a href="lesson-2b-multiple-linear-regression.html#cb51-4" tabindex="-1"></a><span class="fu">tidy</span>(interaction_model)</span>
<span id="cb51-5"><a href="lesson-2b-multiple-linear-regression.html#cb51-5" tabindex="-1"></a><span class="do">## # A tibble: 4 × 5</span></span>
<span id="cb51-6"><a href="lesson-2b-multiple-linear-regression.html#cb51-6" tabindex="-1"></a><span class="do">##   term                      estimate   std.error statistic  p.value</span></span>
<span id="cb51-7"><a href="lesson-2b-multiple-linear-regression.html#cb51-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;                        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb51-8"><a href="lesson-2b-multiple-linear-regression.html#cb51-8" tabindex="-1"></a><span class="do">## 1 (Intercept)            -810498.    210537.         -3.85 1.22e- 4</span></span>
<span id="cb51-9"><a href="lesson-2b-multiple-linear-regression.html#cb51-9" tabindex="-1"></a><span class="do">## 2 Gr_Liv_Area               -729.       127.         -5.75 1.01e- 8</span></span>
<span id="cb51-10"><a href="lesson-2b-multiple-linear-regression.html#cb51-10" tabindex="-1"></a><span class="do">## 3 Year_Built                 431.       107.          4.03 5.83e- 5</span></span>
<span id="cb51-11"><a href="lesson-2b-multiple-linear-regression.html#cb51-11" tabindex="-1"></a><span class="do">## 4 Gr_Liv_Area:Year_Built       0.417      0.0642      6.49 1.04e-10</span></span></code></pre></div>
<p>In this example, we see that the two main effects (<code>Gr_Liv_Area</code> &amp; <code>Year_Built</code>) are statistically significant and so is the interaction term (<code>Gr_Liv_Area:Year_Built</code>). So how do we interpret these results?</p>
<p>Well, we can say that for every 1 additional square feet in <code>Gr_Liv_Area</code>, the <code>Sale_Price</code> of a home increases by <span class="math inline">\(b_1 + b_3 \times \text{Year_Built}\)</span> = -728.5084 + 0.4168489 x <code>Year_Built</code>. Likewise, for each additional year that a home was built, the <code>Sale_Price</code> of a home increases by <span class="math inline">\(b_2 + b_3 \times \text{Gr_Liv_Area}\)</span> = 430.8755 + 0.4168489 x <code>Gr_Liv_Area</code>.</p>
<p>Adding an interaction term now makes the change in one variable non-linear because it includes an additional change based on another feature. This non-linearity (or curvature) that interactions capture can be illustrated in the contour plot below. The left plot illustrates a regression model with main effects only. Note how the fitted regression surface is flat (i.e., it does not twist or bend). While the fitted regression surface with interaction is displayed in the right side plot and you can see the curvature of the relationship induced.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lm-mlr-fit"></span>
<img src="_main_files/figure-html/lm-mlr-fit-1.png" alt="In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane)." width="960" />
<p class="caption">
Figure 6.2: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The ‘best-fit’ plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane).
</p>
</div>
<div class="note">
<p>
Interaction effects are quite prevalent in predictive modeling. Since
linear models are an example of parametric modeling, it is up to the
analyst to decide if and when to include interaction effects. This
becomes quite tedious and unrealistic for larger data sets.
</p>
<p>
In later lessons, we’ll discuss algorithms that can automatically
detect and incorporate interaction effects (albeit in different ways).
For now, just realize that adding interactions is possible with MLR
models.
</p>
</div>
</div>
<div id="qualitative-predictors" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Qualitative predictors<a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/X2LVFJDNpak?si=8CiUPSrvmji2pJks" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
<p>In our discussion so far, we have assumed that all variables in our linear regression model are <em>quantitative</em>. But in practice, this is not necessarily the case; often some predictors are <em>qualitative</em>.</p>
<p>For example, the Credit data set provided by the <strong>ISLR</strong> package records the balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating).</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="lesson-2b-multiple-linear-regression.html#cb52-1" tabindex="-1"></a>credit <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ISLR<span class="sc">::</span>Credit)</span>
<span id="cb52-2"><a href="lesson-2b-multiple-linear-regression.html#cb52-2" tabindex="-1"></a>credit</span>
<span id="cb52-3"><a href="lesson-2b-multiple-linear-regression.html#cb52-3" tabindex="-1"></a><span class="do">## # A tibble: 400 × 12</span></span>
<span id="cb52-4"><a href="lesson-2b-multiple-linear-regression.html#cb52-4" tabindex="-1"></a><span class="do">##       ID Income Limit Rating Cards   Age Education Gender   Student</span></span>
<span id="cb52-5"><a href="lesson-2b-multiple-linear-regression.html#cb52-5" tabindex="-1"></a><span class="do">##    &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  </span></span>
<span id="cb52-6"><a href="lesson-2b-multiple-linear-regression.html#cb52-6" tabindex="-1"></a><span class="do">##  1     1   14.9  3606    283     2    34        11 &quot; Male&quot;  No     </span></span>
<span id="cb52-7"><a href="lesson-2b-multiple-linear-regression.html#cb52-7" tabindex="-1"></a><span class="do">##  2     2  106.   6645    483     3    82        15 &quot;Female&quot; Yes    </span></span>
<span id="cb52-8"><a href="lesson-2b-multiple-linear-regression.html#cb52-8" tabindex="-1"></a><span class="do">##  3     3  105.   7075    514     4    71        11 &quot; Male&quot;  No     </span></span>
<span id="cb52-9"><a href="lesson-2b-multiple-linear-regression.html#cb52-9" tabindex="-1"></a><span class="do">##  4     4  149.   9504    681     3    36        11 &quot;Female&quot; No     </span></span>
<span id="cb52-10"><a href="lesson-2b-multiple-linear-regression.html#cb52-10" tabindex="-1"></a><span class="do">##  5     5   55.9  4897    357     2    68        16 &quot; Male&quot;  No     </span></span>
<span id="cb52-11"><a href="lesson-2b-multiple-linear-regression.html#cb52-11" tabindex="-1"></a><span class="do">##  6     6   80.2  8047    569     4    77        10 &quot; Male&quot;  No     </span></span>
<span id="cb52-12"><a href="lesson-2b-multiple-linear-regression.html#cb52-12" tabindex="-1"></a><span class="do">##  7     7   21.0  3388    259     2    37        12 &quot;Female&quot; No     </span></span>
<span id="cb52-13"><a href="lesson-2b-multiple-linear-regression.html#cb52-13" tabindex="-1"></a><span class="do">##  8     8   71.4  7114    512     2    87         9 &quot; Male&quot;  No     </span></span>
<span id="cb52-14"><a href="lesson-2b-multiple-linear-regression.html#cb52-14" tabindex="-1"></a><span class="do">##  9     9   15.1  3300    266     5    66        13 &quot;Female&quot; No     </span></span>
<span id="cb52-15"><a href="lesson-2b-multiple-linear-regression.html#cb52-15" tabindex="-1"></a><span class="do">## 10    10   71.1  6819    491     3    41        19 &quot;Female&quot; Yes    </span></span>
<span id="cb52-16"><a href="lesson-2b-multiple-linear-regression.html#cb52-16" tabindex="-1"></a><span class="do">## # ℹ 390 more rows</span></span>
<span id="cb52-17"><a href="lesson-2b-multiple-linear-regression.html#cb52-17" tabindex="-1"></a><span class="do">## # ℹ 3 more variables: Married &lt;fct&gt;, Ethnicity &lt;fct&gt;, Balance &lt;int&gt;</span></span></code></pre></div>
<p>Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. For example, based on the gender, we can create a new variable that takes the form</p>
<p><span class="math display">\[
x_i = \Bigg\{ \genfrac{}{}{0pt}{}{1 \hspace{.5cm}\text{ if }i\text{th person is female}\hspace{.25cm}}{0 \hspace{.5cm}\text{ if }i\text{th person is male}}
\]</span></p>
<p>and use this variable as a predictor in the regression equation. This results in the model</p>
<p><span class="math display">\[
y_i = b_0 + b_1x_i = \Bigg\{ \genfrac{}{}{0pt}{}{b_0 + b_1 \hspace{.5cm}\text{ if }i\text{th person is female}\hspace{.3cm}}{b_0 \hspace{1.5cm}\text{ if }i\text{th person is male}}
\]</span>
Now <span class="math inline">\(b_0\)</span> can be interpreted as the average credit card balance among males, <span class="math inline">\(b_0 + b_1\)</span> as the average credit card balance among females, and <span class="math inline">\(b_1\)</span> as the average difference in credit card balance between females and males. We can produce this model in R using the same syntax as we saw earlier:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="lesson-2b-multiple-linear-regression.html#cb53-1" tabindex="-1"></a>qual_model <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb53-2"><a href="lesson-2b-multiple-linear-regression.html#cb53-2" tabindex="-1"></a>   <span class="fu">fit</span>(Balance <span class="sc">~</span> Gender, <span class="at">data =</span> credit)</span>
<span id="cb53-3"><a href="lesson-2b-multiple-linear-regression.html#cb53-3" tabindex="-1"></a></span>
<span id="cb53-4"><a href="lesson-2b-multiple-linear-regression.html#cb53-4" tabindex="-1"></a><span class="fu">tidy</span>(qual_model)</span>
<span id="cb53-5"><a href="lesson-2b-multiple-linear-regression.html#cb53-5" tabindex="-1"></a><span class="do">## # A tibble: 2 × 5</span></span>
<span id="cb53-6"><a href="lesson-2b-multiple-linear-regression.html#cb53-6" tabindex="-1"></a><span class="do">##   term         estimate std.error statistic  p.value</span></span>
<span id="cb53-7"><a href="lesson-2b-multiple-linear-regression.html#cb53-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb53-8"><a href="lesson-2b-multiple-linear-regression.html#cb53-8" tabindex="-1"></a><span class="do">## 1 (Intercept)     510.       33.1    15.4   2.91e-42</span></span>
<span id="cb53-9"><a href="lesson-2b-multiple-linear-regression.html#cb53-9" tabindex="-1"></a><span class="do">## 2 GenderFemale     19.7      46.1     0.429 6.69e- 1</span></span></code></pre></div>
<p>The results above suggest that males are estimated to carry $509.80 in credit card debt where females carry $509.80 + $19.73 = $529.53.</p>
<p>The decision to code males as 0 and females as 1 is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we want to change the reference variable (the variable coded as 0) we can change the factor levels.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="lesson-2b-multiple-linear-regression.html#cb54-1" tabindex="-1"></a>credit<span class="sc">$</span>Gender <span class="ot">&lt;-</span> <span class="fu">factor</span>(credit<span class="sc">$</span>Gender, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot; Male&quot;</span>))</span>
<span id="cb54-2"><a href="lesson-2b-multiple-linear-regression.html#cb54-2" tabindex="-1"></a></span>
<span id="cb54-3"><a href="lesson-2b-multiple-linear-regression.html#cb54-3" tabindex="-1"></a>qual_model <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb54-4"><a href="lesson-2b-multiple-linear-regression.html#cb54-4" tabindex="-1"></a>   <span class="fu">fit</span>(Balance <span class="sc">~</span> Gender, <span class="at">data =</span> credit)</span>
<span id="cb54-5"><a href="lesson-2b-multiple-linear-regression.html#cb54-5" tabindex="-1"></a></span>
<span id="cb54-6"><a href="lesson-2b-multiple-linear-regression.html#cb54-6" tabindex="-1"></a><span class="fu">tidy</span>(qual_model)</span>
<span id="cb54-7"><a href="lesson-2b-multiple-linear-regression.html#cb54-7" tabindex="-1"></a><span class="do">## # A tibble: 2 × 5</span></span>
<span id="cb54-8"><a href="lesson-2b-multiple-linear-regression.html#cb54-8" tabindex="-1"></a><span class="do">##   term        estimate std.error statistic  p.value</span></span>
<span id="cb54-9"><a href="lesson-2b-multiple-linear-regression.html#cb54-9" tabindex="-1"></a><span class="do">##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb54-10"><a href="lesson-2b-multiple-linear-regression.html#cb54-10" tabindex="-1"></a><span class="do">## 1 (Intercept)    530.       32.0    16.6   3.31e-47</span></span>
<span id="cb54-11"><a href="lesson-2b-multiple-linear-regression.html#cb54-11" tabindex="-1"></a><span class="do">## 2 Gender Male    -19.7      46.1    -0.429 6.69e- 1</span></span></code></pre></div>
<p>A similar process ensues for qualitative predictor categories with more than two levels. For instance, if we go back to our Ames housing data we’ll see that there is a <code>Neighborhood</code> variable. In our data there are 28 different neighborhoods.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="lesson-2b-multiple-linear-regression.html#cb55-1" tabindex="-1"></a>ames_train <span class="sc">%&gt;%</span></span>
<span id="cb55-2"><a href="lesson-2b-multiple-linear-regression.html#cb55-2" tabindex="-1"></a>   <span class="fu">count</span>(Neighborhood)</span>
<span id="cb55-3"><a href="lesson-2b-multiple-linear-regression.html#cb55-3" tabindex="-1"></a><span class="do">## # A tibble: 28 × 2</span></span>
<span id="cb55-4"><a href="lesson-2b-multiple-linear-regression.html#cb55-4" tabindex="-1"></a><span class="do">##    Neighborhood           n</span></span>
<span id="cb55-5"><a href="lesson-2b-multiple-linear-regression.html#cb55-5" tabindex="-1"></a><span class="do">##    &lt;fct&gt;              &lt;int&gt;</span></span>
<span id="cb55-6"><a href="lesson-2b-multiple-linear-regression.html#cb55-6" tabindex="-1"></a><span class="do">##  1 North_Ames           306</span></span>
<span id="cb55-7"><a href="lesson-2b-multiple-linear-regression.html#cb55-7" tabindex="-1"></a><span class="do">##  2 College_Creek        199</span></span>
<span id="cb55-8"><a href="lesson-2b-multiple-linear-regression.html#cb55-8" tabindex="-1"></a><span class="do">##  3 Old_Town             164</span></span>
<span id="cb55-9"><a href="lesson-2b-multiple-linear-regression.html#cb55-9" tabindex="-1"></a><span class="do">##  4 Edwards              131</span></span>
<span id="cb55-10"><a href="lesson-2b-multiple-linear-regression.html#cb55-10" tabindex="-1"></a><span class="do">##  5 Somerset             122</span></span>
<span id="cb55-11"><a href="lesson-2b-multiple-linear-regression.html#cb55-11" tabindex="-1"></a><span class="do">##  6 Northridge_Heights   116</span></span>
<span id="cb55-12"><a href="lesson-2b-multiple-linear-regression.html#cb55-12" tabindex="-1"></a><span class="do">##  7 Gilbert              111</span></span>
<span id="cb55-13"><a href="lesson-2b-multiple-linear-regression.html#cb55-13" tabindex="-1"></a><span class="do">##  8 Sawyer               108</span></span>
<span id="cb55-14"><a href="lesson-2b-multiple-linear-regression.html#cb55-14" tabindex="-1"></a><span class="do">##  9 Northwest_Ames        82</span></span>
<span id="cb55-15"><a href="lesson-2b-multiple-linear-regression.html#cb55-15" tabindex="-1"></a><span class="do">## 10 Sawyer_West           94</span></span>
<span id="cb55-16"><a href="lesson-2b-multiple-linear-regression.html#cb55-16" tabindex="-1"></a><span class="do">## # ℹ 18 more rows</span></span></code></pre></div>
<p>Most people are aware that different neighborhoods can generate significantly different home prices than other neighborhoods. In this data we can visualize this by looking at the distribution of <code>Sale_Price</code> across neighborhoods. We see that the Stone Brook neighborhood has the highest average sale price whereas Meadow Village has the lowest. This could be for many reasons (i.e. age of the neighborhood, amenities provided by the neighborhood, proximity to undesirable things such as manufacturing plants).</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="lesson-2b-multiple-linear-regression.html#cb56-1" tabindex="-1"></a><span class="fu">ggplot</span>(ames_train, <span class="fu">aes</span>(<span class="fu">fct_reorder</span>(Neighborhood, Sale_Price), Sale_Price)) <span class="sc">+</span></span>
<span id="cb56-2"><a href="lesson-2b-multiple-linear-regression.html#cb56-2" tabindex="-1"></a>   <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb56-3"><a href="lesson-2b-multiple-linear-regression.html#cb56-3" tabindex="-1"></a>   <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb56-4"><a href="lesson-2b-multiple-linear-regression.html#cb56-4" tabindex="-1"></a>   <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Sale Price&quot;</span>, <span class="at">labels =</span> scales<span class="sc">::</span>dollar) <span class="sc">+</span></span>
<span id="cb56-5"><a href="lesson-2b-multiple-linear-regression.html#cb56-5" tabindex="-1"></a>   <span class="fu">coord_flip</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-113-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>So, naturally, we can assume there is some relationship between <code>Neighborhood</code> and <code>Sale_Price</code>. We can assess this relationship by running the following model.</p>
<p>Based on the results we see that the reference Neighborhood (which is North Ames based on <code>levels(ames_train$Neighborhood)</code>) has an average <code>Sale_Price</code> of $143,516.75 (based on the intercept). Whereas College Creek has a <code>Sale_Price</code> of $143,516.75 + $57,006.75 = $200,523.50. The <code>p.value</code> for College Creek is very small suggesting that this difference between North Ames and College Creek is statistically significant.</p>
<p>However, look at the results for the Sawyer neighborhood. The coefficient suggests that the average <code>Sale_Price</code> for the Sawyer neighborhood is $143,516.75 - $4,591.68 = $148,108.40. However, the <code>p.value</code> is 0.43 which suggests that there is no statistical difference between the reference neighborhood (North Ames) and Sawyer.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="lesson-2b-multiple-linear-regression.html#cb57-1" tabindex="-1"></a>neighborhood_model <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb57-2"><a href="lesson-2b-multiple-linear-regression.html#cb57-2" tabindex="-1"></a>   <span class="fu">fit</span>(Sale_Price <span class="sc">~</span> Neighborhood, <span class="at">data =</span> ames_train)</span>
<span id="cb57-3"><a href="lesson-2b-multiple-linear-regression.html#cb57-3" tabindex="-1"></a></span>
<span id="cb57-4"><a href="lesson-2b-multiple-linear-regression.html#cb57-4" tabindex="-1"></a><span class="fu">tidy</span>(neighborhood_model)</span>
<span id="cb57-5"><a href="lesson-2b-multiple-linear-regression.html#cb57-5" tabindex="-1"></a><span class="do">## # A tibble: 28 × 5</span></span>
<span id="cb57-6"><a href="lesson-2b-multiple-linear-regression.html#cb57-6" tabindex="-1"></a><span class="do">##    term                          estimate std.error statistic   p.value</span></span>
<span id="cb57-7"><a href="lesson-2b-multiple-linear-regression.html#cb57-7" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb57-8"><a href="lesson-2b-multiple-linear-regression.html#cb57-8" tabindex="-1"></a><span class="do">##  1 (Intercept)                    143517.     2996.    47.9   0        </span></span>
<span id="cb57-9"><a href="lesson-2b-multiple-linear-regression.html#cb57-9" tabindex="-1"></a><span class="do">##  2 NeighborhoodCollege_Creek       57007.     4773.    11.9   8.04e- 32</span></span>
<span id="cb57-10"><a href="lesson-2b-multiple-linear-regression.html#cb57-10" tabindex="-1"></a><span class="do">##  3 NeighborhoodOld_Town           -19609.     5072.    -3.87  1.14e-  4</span></span>
<span id="cb57-11"><a href="lesson-2b-multiple-linear-regression.html#cb57-11" tabindex="-1"></a><span class="do">##  4 NeighborhoodEdwards            -10196.     5472.    -1.86  6.26e-  2</span></span>
<span id="cb57-12"><a href="lesson-2b-multiple-linear-regression.html#cb57-12" tabindex="-1"></a><span class="do">##  5 NeighborhoodSomerset            87794.     5612.    15.6   3.68e- 52</span></span>
<span id="cb57-13"><a href="lesson-2b-multiple-linear-regression.html#cb57-13" tabindex="-1"></a><span class="do">##  6 NeighborhoodNorthridge_Heigh…  177986.     5715.    31.1   2.85e-174</span></span>
<span id="cb57-14"><a href="lesson-2b-multiple-linear-regression.html#cb57-14" tabindex="-1"></a><span class="do">##  7 NeighborhoodGilbert             44653.     5807.     7.69  2.30e- 14</span></span>
<span id="cb57-15"><a href="lesson-2b-multiple-linear-regression.html#cb57-15" tabindex="-1"></a><span class="do">##  8 NeighborhoodSawyer              -4592.     5866.    -0.783 4.34e-  1</span></span>
<span id="cb57-16"><a href="lesson-2b-multiple-linear-regression.html#cb57-16" tabindex="-1"></a><span class="do">##  9 NeighborhoodNorthwest_Ames      44635.     6518.     6.85  9.87e- 12</span></span>
<span id="cb57-17"><a href="lesson-2b-multiple-linear-regression.html#cb57-17" tabindex="-1"></a><span class="do">## 10 NeighborhoodSawyer_West         40081.     6181.     6.48  1.11e- 10</span></span>
<span id="cb57-18"><a href="lesson-2b-multiple-linear-regression.html#cb57-18" tabindex="-1"></a><span class="do">## # ℹ 18 more rows</span></span></code></pre></div>
<div id="knowledge-check-11" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Knowledge check<a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
The Ames housing data has an <code>Overall_Qual</code> variable that
measures the overall quality of a home (Very Poor, Poor, …, Excellent,
Very Excellent).
</p>
<ol style="list-style-type: decimal">
<li>
Plot the relationship between <code>Sale_Price</code> and the
<code>Overall_Qual</code> variable. Does there look to be a relationship
between the quality of a home and its sale price?
</li>
<li>
Model this relationship with a simple linear regression model.
</li>
<li>
Interpret the coefficients.
</li>
</ol>
</div>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/9ZDG9qKFadY?si=dIQZjO7VfyjgFmBP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
</div>
</div>
<div id="including-many-predictors" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Including many predictors<a href="lesson-2b-multiple-linear-regression.html#including-many-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/tYyVsGDqeYM?si=JBha9YxjtFgXZ0jJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
<p>In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with <em>p</em> distinct predictors is</p>
<p><span class="math display">\[\begin{equation}
  \widehat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_p x_p,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i = 1, 2, \dots, p\)</span> are the predictors of interest. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS.</p>
<p>The code below creates a model where we use all features in our data set as main effects (i.e., no interaction terms) to predict <code>Sale_Price</code>.</p>
<div class="note">
<p>
However, note that we remove a few variables first. This is because
these variables introduce some new problems that require us to do some
feature engineering steps. We’ll discuss this in a future lesson but for
now we’ll just put these feature variables to the side.
</p>
</div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="lesson-2b-multiple-linear-regression.html#cb58-1" tabindex="-1"></a><span class="co"># remove some trouble variables</span></span>
<span id="cb58-2"><a href="lesson-2b-multiple-linear-regression.html#cb58-2" tabindex="-1"></a>trbl_vars <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MS_SubClass&quot;</span>, <span class="st">&quot;Condition_2&quot;</span>, <span class="st">&quot;Exterior_1st&quot;</span>, </span>
<span id="cb58-3"><a href="lesson-2b-multiple-linear-regression.html#cb58-3" tabindex="-1"></a>               <span class="st">&quot;Exterior_2nd&quot;</span>, <span class="st">&quot;Misc_Feature&quot;</span>)</span>
<span id="cb58-4"><a href="lesson-2b-multiple-linear-regression.html#cb58-4" tabindex="-1"></a>ames_train <span class="ot">&lt;-</span> ames_train <span class="sc">%&gt;%</span></span>
<span id="cb58-5"><a href="lesson-2b-multiple-linear-regression.html#cb58-5" tabindex="-1"></a>   <span class="fu">select</span>(<span class="sc">-</span>trbl_vars)</span>
<span id="cb58-6"><a href="lesson-2b-multiple-linear-regression.html#cb58-6" tabindex="-1"></a></span>
<span id="cb58-7"><a href="lesson-2b-multiple-linear-regression.html#cb58-7" tabindex="-1"></a><span class="co"># include all possible main effects</span></span>
<span id="cb58-8"><a href="lesson-2b-multiple-linear-regression.html#cb58-8" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb58-9"><a href="lesson-2b-multiple-linear-regression.html#cb58-9" tabindex="-1"></a>   <span class="fu">fit</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_train) </span>
<span id="cb58-10"><a href="lesson-2b-multiple-linear-regression.html#cb58-10" tabindex="-1"></a></span>
<span id="cb58-11"><a href="lesson-2b-multiple-linear-regression.html#cb58-11" tabindex="-1"></a><span class="co"># print estimated coefficients in a tidy data frame</span></span>
<span id="cb58-12"><a href="lesson-2b-multiple-linear-regression.html#cb58-12" tabindex="-1"></a><span class="fu">tidy</span>(model3)  </span>
<span id="cb58-13"><a href="lesson-2b-multiple-linear-regression.html#cb58-13" tabindex="-1"></a><span class="do">## # A tibble: 249 × 5</span></span>
<span id="cb58-14"><a href="lesson-2b-multiple-linear-regression.html#cb58-14" tabindex="-1"></a><span class="do">##    term                            estimate std.error statistic p.value</span></span>
<span id="cb58-15"><a href="lesson-2b-multiple-linear-regression.html#cb58-15" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span id="cb58-16"><a href="lesson-2b-multiple-linear-regression.html#cb58-16" tabindex="-1"></a><span class="do">##  1 (Intercept)                     -1.76e+7   1.22e+7   -1.44   1.49e-1</span></span>
<span id="cb58-17"><a href="lesson-2b-multiple-linear-regression.html#cb58-17" tabindex="-1"></a><span class="do">##  2 MS_ZoningResidential_High_Dens…  6.16e+3   8.84e+3    0.697  4.86e-1</span></span>
<span id="cb58-18"><a href="lesson-2b-multiple-linear-regression.html#cb58-18" tabindex="-1"></a><span class="do">##  3 MS_ZoningResidential_Low_Densi…  3.69e+2   5.64e+3    0.0655 9.48e-1</span></span>
<span id="cb58-19"><a href="lesson-2b-multiple-linear-regression.html#cb58-19" tabindex="-1"></a><span class="do">##  4 MS_ZoningResidential_Medium_De…  9.51e+2   6.40e+3    0.149  8.82e-1</span></span>
<span id="cb58-20"><a href="lesson-2b-multiple-linear-regression.html#cb58-20" tabindex="-1"></a><span class="do">##  5 MS_ZoningA_agr                  -5.26e+4   5.44e+4   -0.966  3.34e-1</span></span>
<span id="cb58-21"><a href="lesson-2b-multiple-linear-regression.html#cb58-21" tabindex="-1"></a><span class="do">##  6 MS_ZoningC_all                  -1.55e+4   1.01e+4   -1.53   1.25e-1</span></span>
<span id="cb58-22"><a href="lesson-2b-multiple-linear-regression.html#cb58-22" tabindex="-1"></a><span class="do">##  7 MS_ZoningI_all                  -1.98e+4   2.70e+4   -0.736  4.62e-1</span></span>
<span id="cb58-23"><a href="lesson-2b-multiple-linear-regression.html#cb58-23" tabindex="-1"></a><span class="do">##  8 Lot_Frontage                    -1.67e+1   2.07e+1   -0.807  4.20e-1</span></span>
<span id="cb58-24"><a href="lesson-2b-multiple-linear-regression.html#cb58-24" tabindex="-1"></a><span class="do">##  9 Lot_Area                         5.89e-1   1.08e-1    5.47   5.24e-8</span></span>
<span id="cb58-25"><a href="lesson-2b-multiple-linear-regression.html#cb58-25" tabindex="-1"></a><span class="do">## 10 StreetPave                       1.52e+3   1.05e+4    0.145  8.85e-1</span></span>
<span id="cb58-26"><a href="lesson-2b-multiple-linear-regression.html#cb58-26" tabindex="-1"></a><span class="do">## # ℹ 239 more rows</span></span></code></pre></div>
<p>You’ll notice that our model’s results includes the intercept plus 248 predictor variable coefficients. However, our <code>ames_train</code> data only includes 75 predictor variables after removing those 5 troublesome variables! What gives?</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="lesson-2b-multiple-linear-regression.html#cb59-1" tabindex="-1"></a>ames_train <span class="sc">%&gt;%</span></span>
<span id="cb59-2"><a href="lesson-2b-multiple-linear-regression.html#cb59-2" tabindex="-1"></a>   <span class="fu">select</span>(<span class="sc">-</span>Sale_Price) <span class="sc">%&gt;%</span></span>
<span id="cb59-3"><a href="lesson-2b-multiple-linear-regression.html#cb59-3" tabindex="-1"></a>   <span class="fu">dim</span>()</span>
<span id="cb59-4"><a href="lesson-2b-multiple-linear-regression.html#cb59-4" tabindex="-1"></a><span class="do">## [1] 2049   75</span></span></code></pre></div>
<p>The reason is that 41 of our predictor variables are qualitative and many of these include several levels. So the dummy encoding procedure discussed in the last section causes us to have many more coefficients than initial predictor variables.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="lesson-2b-multiple-linear-regression.html#cb60-1" tabindex="-1"></a>ames_train <span class="sc">%&gt;%</span></span>
<span id="cb60-2"><a href="lesson-2b-multiple-linear-regression.html#cb60-2" tabindex="-1"></a>   <span class="fu">select_if</span>(is.factor) <span class="sc">%&gt;%</span></span>
<span id="cb60-3"><a href="lesson-2b-multiple-linear-regression.html#cb60-3" tabindex="-1"></a>   <span class="fu">colnames</span>()</span>
<span id="cb60-4"><a href="lesson-2b-multiple-linear-regression.html#cb60-4" tabindex="-1"></a><span class="do">##  [1] &quot;MS_Zoning&quot;      &quot;Street&quot;         &quot;Alley&quot;         </span></span>
<span id="cb60-5"><a href="lesson-2b-multiple-linear-regression.html#cb60-5" tabindex="-1"></a><span class="do">##  [4] &quot;Lot_Shape&quot;      &quot;Land_Contour&quot;   &quot;Utilities&quot;     </span></span>
<span id="cb60-6"><a href="lesson-2b-multiple-linear-regression.html#cb60-6" tabindex="-1"></a><span class="do">##  [7] &quot;Lot_Config&quot;     &quot;Land_Slope&quot;     &quot;Neighborhood&quot;  </span></span>
<span id="cb60-7"><a href="lesson-2b-multiple-linear-regression.html#cb60-7" tabindex="-1"></a><span class="do">## [10] &quot;Condition_1&quot;    &quot;Bldg_Type&quot;      &quot;House_Style&quot;   </span></span>
<span id="cb60-8"><a href="lesson-2b-multiple-linear-regression.html#cb60-8" tabindex="-1"></a><span class="do">## [13] &quot;Overall_Qual&quot;   &quot;Overall_Cond&quot;   &quot;Roof_Style&quot;    </span></span>
<span id="cb60-9"><a href="lesson-2b-multiple-linear-regression.html#cb60-9" tabindex="-1"></a><span class="do">## [16] &quot;Roof_Matl&quot;      &quot;Mas_Vnr_Type&quot;   &quot;Exter_Qual&quot;    </span></span>
<span id="cb60-10"><a href="lesson-2b-multiple-linear-regression.html#cb60-10" tabindex="-1"></a><span class="do">## [19] &quot;Exter_Cond&quot;     &quot;Foundation&quot;     &quot;Bsmt_Qual&quot;     </span></span>
<span id="cb60-11"><a href="lesson-2b-multiple-linear-regression.html#cb60-11" tabindex="-1"></a><span class="do">## [22] &quot;Bsmt_Cond&quot;      &quot;Bsmt_Exposure&quot;  &quot;BsmtFin_Type_1&quot;</span></span>
<span id="cb60-12"><a href="lesson-2b-multiple-linear-regression.html#cb60-12" tabindex="-1"></a><span class="do">## [25] &quot;BsmtFin_Type_2&quot; &quot;Heating&quot;        &quot;Heating_QC&quot;    </span></span>
<span id="cb60-13"><a href="lesson-2b-multiple-linear-regression.html#cb60-13" tabindex="-1"></a><span class="do">## [28] &quot;Central_Air&quot;    &quot;Electrical&quot;     &quot;Kitchen_Qual&quot;  </span></span>
<span id="cb60-14"><a href="lesson-2b-multiple-linear-regression.html#cb60-14" tabindex="-1"></a><span class="do">## [31] &quot;Functional&quot;     &quot;Fireplace_Qu&quot;   &quot;Garage_Type&quot;   </span></span>
<span id="cb60-15"><a href="lesson-2b-multiple-linear-regression.html#cb60-15" tabindex="-1"></a><span class="do">## [34] &quot;Garage_Finish&quot;  &quot;Garage_Qual&quot;    &quot;Garage_Cond&quot;   </span></span>
<span id="cb60-16"><a href="lesson-2b-multiple-linear-regression.html#cb60-16" tabindex="-1"></a><span class="do">## [37] &quot;Paved_Drive&quot;    &quot;Pool_QC&quot;        &quot;Fence&quot;         </span></span>
<span id="cb60-17"><a href="lesson-2b-multiple-linear-regression.html#cb60-17" tabindex="-1"></a><span class="do">## [40] &quot;Sale_Type&quot;      &quot;Sale_Condition&quot;</span></span></code></pre></div>
<p>If we wanted to assess which features have a relationship we could easily filter our model results to find which coefficients have <code>p.values</code> less than 0.05. In this model we see that 67 (68 minus the intercept) features have a statistical relationship with <code>Sale_Price</code>.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="lesson-2b-multiple-linear-regression.html#cb61-1" tabindex="-1"></a><span class="fu">tidy</span>(model3) <span class="sc">%&gt;%</span></span>
<span id="cb61-2"><a href="lesson-2b-multiple-linear-regression.html#cb61-2" tabindex="-1"></a>   <span class="fu">filter</span>(p.value <span class="sc">&lt;</span> <span class="fl">0.05</span>)</span>
<span id="cb61-3"><a href="lesson-2b-multiple-linear-regression.html#cb61-3" tabindex="-1"></a><span class="do">## # A tibble: 68 × 5</span></span>
<span id="cb61-4"><a href="lesson-2b-multiple-linear-regression.html#cb61-4" tabindex="-1"></a><span class="do">##    term                            estimate std.error statistic p.value</span></span>
<span id="cb61-5"><a href="lesson-2b-multiple-linear-regression.html#cb61-5" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span id="cb61-6"><a href="lesson-2b-multiple-linear-regression.html#cb61-6" tabindex="-1"></a><span class="do">##  1 Lot_Area                         5.89e-1     0.108      5.47 5.24e-8</span></span>
<span id="cb61-7"><a href="lesson-2b-multiple-linear-regression.html#cb61-7" tabindex="-1"></a><span class="do">##  2 Lot_ShapeModerately_Irregular    1.03e+4  3759.         2.74 6.21e-3</span></span>
<span id="cb61-8"><a href="lesson-2b-multiple-linear-regression.html#cb61-8" tabindex="-1"></a><span class="do">##  3 Land_ContourHLS                  1.62e+4  4288.         3.78 1.61e-4</span></span>
<span id="cb61-9"><a href="lesson-2b-multiple-linear-regression.html#cb61-9" tabindex="-1"></a><span class="do">##  4 Land_ContourLvl                  1.09e+4  3040.         3.57 3.61e-4</span></span>
<span id="cb61-10"><a href="lesson-2b-multiple-linear-regression.html#cb61-10" tabindex="-1"></a><span class="do">##  5 Lot_ConfigCulDSac                6.08e+3  2930.         2.07 3.82e-2</span></span>
<span id="cb61-11"><a href="lesson-2b-multiple-linear-regression.html#cb61-11" tabindex="-1"></a><span class="do">##  6 Lot_ConfigFR2                   -8.00e+3  3563.        -2.25 2.49e-2</span></span>
<span id="cb61-12"><a href="lesson-2b-multiple-linear-regression.html#cb61-12" tabindex="-1"></a><span class="do">##  7 Land_SlopeSev                   -3.42e+4 11287.        -3.03 2.48e-3</span></span>
<span id="cb61-13"><a href="lesson-2b-multiple-linear-regression.html#cb61-13" tabindex="-1"></a><span class="do">##  8 NeighborhoodCollege_Creek        2.66e+4 10478.         2.54 1.13e-2</span></span>
<span id="cb61-14"><a href="lesson-2b-multiple-linear-regression.html#cb61-14" tabindex="-1"></a><span class="do">##  9 NeighborhoodSomerset             2.31e+4  6180.         3.73 1.95e-4</span></span>
<span id="cb61-15"><a href="lesson-2b-multiple-linear-regression.html#cb61-15" tabindex="-1"></a><span class="do">## 10 NeighborhoodNorthridge_Heights   3.19e+4  6159.         5.19 2.37e-7</span></span>
<span id="cb61-16"><a href="lesson-2b-multiple-linear-regression.html#cb61-16" tabindex="-1"></a><span class="do">## # ℹ 58 more rows</span></span></code></pre></div>
<p>How does our model with all available predictors perform? We can compute the generalization error to assess.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="lesson-2b-multiple-linear-regression.html#cb62-1" tabindex="-1"></a>model3 <span class="sc">%&gt;%</span></span>
<span id="cb62-2"><a href="lesson-2b-multiple-linear-regression.html#cb62-2" tabindex="-1"></a>   <span class="fu">predict</span>(ames_test) <span class="sc">%&gt;%</span></span>
<span id="cb62-3"><a href="lesson-2b-multiple-linear-regression.html#cb62-3" tabindex="-1"></a>   <span class="fu">bind_cols</span>(ames_test) <span class="sc">%&gt;%</span></span>
<span id="cb62-4"><a href="lesson-2b-multiple-linear-regression.html#cb62-4" tabindex="-1"></a>   <span class="fu">rmse</span>(<span class="at">truth =</span> Sale_Price, <span class="at">estimate =</span> .pred)</span>
<span id="cb62-5"><a href="lesson-2b-multiple-linear-regression.html#cb62-5" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb62-6"><a href="lesson-2b-multiple-linear-regression.html#cb62-6" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb62-7"><a href="lesson-2b-multiple-linear-regression.html#cb62-7" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb62-8"><a href="lesson-2b-multiple-linear-regression.html#cb62-8" tabindex="-1"></a><span class="do">## 1 rmse    standard      22959.</span></span></code></pre></div>
<p>Not too shabby! Using all the predictor variables in our model has drastically reduced our test RMSE!</p>
</div>
<div id="feature-importance" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Feature importance<a href="lesson-2b-multiple-linear-regression.html#feature-importance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jgEwaINwF7c?si=Nxw1ITQV5cM8p8oI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
<p>Ok, so we found a linear regression model that performs pretty good compared to the other linear regression models we trained. Our next goal is often to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the lesson, this constant rate of change is provided by the coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables?</p>
<p>Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the t-statistic for each model parameter used. Rather than search through each of the variables to compare their t-statistic values, we can use <code>vip::vip()</code> to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). The plot below illustrates the top 20 most influential variables. We see that the top 4 most important variables have to do with roofing material followed by the total square footage on the second floor.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="lesson-2b-multiple-linear-regression.html#cb63-1" tabindex="-1"></a><span class="co"># plot top 10 influential features</span></span>
<span id="cb63-2"><a href="lesson-2b-multiple-linear-regression.html#cb63-2" tabindex="-1"></a>model3 <span class="sc">%&gt;%</span></span>
<span id="cb63-3"><a href="lesson-2b-multiple-linear-regression.html#cb63-3" tabindex="-1"></a>   vip<span class="sc">::</span><span class="fu">vip</span>(<span class="at">num_features =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/lm-vip-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>This is basically saying that our model finds that these are the most influential features in our data set that have the largest impact on the predicted outcome. If we order our data based on the t-statistic we see similar results.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="lesson-2b-multiple-linear-regression.html#cb64-1" tabindex="-1"></a><span class="fu">tidy</span>(model3) <span class="sc">%&gt;%</span></span>
<span id="cb64-2"><a href="lesson-2b-multiple-linear-regression.html#cb64-2" tabindex="-1"></a>   <span class="fu">arrange</span>(<span class="fu">desc</span>(statistic))</span>
<span id="cb64-3"><a href="lesson-2b-multiple-linear-regression.html#cb64-3" tabindex="-1"></a><span class="do">## # A tibble: 249 × 5</span></span>
<span id="cb64-4"><a href="lesson-2b-multiple-linear-regression.html#cb64-4" tabindex="-1"></a><span class="do">##    term                    estimate std.error statistic  p.value</span></span>
<span id="cb64-5"><a href="lesson-2b-multiple-linear-regression.html#cb64-5" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb64-6"><a href="lesson-2b-multiple-linear-regression.html#cb64-6" tabindex="-1"></a><span class="do">##  1 Roof_MatlWdShngl        663237.   43673.       15.2  4.08e-49</span></span>
<span id="cb64-7"><a href="lesson-2b-multiple-linear-regression.html#cb64-7" tabindex="-1"></a><span class="do">##  2 Roof_MatlCompShg        591744.   41975.       14.1  6.74e-43</span></span>
<span id="cb64-8"><a href="lesson-2b-multiple-linear-regression.html#cb64-8" tabindex="-1"></a><span class="do">##  3 Roof_MatlWdShake        580841.   43901.       13.2  3.28e-38</span></span>
<span id="cb64-9"><a href="lesson-2b-multiple-linear-regression.html#cb64-9" tabindex="-1"></a><span class="do">##  4 Roof_MatlTar&amp;Grv        585872.   44342.       13.2  4.08e-38</span></span>
<span id="cb64-10"><a href="lesson-2b-multiple-linear-regression.html#cb64-10" tabindex="-1"></a><span class="do">##  5 Second_Flr_SF               60.4      4.80     12.6  6.62e-35</span></span>
<span id="cb64-11"><a href="lesson-2b-multiple-linear-regression.html#cb64-11" tabindex="-1"></a><span class="do">##  6 Roof_MatlRoll           603645.   48760.       12.4  7.56e-34</span></span>
<span id="cb64-12"><a href="lesson-2b-multiple-linear-regression.html#cb64-12" tabindex="-1"></a><span class="do">##  7 Roof_MatlMembran        642865.   52863.       12.2  9.20e-33</span></span>
<span id="cb64-13"><a href="lesson-2b-multiple-linear-regression.html#cb64-13" tabindex="-1"></a><span class="do">##  8 Roof_MatlMetal          636083.   52812.       12.0  3.42e-32</span></span>
<span id="cb64-14"><a href="lesson-2b-multiple-linear-regression.html#cb64-14" tabindex="-1"></a><span class="do">##  9 First_Flr_SF                43.0      4.16     10.3  2.63e-24</span></span>
<span id="cb64-15"><a href="lesson-2b-multiple-linear-regression.html#cb64-15" tabindex="-1"></a><span class="do">## 10 NeighborhoodGreen_Hills 159547.   19970.        7.99 2.39e-15</span></span>
<span id="cb64-16"><a href="lesson-2b-multiple-linear-regression.html#cb64-16" tabindex="-1"></a><span class="do">## # ℹ 239 more rows</span></span></code></pre></div>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Exercises<a href="lesson-2b-multiple-linear-regression.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Using the Boston housing data set where the response feature is the
median value of homes within a census tract (<code>cmedv</code>):
</p>
<ol style="list-style-type: decimal">
<li>
Split the data into 70-30 training-test sets.
</li>
<li>
Train an MLR model that includes all the predictor variables.
</li>
<li>
Assess and interpret the coefficients. Are all predictor variables
statistically significant? Explain why or why not.
</li>
<li>
What is the generalization error of this model?
</li>
<li>
Which features are most influential in this model and which features
are not?
</li>
</ol>
</div>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="lesson-2a-simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overview-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-2/lesson-2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
