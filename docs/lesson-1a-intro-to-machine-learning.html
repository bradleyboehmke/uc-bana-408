<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Lesson 1a: Intro to machine learning | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Lesson 1a: Intro to machine learning | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Lesson 1a: Intro to machine learning | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="overview.html"/>
<link rel="next" href="lesson-1b-first-model-with-tidymodels.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.10/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing</a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes</a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers</a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing</a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition</a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 5</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>15</b> Lesson 5b: Multivariate Adaptive Regression Splines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#learning-objectives-15"><i class="fa fa-check"></i><b>15.1</b> Learning objectives</a></li>
<li class="chapter" data-level="15.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#prerequisites-8"><i class="fa fa-check"></i><b>15.2</b> Prerequisites</a></li>
<li class="chapter" data-level="15.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#nonlinearity"><i class="fa fa-check"></i><b>15.3</b> Nonlinearity</a></li>
<li class="chapter" data-level="15.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>15.4</b> Multivariate adaptive regression splines</a></li>
<li class="chapter" data-level="15.5" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-mars-model"><i class="fa fa-check"></i><b>15.5</b> Fitting a MARS model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-basic-model"><i class="fa fa-check"></i><b>15.5.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="15.5.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model"><i class="fa fa-check"></i><b>15.5.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="15.5.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model-with-interactions"><i class="fa fa-check"></i><b>15.5.3</b> Fitting a full model with interactions</a></li>
<li class="chapter" data-level="15.5.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-25"><i class="fa fa-check"></i><b>15.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#tuning-1"><i class="fa fa-check"></i><b>15.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-26"><i class="fa fa-check"></i><b>15.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#feature-interpretation-1"><i class="fa fa-check"></i><b>15.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="15.8" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#final-thoughts-2"><i class="fa fa-check"></i><b>15.8</b> Final thoughts</a></li>
<li class="chapter" data-level="15.9" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#exercises-9"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Module 6</b></span></li>
<li class="chapter" data-level="16" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i><b>16</b> Overview</a>
<ul>
<li class="chapter" data-level="16.1" data-path="overview-5.html"><a href="overview-5.html#learning-objectives-16"><i class="fa fa-check"></i><b>16.1</b> Learning objectives</a></li>
<li class="chapter" data-level="16.2" data-path="overview-5.html"><a href="overview-5.html#estimated-time-requirement-5"><i class="fa fa-check"></i><b>16.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="16.3" data-path="overview-5.html"><a href="overview-5.html#tasks-5"><i class="fa fa-check"></i><b>16.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html"><i class="fa fa-check"></i><b>17</b> Lesson 6a: Decision Trees</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#learning-objectives-17"><i class="fa fa-check"></i><b>17.1</b> Learning objectives</a></li>
<li class="chapter" data-level="17.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#prerequisites-9"><i class="fa fa-check"></i><b>17.2</b> Prerequisites</a></li>
<li class="chapter" data-level="17.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#structure"><i class="fa fa-check"></i><b>17.3</b> Structure</a></li>
<li class="chapter" data-level="17.4" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#partitioning"><i class="fa fa-check"></i><b>17.4</b> Partitioning</a></li>
<li class="chapter" data-level="17.5" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#how-deep"><i class="fa fa-check"></i><b>17.5</b> How deep?</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#early-stopping"><i class="fa fa-check"></i><b>17.5.1</b> Early stopping</a></li>
<li class="chapter" data-level="17.5.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#pruning"><i class="fa fa-check"></i><b>17.5.2</b> Pruning</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-decision-tree"><i class="fa fa-check"></i><b>17.6</b> Fitting a decision tree</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1"><i class="fa fa-check"></i><b>17.6.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="17.6.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-full-model-1"><i class="fa fa-check"></i><b>17.6.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="17.6.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-27"><i class="fa fa-check"></i><b>17.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#tuning-2"><i class="fa fa-check"></i><b>17.7</b> Tuning</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-28"><i class="fa fa-check"></i><b>17.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#feature-interpretation-2"><i class="fa fa-check"></i><b>17.8</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-29"><i class="fa fa-check"></i><b>17.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#final-thoughts-3"><i class="fa fa-check"></i><b>17.9</b> Final thoughts</a></li>
<li class="chapter" data-level="17.10" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>17.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html"><i class="fa fa-check"></i><b>18</b> Lesson 6b: Bagging</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#learning-objectives-18"><i class="fa fa-check"></i><b>18.1</b> Learning objectives</a></li>
<li class="chapter" data-level="18.2" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#prerequisites-10"><i class="fa fa-check"></i><b>18.2</b> Prerequisites</a></li>
<li class="chapter" data-level="18.3" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#why-and-when-bagging-works"><i class="fa fa-check"></i><b>18.3</b> Why and when bagging works</a></li>
<li class="chapter" data-level="18.4" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#fitting-a-bagged-decision-tree-model"><i class="fa fa-check"></i><b>18.4</b> Fitting a bagged decision tree model</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-30"><i class="fa fa-check"></i><b>18.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#tuning-3"><i class="fa fa-check"></i><b>18.5</b> Tuning</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-31"><i class="fa fa-check"></i><b>18.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#feature-interpretation-3"><i class="fa fa-check"></i><b>18.6</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-32"><i class="fa fa-check"></i><b>18.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#final-thoughts-4"><i class="fa fa-check"></i><b>18.7</b> Final thoughts</a></li>
<li class="chapter" data-level="18.8" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#exercises-11"><i class="fa fa-check"></i><b>18.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html"><i class="fa fa-check"></i><b>19</b> Lesson 6c: Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#learning-objectives-19"><i class="fa fa-check"></i><b>19.1</b> Learning objectives</a></li>
<li class="chapter" data-level="19.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#prerequisites-11"><i class="fa fa-check"></i><b>19.2</b> Prerequisites</a></li>
<li class="chapter" data-level="19.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#extending-bagging"><i class="fa fa-check"></i><b>19.3</b> Extending bagging</a></li>
<li class="chapter" data-level="19.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#out-of-the-box-performance"><i class="fa fa-check"></i><b>19.4</b> Out-of-the-box performance</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-33"><i class="fa fa-check"></i><b>19.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#hyperparameters"><i class="fa fa-check"></i><b>19.5</b> Hyperparameters</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#number-of-trees"><i class="fa fa-check"></i><b>19.5.1</b> Number of trees</a></li>
<li class="chapter" data-level="19.5.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#mtry"><i class="fa fa-check"></i><b>19.5.2</b> <span class="math inline">\(m_{try}\)</span></a></li>
<li class="chapter" data-level="19.5.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tree-complexity"><i class="fa fa-check"></i><b>19.5.3</b> Tree complexity</a></li>
<li class="chapter" data-level="19.5.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#others"><i class="fa fa-check"></i><b>19.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tuning-4"><i class="fa fa-check"></i><b>19.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="19.6.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-34"><i class="fa fa-check"></i><b>19.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#feature-interpretation-4"><i class="fa fa-check"></i><b>19.7</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="19.7.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-35"><i class="fa fa-check"></i><b>19.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#final-thoughts-5"><i class="fa fa-check"></i><b>19.8</b> Final thoughts</a></li>
<li class="chapter" data-level="19.9" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#exercises-12"><i class="fa fa-check"></i><b>19.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VII Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-1a-intro-to-machine-learning" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Lesson 1a: Intro to machine learning<a href="lesson-1a-intro-to-machine-learning.html#lesson-1a-intro-to-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="video">
<p>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UUtNO3SqCgY?si=jHhRBFR7hkgE6kN7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>
</div>
<p>Machine learning (ML) continues to grow in importance for many organizations across nearly all domains. Some example applications of machine learning in practice include:</p>
<ul>
<li>Predicting the likelihood of a patient returning to the hospital (<em>readmission</em>) within 30 days of discharge.</li>
<li>Segmenting customers based on common attributes or purchasing behavior for targeted marketing.</li>
<li>Predicting coupon redemption rates for a given marketing campaign.</li>
<li>Predicting customer churn so an organization can perform preventative intervention.</li>
<li>And many more!</li>
</ul>
<p>In essence, these tasks all seek to learn from data. To address each scenario, we can use a given set of <em>features</em> to train an algorithm and extract insights. These algorithms, or <em>learners</em>, can be classified according to the amount and type of supervision needed during training.</p>
<div id="learning-objectives-2" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Learning objectives<a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This lesson will introduce you to some fundamental concepts around ML and this class. By the end of this lesson you will:</p>
<ol style="list-style-type: decimal">
<li>Be able to explain the difference between supervised and unsupervised learning.</li>
<li>Know when a problem is considered a regression or classification problem.</li>
<li>Be able to import and explore the data sets we’ll use through various examples.</li>
</ol>
</div>
<div id="supervised-learning" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Supervised learning<a href="lesson-1a-intro-to-machine-learning.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong><em>predictive model</em></strong> is used for tasks that involve the prediction of a given output (or target) using other variables (or features) in the data set. The learning algorithm in a predictive model attempts to discover and model the relationships among the <font color="red">target</font> variable (the variable being predicted) and the other <font color="blue">features</font> (aka predictor variables). Examples of predictive modeling include:</p>
<ul>
<li>using <font color="blue">customer attributes</font> to predict the probability of the <font color="red">customer churning</font> in the next 6 weeks;</li>
<li>using <font color="blue">home attributes</font> to predict the <font color="red">sales price</font>;</li>
<li>using <font color="blue">employee attributes</font> to predict the likelihood of <font color="red">attrition</font>;</li>
<li>using <font color="blue">patient attributes</font> and symptoms to predict the risk of <font color="red">readmission</font>;</li>
<li>using <font color="blue">production attributes</font> to predict <font color="red">time to market</font>.</li>
</ul>
<p>Each of these examples has a defined learning task; they each intend to use attributes (<span class="math inline">\(X\)</span>) to predict an outcome measurement (<span class="math inline">\(Y\)</span>).</p>
<div class="note">
<p>
Throughout this course we’ll use various terms interchangeably
for
</p>
<ul>
<li>
<span class="math inline"><span class="math inline">\(X\)</span></span>: “predictor variable”,
“independent variable”, “attribute”, “feature”, “predictor”
</li>
<li>
<span class="math inline"><span class="math inline">\(Y\)</span></span>: “target variable”,
“dependent variable”, “response”, “outcome measurement”
</li>
</ul>
</div>
<p>The predictive modeling examples above describe what is known as <em>supervised learning</em>. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible.</p>
<div class="note">
<p>
In supervised learning, the training data you feed the algorithm
includes the target values. Consequently, the solutions can be used to
help <em>supervise</em> the training process to find the optimal
algorithm parameters.
</p>
</div>
<p>Most supervised learning problems can be bucketed into one of two categories, <em>regression</em> or <em>classification</em>, which we discuss next.</p>
<div id="regression-problems" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Regression problems<a href="lesson-1a-intro-to-machine-learning.html#regression-problems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a <strong><em>regression problem</em></strong> (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuum. In the examples above, predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along some continuous spectrum (e.g., the predicted sales price of a particular home could be between $80,000 and $755,000). The figure below illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along a plane.</p>
<div class="figure"><span style="display:block;" id="fig:intro-regression-problem"></span>
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-fab8cfdd4ecf7975a150" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-fab8cfdd4ecf7975a150">{"x":{"visdat":{"1754a2e4494ae":["function () ","plotlyVisDat"]},"cur_data":"1754a2e4494ae","attrs":{"1754a2e4494ae":{"x":[334,882,988,1082,1177,1268,1358,1442,1520,1614,1696,1808,1982,2315,5642],"y":[1872,1920,1935,1949,1956,1962,1967,1973,1979,1993,1998,2003,2005,2006,2010],"z":[[148582.36874695469,153868.34756493304,162109.3379646251,162241.48962844998,162307.20990874688,162363.35465175548,162410.01093619951,162465.84218015702,162521.50389578493,162650.72770366183,162696.65918412534,162742.47586446803,162760.77052093894,162769.91100692356,154666.10606974064],[167295.48028963819,173265.34463343525,182551.06771713999,182705.3021378268,182782.00438330439,182847.53100371847,182901.98362878838,182967.14436395268,183032.10724232916,183182.92455760148,183236.53126277291,183290.00398453759,183311.3557077077,183322.02358369072,174196.40987526247],[169289.57290025402,175333.07273941665,184730.40967292918,184887.22499419178,184965.21074609182,185031.83386423707,185087.1976789803,185153.44878929012,185219.49873195135,185372.83976724729,185427.34350678776,185481.71102089368,185503.42003573955,185514.26642393239,176279.71807554457],[170844.52230874167,176945.62253214707,186430.0615912191,186588.94370802704,186667.95729702446,186735.45849495614,186791.55199404887,186858.67628115392,186925.59674925977,187080.9587896853,187136.18087795077,187191.26494535687,187213.26008081145,187224.2494222988,177904.74447035705],[172247.79221647728,178401.03643126209,187964.14218793617,188124.93814114921,188204.9034992076,188273.21779210924,188329.98697316545,188397.91981510923,188465.64638292397,188622.87985819005,188678.76713172,188734.51472184132,188756.77480272719,188767.89651783986,179371.71160478352],[173457.66728805081,179656.01427568114,189286.99969393737,189449.48922202422,189530.29681098755,189599.33062056647,189656.69772002005,189725.34606102906,189793.78595533769,189952.67548386703,190009.15138714988,190065.48613581606,190087.98066982671,190099.21952381564,180636.91303413527],[174543.13061576438,180782.07583911298,190474.01101740403,190638.05995125664,190719.6430475504,190789.33937195074,190847.2570212564,190916.56417775981,190985.66088710784,191146.07527234533,191203.09317270483,191259.96856379218,191282.67897698254,191294.02568982195,181772.38824531148],[175469.14059914718,181742.83547308599,191486.81031573989,191652.22412438289,191734.48598590138,191804.76217847841,191863.16169826989,191933.04548510237,192002.71707387493,192164.46609473173,192221.95837973352,192279.30696979671,192302.20633191659,192313.64744855338,182741.3872040158],[176262.27539834019,182565.83023519846,192354.41380314875,192521.02559625357,192603.88322684783,192674.66838410826,192733.49085357884,192803.88076315552,192874.05693785995,193036.9774015079,193094.88606567823,193152.64999422114,193175.71520150368,193187.23917869528,183571.6138258368],[177142.00603737234,183478.80044652396,193316.90813246221,193484.88450984604,193568.42076119094,193639.78566320403,193699.08990059511,193770.05631776596,193840.80724953278,194005.06206472503,194063.44501256559,194121.68203936471,194144.93615535932,194156.5545162546,184492.82160816668],[177848.46507936055,184212.05617144413,194089.97282615947,194259.07618286114,194343.1728917479,194415.01659185055,194474.7187104797,194546.16125224414,194617.38686287985,194782.74368885756,194841.51833686311,194900.1460848205,194923.55621625128,194935.25252651441,185232.88055681877],[178731.62775371104,185128.86953183578,195056.60938333056,195227.16700355214,195311.98693152968,195384.44847668428,195444.66402458461,195516.72096149935,195588.55910171021,195755.33797160521,195814.61807277362,195873.75001057546,195897.36146562747,195909.15836240564,186158.47285390858],[179940.47498051636,186384.10718110294,196380.16783930143,196552.81502827237,196638.67411867832,196712.02341883373,196772.97669118727,196845.91642608753,196918.63468371911,197087.45682740113,197147.46319249543,197207.31957901528,197231.22030732606,197243.1617325446,187426.32458000907],[181804.68125965152,188320.88592863991,198422.69481295883,198598.87376671145,198686.48923736025,198761.33901068635,198823.53917632499,198897.97100610644,198972.17682795969,199144.45248921693,199205.68637714657,199266.76721845847,199291.15687306455,199303.34257857106,189384.42349340711],[185980.18043733644,192700.13145909898,203054.29569756953,203250.73306955188,203348.42324441468,203431.87982337869,203501.23224496297,203584.22282200219,203666.96140297432,203859.04665171186,203927.32168550533,203995.42607419769,204022.62023985895,204036.20715199836,193885.96279580175]],"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Feature: square footage"},"yaxis":{"title":"Feature: year built"},"zaxis":{"title":"Response: sale price"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666668","rgba(70,19,97,1)"],["0.0833333333333336","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333334","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666666","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"x":[334,882,988,1082,1177,1268,1358,1442,1520,1614,1696,1808,1982,2315,5642],"y":[1872,1920,1935,1949,1956,1962,1967,1973,1979,1993,1998,2003,2005,2006,2010],"z":[[148582.36874695469,153868.34756493304,162109.3379646251,162241.48962844998,162307.20990874688,162363.35465175548,162410.01093619951,162465.84218015702,162521.50389578493,162650.72770366183,162696.65918412534,162742.47586446803,162760.77052093894,162769.91100692356,154666.10606974064],[167295.48028963819,173265.34463343525,182551.06771713999,182705.3021378268,182782.00438330439,182847.53100371847,182901.98362878838,182967.14436395268,183032.10724232916,183182.92455760148,183236.53126277291,183290.00398453759,183311.3557077077,183322.02358369072,174196.40987526247],[169289.57290025402,175333.07273941665,184730.40967292918,184887.22499419178,184965.21074609182,185031.83386423707,185087.1976789803,185153.44878929012,185219.49873195135,185372.83976724729,185427.34350678776,185481.71102089368,185503.42003573955,185514.26642393239,176279.71807554457],[170844.52230874167,176945.62253214707,186430.0615912191,186588.94370802704,186667.95729702446,186735.45849495614,186791.55199404887,186858.67628115392,186925.59674925977,187080.9587896853,187136.18087795077,187191.26494535687,187213.26008081145,187224.2494222988,177904.74447035705],[172247.79221647728,178401.03643126209,187964.14218793617,188124.93814114921,188204.9034992076,188273.21779210924,188329.98697316545,188397.91981510923,188465.64638292397,188622.87985819005,188678.76713172,188734.51472184132,188756.77480272719,188767.89651783986,179371.71160478352],[173457.66728805081,179656.01427568114,189286.99969393737,189449.48922202422,189530.29681098755,189599.33062056647,189656.69772002005,189725.34606102906,189793.78595533769,189952.67548386703,190009.15138714988,190065.48613581606,190087.98066982671,190099.21952381564,180636.91303413527],[174543.13061576438,180782.07583911298,190474.01101740403,190638.05995125664,190719.6430475504,190789.33937195074,190847.2570212564,190916.56417775981,190985.66088710784,191146.07527234533,191203.09317270483,191259.96856379218,191282.67897698254,191294.02568982195,181772.38824531148],[175469.14059914718,181742.83547308599,191486.81031573989,191652.22412438289,191734.48598590138,191804.76217847841,191863.16169826989,191933.04548510237,192002.71707387493,192164.46609473173,192221.95837973352,192279.30696979671,192302.20633191659,192313.64744855338,182741.3872040158],[176262.27539834019,182565.83023519846,192354.41380314875,192521.02559625357,192603.88322684783,192674.66838410826,192733.49085357884,192803.88076315552,192874.05693785995,193036.9774015079,193094.88606567823,193152.64999422114,193175.71520150368,193187.23917869528,183571.6138258368],[177142.00603737234,183478.80044652396,193316.90813246221,193484.88450984604,193568.42076119094,193639.78566320403,193699.08990059511,193770.05631776596,193840.80724953278,194005.06206472503,194063.44501256559,194121.68203936471,194144.93615535932,194156.5545162546,184492.82160816668],[177848.46507936055,184212.05617144413,194089.97282615947,194259.07618286114,194343.1728917479,194415.01659185055,194474.7187104797,194546.16125224414,194617.38686287985,194782.74368885756,194841.51833686311,194900.1460848205,194923.55621625128,194935.25252651441,185232.88055681877],[178731.62775371104,185128.86953183578,195056.60938333056,195227.16700355214,195311.98693152968,195384.44847668428,195444.66402458461,195516.72096149935,195588.55910171021,195755.33797160521,195814.61807277362,195873.75001057546,195897.36146562747,195909.15836240564,186158.47285390858],[179940.47498051636,186384.10718110294,196380.16783930143,196552.81502827237,196638.67411867832,196712.02341883373,196772.97669118727,196845.91642608753,196918.63468371911,197087.45682740113,197147.46319249543,197207.31957901528,197231.22030732606,197243.1617325446,187426.32458000907],[181804.68125965152,188320.88592863991,198422.69481295883,198598.87376671145,198686.48923736025,198761.33901068635,198823.53917632499,198897.97100610644,198972.17682795969,199144.45248921693,199205.68637714657,199266.76721845847,199291.15687306455,199303.34257857106,189384.42349340711],[185980.18043733644,192700.13145909898,203054.29569756953,203250.73306955188,203348.42324441468,203431.87982337869,203501.23224496297,203584.22282200219,203666.96140297432,203859.04665171186,203927.32168550533,203995.42607419769,204022.62023985895,204036.20715199836,193885.96279580175]],"type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 2.1: Average home sales price as a function of year built and total square footage.
</p>
</div>
</div>
<div id="classification-problems" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Classification problems<a href="lesson-1a-intro-to-machine-learning.html#classification-problems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the objective of our supervised learning is to predict a categorical outcome, we refer to this as a <strong><em>classification problem</em></strong>. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as:</p>
<ul>
<li>Did a customer redeem a coupon (coded as yes/no or 1/0)?</li>
<li>Did a customer churn (coded as yes/no or 1/0)?</li>
<li>Did a customer click on our online ad (coded as yes/no or 1/0)?</li>
<li>Classifying customer reviews:
<ul>
<li>Binary: positive vs. negative.</li>
<li>Multinomial: extremely negative to extremely positive on a 0–5 Likert scale.</li>
</ul></li>
</ul>
<div class="figure"><span style="display:block;" id="fig:classification-problem"></span>
<div class="grViz html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-b3726520e934de4675ce" style="width:50%;height:50%;"></div>
<script type="application/json" data-for="htmlwidget-b3726520e934de4675ce">{"x":{"diagram":"\n\n  digraph boxes_and_circles {\n    node [shape = circle]\n    x1; x2; x3;\n\n    node [shape = box]\n    Model;\n\n    node [shape = triangle]\n    Yes; No;\n\n    x1->Model; x2->Model; x3->Model; Model->No; Model->Yes;\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 2.2: Classification problem modeling ‘Yes’/‘No’ response based on three features.
</p>
</div>
<p>However, when we apply machine learning models for classification problems, rather than predict a particular class (i.e., “yes” or “no”), we often want to predict the <em>probability</em> of a particular class (i.e., yes: 0.65, no: 0.35). By default, the class with the highest predicted probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem.</p>
<p>Although there are machine learning algorithms that can be applied to regression problems but not classification and vice versa, many of the supervised learning algorithms we cover in this class can be applied to both. These algorithms have become the most popular machine learning applications in recent years.</p>
</div>
<div id="knowledge-check" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Knowledge check<a href="lesson-1a-intro-to-machine-learning.html#knowledge-check" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Identify the features, response variable, and the type of supervised
model required for the following tasks:
</p>
<ul>
<li>
There is an online retailer that wants to predict whether you will
click on a certain featured product given your demographics, the current
products in your online basket, and the time since your previous
purchase.
</li>
<li>
A bank wants to use a customers historical data such as the number
of loans they’ve had, the time it took to payoff those loans, previous
loan defaults, the number of new loans within the past two years, along
with the customers income and level of education to determine if they
should issue a new loan for a car.
</li>
<li>
If the bank above does issue a new loan, they want to use the same
information to determine the interest rate of the new loan issued.
</li>
<li>
To better plan incoming and outgoing flights, an airline wants to
use flight information such as scheduled flight time, day/month of year,
number of passengers, airport departing from, airport arriving to,
distance to travel, and weather warnings to determine if a flight will
be delayed.
</li>
<li>
What if the above airline wants to use the same information to
predict the number of minutes a flight will arrive late or early?
</li>
</ul>
</div>
</div>
</div>
<div id="unsupervised-learning" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Unsupervised learning<a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong><em>Unsupervised learning</em></strong>, in contrast to supervised learning, includes a set of statistical tools to better understand and describe your data, but performs the analysis without a target variable. In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., <em>clustering</em>) or the columns (i.e., <em>dimension reduction</em>); however, the motive in each case is quite different.</p>
<p>The goal of <strong><em>clustering</em></strong> is to segment observations into similar groups based on the observed variables; for example, to divide consumers into different homogeneous groups, a process known as market segmentation. In <strong>dimension reduction</strong>, we are often concerned with reducing the number of variables in a data set. For example, classical linear regression models break down in the presence of highly correlated features. Some dimension reduction techniques can be used to reduce the feature set to a potentially smaller set of uncorrelated variables. Such a reduced feature set is often used as input to downstream supervised learning models (e.g., principal component regression).</p>
<p>Unsupervised learning is often performed as part of an exploratory data analysis (EDA). However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e., linear regression), then it is possible to check our work by seeing how well our model predicts the response <em>Y</em> on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised!</p>
<p>Despite its subjectivity, the importance of unsupervised learning should not be overlooked and such techniques are often used in organizations to:</p>
<ul>
<li>Divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment.</li>
<li>Identify groups of online shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers.</li>
<li>Identify products that have similar purchasing behavior so that managers can manage them as product groups.</li>
</ul>
<p>These questions, and many more, can be addressed with unsupervised learning. Moreover, the outputs of unsupervised learning models can be used as inputs to downstream supervised learning models.</p>
<div id="knowledge-check-1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Knowledge check<a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Identify the type of unsupervised model required for the following
tasks:
</p>
<ul>
<li>
Say you have a YouTube channel. You may have a lot of data about the
subscribers of your channel. What if you want to use that data to detect
groups of similar subscribers?
</li>
<li>
Say you’d like to group Ohio counties together based on the
demographics of their residents.
</li>
<li>
A retailer has collected hundreds of attributes about all their
customers; however, many of those features are highly correlated. They’d
like to reduce the number of features down by combining all those highly
correlated features into groups.
</li>
</ul>
</div>
</div>
</div>
<div id="machine-learning-in" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Historically, the R ecosystem provides a wide variety of ML algorithm implementations. This has its benefits; however, this also has drawbacks as it requires the users to learn many different formula interfaces and syntax nuances.</p>
<p>More recently, development on a group of packages called <a href="https://www.tidymodels.org/"><strong>Tidymodels</strong></a> has helped to make implementation easier. The <strong>tidymodels</strong> collection allows you to perform discrete parts of the ML workflow with discrete packages:</p>
<ul>
<li><a href="https://rsample.tidymodels.org/">rsample</a> for data splitting and resampling</li>
<li><a href="https://recipes.tidymodels.org/">recipes</a> for data pre-processing and feature engineering</li>
<li><a href="https://parsnip.tidymodels.org/">parsnip</a> for applying algorithms</li>
<li><a href="https://tune.tidymodels.org/">tune</a> for hyperparameter tuning</li>
<li><a href="https://yardstick.tidymodels.org/">yardstick</a> for measuring model performance</li>
<li>and several others!</li>
</ul>
<p>Throughout this course you’ll be exposed to several of these packages. Go ahead and make sure you have the following packages installed.</p>
<div class="note">
<p>
Just like the <strong>tidyverse</strong> package, when you install
<strong>tidymodels</strong> you are actually installing several packages
that exist in the <strong>tidymodels</strong> ecosystem as discussed
above.
</p>
</div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="lesson-1a-intro-to-machine-learning.html#cb2-1" tabindex="-1"></a><span class="co"># common data wrangling and visualization</span></span>
<span id="cb2-2"><a href="lesson-1a-intro-to-machine-learning.html#cb2-2" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidyverse&quot;</span>)</span>
<span id="cb2-3"><a href="lesson-1a-intro-to-machine-learning.html#cb2-3" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;vip&quot;</span>)</span>
<span id="cb2-4"><a href="lesson-1a-intro-to-machine-learning.html#cb2-4" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;here&quot;</span>)</span>
<span id="cb2-5"><a href="lesson-1a-intro-to-machine-learning.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="lesson-1a-intro-to-machine-learning.html#cb2-6" tabindex="-1"></a><span class="co"># modeling</span></span>
<span id="cb2-7"><a href="lesson-1a-intro-to-machine-learning.html#cb2-7" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidymodels&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="lesson-1a-intro-to-machine-learning.html#cb3-1" tabindex="-1"></a><span class="fu">packageVersion</span>(<span class="st">&quot;tidymodels&quot;</span>)</span>
<span id="cb3-2"><a href="lesson-1a-intro-to-machine-learning.html#cb3-2" tabindex="-1"></a><span class="do">## [1] &#39;1.1.1&#39;</span></span>
<span id="cb3-3"><a href="lesson-1a-intro-to-machine-learning.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="lesson-1a-intro-to-machine-learning.html#cb3-4" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb3-5"><a href="lesson-1a-intro-to-machine-learning.html#cb3-5" tabindex="-1"></a><span class="do">## ── Attaching packages ── tidymodels 1.1.1 ──</span></span>
<span id="cb3-6"><a href="lesson-1a-intro-to-machine-learning.html#cb3-6" tabindex="-1"></a><span class="do">## ✔ broom        1.0.5     ✔ rsample      1.2.0</span></span>
<span id="cb3-7"><a href="lesson-1a-intro-to-machine-learning.html#cb3-7" tabindex="-1"></a><span class="do">## ✔ dials        1.2.0     ✔ tibble       3.2.1</span></span>
<span id="cb3-8"><a href="lesson-1a-intro-to-machine-learning.html#cb3-8" tabindex="-1"></a><span class="do">## ✔ dplyr        1.1.2     ✔ tidyr        1.3.0</span></span>
<span id="cb3-9"><a href="lesson-1a-intro-to-machine-learning.html#cb3-9" tabindex="-1"></a><span class="do">## ✔ infer        1.0.4     ✔ tune         1.1.2</span></span>
<span id="cb3-10"><a href="lesson-1a-intro-to-machine-learning.html#cb3-10" tabindex="-1"></a><span class="do">## ✔ modeldata    1.2.0     ✔ workflows    1.1.3</span></span>
<span id="cb3-11"><a href="lesson-1a-intro-to-machine-learning.html#cb3-11" tabindex="-1"></a><span class="do">## ✔ parsnip      1.1.1     ✔ workflowsets 1.0.1</span></span>
<span id="cb3-12"><a href="lesson-1a-intro-to-machine-learning.html#cb3-12" tabindex="-1"></a><span class="do">## ✔ purrr        1.0.2     ✔ yardstick    1.2.0</span></span>
<span id="cb3-13"><a href="lesson-1a-intro-to-machine-learning.html#cb3-13" tabindex="-1"></a><span class="do">## ✔ recipes      1.0.9</span></span>
<span id="cb3-14"><a href="lesson-1a-intro-to-machine-learning.html#cb3-14" tabindex="-1"></a><span class="do">## ── Conflicts ───── tidymodels_conflicts() ──</span></span>
<span id="cb3-15"><a href="lesson-1a-intro-to-machine-learning.html#cb3-15" tabindex="-1"></a><span class="do">## ✖ purrr::discard() masks scales::discard()</span></span>
<span id="cb3-16"><a href="lesson-1a-intro-to-machine-learning.html#cb3-16" tabindex="-1"></a><span class="do">## ✖ dplyr::filter()  masks plotly::filter(), stats::filter()</span></span>
<span id="cb3-17"><a href="lesson-1a-intro-to-machine-learning.html#cb3-17" tabindex="-1"></a><span class="do">## ✖ dplyr::lag()     masks stats::lag()</span></span>
<span id="cb3-18"><a href="lesson-1a-intro-to-machine-learning.html#cb3-18" tabindex="-1"></a><span class="do">## ✖ recipes::step()  masks stats::step()</span></span>
<span id="cb3-19"><a href="lesson-1a-intro-to-machine-learning.html#cb3-19" tabindex="-1"></a><span class="do">## • Dig deeper into tidy modeling with R at https://www.tmwr.org</span></span></code></pre></div>
<div id="knowledge-check-2" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Knowledge check<a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Check out the Tidymodels website: <a href="https://www.tidymodels.org/" class="uri">https://www.tidymodels.org/</a>.
Identify which packages can be used for:
</p>
<ol style="list-style-type: decimal">
<li>
Efficiently splitting your data
</li>
<li>
Optimizing hyperparameters
</li>
<li>
Measuring the effectiveness of your model
</li>
<li>
Working with correlation matrices
</li>
</ol>
</div>
</div>
</div>
<div id="the-data-sets" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> The data sets<a href="lesson-1a-intro-to-machine-learning.html#the-data-sets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The data sets chosen for this course allow us to illustrate the different features of the presented machine learning algorithms. Since the goal of this course is to demonstrate how to implement ML workflows, we make the assumption that you have already spent significant time wrangling, cleaning and getting to know your data via exploratory data analysis. This would allow you to perform many necessary tasks prior to the ML tasks outlined in this course such as:</p>
<ul>
<li>Feature selection (i.e., removing unnecessary variables and retaining only those variables you wish to include in your modeling process).</li>
<li>Recoding variable names and values so that they are meaningful and more interpretable.</li>
<li>Tidying data so that each column is a discrete variable and each row is an individual observation.</li>
<li>Recoding, removing, or some other approach to handling missing values.</li>
</ul>
<p>Consequently, the exemplar data sets we use throughout this book have, for the most part, gone through the necessary cleaning processes. As mentioned above, these data sets are fairly common data sets that provide good benchmarks to compare and illustrate ML workflows. Although some of these data sets are available in R, we will import these data sets from a .csv file to ensure consistency over time.</p>
<div id="boston-housing" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Boston housing<a href="lesson-1a-intro-to-machine-learning.html#boston-housing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in <span class="citation">Harrison Jr and Rubinfeld (<a href="#ref-harrison1978hedonic">1978</a>)</span> <a href="https://deepblue.lib.umich.edu/handle/2027.42/22636"><svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a>, it contains 13 attributes to predict the median property value.</p>
<ul>
<li><strong>problem type</strong>: supervised regression</li>
<li><strong>response variable</strong>: <code>medv</code> median value of owner-occupied homes in USD 1000’s (i.e. 21.8, 24.5)</li>
<li><strong>features</strong>: 13</li>
<li><strong>observations</strong>: 506</li>
<li><strong>objective</strong>: use property attributes to predict the median value of owner-occupied homes</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="lesson-1a-intro-to-machine-learning.html#cb4-1" tabindex="-1"></a><span class="co"># data file path</span></span>
<span id="cb4-2"><a href="lesson-1a-intro-to-machine-learning.html#cb4-2" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb4-3"><a href="lesson-1a-intro-to-machine-learning.html#cb4-3" tabindex="-1"></a>data_path <span class="ot">&lt;-</span> <span class="fu">here</span>(<span class="st">&quot;data&quot;</span>)</span>
<span id="cb4-4"><a href="lesson-1a-intro-to-machine-learning.html#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="lesson-1a-intro-to-machine-learning.html#cb4-5" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb4-6"><a href="lesson-1a-intro-to-machine-learning.html#cb4-6" tabindex="-1"></a>boston <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;boston.csv&quot;</span>))</span>
<span id="cb4-7"><a href="lesson-1a-intro-to-machine-learning.html#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="lesson-1a-intro-to-machine-learning.html#cb4-8" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb4-9"><a href="lesson-1a-intro-to-machine-learning.html#cb4-9" tabindex="-1"></a><span class="fu">dim</span>(boston)</span>
<span id="cb4-10"><a href="lesson-1a-intro-to-machine-learning.html#cb4-10" tabindex="-1"></a><span class="do">## [1] 506  16</span></span>
<span id="cb4-11"><a href="lesson-1a-intro-to-machine-learning.html#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="lesson-1a-intro-to-machine-learning.html#cb4-12" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb4-13"><a href="lesson-1a-intro-to-machine-learning.html#cb4-13" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(boston, <span class="sc">-</span>cmedv)</span>
<span id="cb4-14"><a href="lesson-1a-intro-to-machine-learning.html#cb4-14" tabindex="-1"></a><span class="do">## # A tibble: 506 × 15</span></span>
<span id="cb4-15"><a href="lesson-1a-intro-to-machine-learning.html#cb4-15" tabindex="-1"></a><span class="do">##      lon   lat    crim    zn indus  chas</span></span>
<span id="cb4-16"><a href="lesson-1a-intro-to-machine-learning.html#cb4-16" tabindex="-1"></a><span class="do">##    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb4-17"><a href="lesson-1a-intro-to-machine-learning.html#cb4-17" tabindex="-1"></a><span class="do">##  1 -71.0  42.3 0.00632  18    2.31     0</span></span>
<span id="cb4-18"><a href="lesson-1a-intro-to-machine-learning.html#cb4-18" tabindex="-1"></a><span class="do">##  2 -71.0  42.3 0.0273    0    7.07     0</span></span>
<span id="cb4-19"><a href="lesson-1a-intro-to-machine-learning.html#cb4-19" tabindex="-1"></a><span class="do">##  3 -70.9  42.3 0.0273    0    7.07     0</span></span>
<span id="cb4-20"><a href="lesson-1a-intro-to-machine-learning.html#cb4-20" tabindex="-1"></a><span class="do">##  4 -70.9  42.3 0.0324    0    2.18     0</span></span>
<span id="cb4-21"><a href="lesson-1a-intro-to-machine-learning.html#cb4-21" tabindex="-1"></a><span class="do">##  5 -70.9  42.3 0.0690    0    2.18     0</span></span>
<span id="cb4-22"><a href="lesson-1a-intro-to-machine-learning.html#cb4-22" tabindex="-1"></a><span class="do">##  6 -70.9  42.3 0.0298    0    2.18     0</span></span>
<span id="cb4-23"><a href="lesson-1a-intro-to-machine-learning.html#cb4-23" tabindex="-1"></a><span class="do">##  7 -70.9  42.3 0.0883   12.5  7.87     0</span></span>
<span id="cb4-24"><a href="lesson-1a-intro-to-machine-learning.html#cb4-24" tabindex="-1"></a><span class="do">##  8 -70.9  42.3 0.145    12.5  7.87     0</span></span>
<span id="cb4-25"><a href="lesson-1a-intro-to-machine-learning.html#cb4-25" tabindex="-1"></a><span class="do">##  9 -70.9  42.3 0.211    12.5  7.87     0</span></span>
<span id="cb4-26"><a href="lesson-1a-intro-to-machine-learning.html#cb4-26" tabindex="-1"></a><span class="do">## 10 -70.9  42.3 0.170    12.5  7.87     0</span></span>
<span id="cb4-27"><a href="lesson-1a-intro-to-machine-learning.html#cb4-27" tabindex="-1"></a><span class="do">## # ℹ 496 more rows</span></span>
<span id="cb4-28"><a href="lesson-1a-intro-to-machine-learning.html#cb4-28" tabindex="-1"></a><span class="do">## # ℹ 9 more variables: nox &lt;dbl&gt;, rm &lt;dbl&gt;,</span></span>
<span id="cb4-29"><a href="lesson-1a-intro-to-machine-learning.html#cb4-29" tabindex="-1"></a><span class="do">## #   age &lt;dbl&gt;, dis &lt;dbl&gt;, rad &lt;dbl&gt;,</span></span>
<span id="cb4-30"><a href="lesson-1a-intro-to-machine-learning.html#cb4-30" tabindex="-1"></a><span class="do">## #   tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, b &lt;dbl&gt;,</span></span>
<span id="cb4-31"><a href="lesson-1a-intro-to-machine-learning.html#cb4-31" tabindex="-1"></a><span class="do">## #   lstat &lt;dbl&gt;</span></span>
<span id="cb4-32"><a href="lesson-1a-intro-to-machine-learning.html#cb4-32" tabindex="-1"></a></span>
<span id="cb4-33"><a href="lesson-1a-intro-to-machine-learning.html#cb4-33" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb4-34"><a href="lesson-1a-intro-to-machine-learning.html#cb4-34" tabindex="-1"></a><span class="fu">head</span>(boston<span class="sc">$</span>cmedv)</span>
<span id="cb4-35"><a href="lesson-1a-intro-to-machine-learning.html#cb4-35" tabindex="-1"></a><span class="do">## [1] 24.0 21.6 34.7 33.4 36.2 28.7</span></span></code></pre></div>
</div>
<div id="pima-indians-diabetes" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Pima Indians Diabetes<a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and published in smith1988using <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/pdf/procascamc00018-0276.pdf"><svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a>, it contains 8 attributes to predict the presence of diabetes.</p>
<ul>
<li><strong>problem type</strong>: supervised binary classification</li>
<li><strong>response variable</strong>: <code>diabetes</code> positive or negative response (i.e. “pos”, “neg”)</li>
<li><strong>features</strong>: 8</li>
<li><strong>observations</strong>: 768</li>
<li><strong>objective</strong>: use biological attributes to predict the presence of diabetes</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="lesson-1a-intro-to-machine-learning.html#cb5-1" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb5-2"><a href="lesson-1a-intro-to-machine-learning.html#cb5-2" tabindex="-1"></a>pima <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;pima.csv&quot;</span>))</span>
<span id="cb5-3"><a href="lesson-1a-intro-to-machine-learning.html#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="lesson-1a-intro-to-machine-learning.html#cb5-4" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb5-5"><a href="lesson-1a-intro-to-machine-learning.html#cb5-5" tabindex="-1"></a><span class="fu">dim</span>(pima)</span>
<span id="cb5-6"><a href="lesson-1a-intro-to-machine-learning.html#cb5-6" tabindex="-1"></a><span class="do">## [1] 768   9</span></span>
<span id="cb5-7"><a href="lesson-1a-intro-to-machine-learning.html#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="lesson-1a-intro-to-machine-learning.html#cb5-8" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb5-9"><a href="lesson-1a-intro-to-machine-learning.html#cb5-9" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(pima, <span class="sc">-</span>diabetes)</span>
<span id="cb5-10"><a href="lesson-1a-intro-to-machine-learning.html#cb5-10" tabindex="-1"></a><span class="do">## # A tibble: 768 × 8</span></span>
<span id="cb5-11"><a href="lesson-1a-intro-to-machine-learning.html#cb5-11" tabindex="-1"></a><span class="do">##    pregnant glucose pressure triceps insulin</span></span>
<span id="cb5-12"><a href="lesson-1a-intro-to-machine-learning.html#cb5-12" tabindex="-1"></a><span class="do">##       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span id="cb5-13"><a href="lesson-1a-intro-to-machine-learning.html#cb5-13" tabindex="-1"></a><span class="do">##  1        6     148       72      35       0</span></span>
<span id="cb5-14"><a href="lesson-1a-intro-to-machine-learning.html#cb5-14" tabindex="-1"></a><span class="do">##  2        1      85       66      29       0</span></span>
<span id="cb5-15"><a href="lesson-1a-intro-to-machine-learning.html#cb5-15" tabindex="-1"></a><span class="do">##  3        8     183       64       0       0</span></span>
<span id="cb5-16"><a href="lesson-1a-intro-to-machine-learning.html#cb5-16" tabindex="-1"></a><span class="do">##  4        1      89       66      23      94</span></span>
<span id="cb5-17"><a href="lesson-1a-intro-to-machine-learning.html#cb5-17" tabindex="-1"></a><span class="do">##  5        0     137       40      35     168</span></span>
<span id="cb5-18"><a href="lesson-1a-intro-to-machine-learning.html#cb5-18" tabindex="-1"></a><span class="do">##  6        5     116       74       0       0</span></span>
<span id="cb5-19"><a href="lesson-1a-intro-to-machine-learning.html#cb5-19" tabindex="-1"></a><span class="do">##  7        3      78       50      32      88</span></span>
<span id="cb5-20"><a href="lesson-1a-intro-to-machine-learning.html#cb5-20" tabindex="-1"></a><span class="do">##  8       10     115        0       0       0</span></span>
<span id="cb5-21"><a href="lesson-1a-intro-to-machine-learning.html#cb5-21" tabindex="-1"></a><span class="do">##  9        2     197       70      45     543</span></span>
<span id="cb5-22"><a href="lesson-1a-intro-to-machine-learning.html#cb5-22" tabindex="-1"></a><span class="do">## 10        8     125       96       0       0</span></span>
<span id="cb5-23"><a href="lesson-1a-intro-to-machine-learning.html#cb5-23" tabindex="-1"></a><span class="do">## # ℹ 758 more rows</span></span>
<span id="cb5-24"><a href="lesson-1a-intro-to-machine-learning.html#cb5-24" tabindex="-1"></a><span class="do">## # ℹ 3 more variables: mass &lt;dbl&gt;,</span></span>
<span id="cb5-25"><a href="lesson-1a-intro-to-machine-learning.html#cb5-25" tabindex="-1"></a><span class="do">## #   pedigree &lt;dbl&gt;, age &lt;dbl&gt;</span></span>
<span id="cb5-26"><a href="lesson-1a-intro-to-machine-learning.html#cb5-26" tabindex="-1"></a></span>
<span id="cb5-27"><a href="lesson-1a-intro-to-machine-learning.html#cb5-27" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb5-28"><a href="lesson-1a-intro-to-machine-learning.html#cb5-28" tabindex="-1"></a><span class="fu">head</span>(pima<span class="sc">$</span>diabetes)</span>
<span id="cb5-29"><a href="lesson-1a-intro-to-machine-learning.html#cb5-29" tabindex="-1"></a><span class="do">## [1] &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot; &quot;pos&quot; &quot;neg&quot;</span></span></code></pre></div>
</div>
<div id="iris-flowers" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Iris flowers<a href="lesson-1a-intro-to-machine-learning.html#iris-flowers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper <span class="citation">(<a href="#ref-fisher1936use">R. A. Fisher 1936</a>)</span> <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x"><svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a>. It is sometimes called Anderson’s Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.</p>
<ul>
<li><strong>problem type</strong>: supervised multinomial classification</li>
<li><strong>response variable</strong>: <code>species</code> (i.e. “setosa”, “virginica”, “versicolor”)</li>
<li><strong>features</strong>: 4</li>
<li><strong>observations</strong>: 150</li>
<li><strong>objective</strong>: use plant leaf attributes to predict the type of flower</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="lesson-1a-intro-to-machine-learning.html#cb6-1" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb6-2"><a href="lesson-1a-intro-to-machine-learning.html#cb6-2" tabindex="-1"></a>iris <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;iris.csv&quot;</span>))</span>
<span id="cb6-3"><a href="lesson-1a-intro-to-machine-learning.html#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="lesson-1a-intro-to-machine-learning.html#cb6-4" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb6-5"><a href="lesson-1a-intro-to-machine-learning.html#cb6-5" tabindex="-1"></a><span class="fu">dim</span>(iris)</span>
<span id="cb6-6"><a href="lesson-1a-intro-to-machine-learning.html#cb6-6" tabindex="-1"></a><span class="do">## [1] 150   5</span></span>
<span id="cb6-7"><a href="lesson-1a-intro-to-machine-learning.html#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="lesson-1a-intro-to-machine-learning.html#cb6-8" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb6-9"><a href="lesson-1a-intro-to-machine-learning.html#cb6-9" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(iris, <span class="sc">-</span>Species)</span>
<span id="cb6-10"><a href="lesson-1a-intro-to-machine-learning.html#cb6-10" tabindex="-1"></a><span class="do">## # A tibble: 150 × 4</span></span>
<span id="cb6-11"><a href="lesson-1a-intro-to-machine-learning.html#cb6-11" tabindex="-1"></a><span class="do">##    Sepal.Length Sepal.Width Petal.Length</span></span>
<span id="cb6-12"><a href="lesson-1a-intro-to-machine-learning.html#cb6-12" tabindex="-1"></a><span class="do">##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;</span></span>
<span id="cb6-13"><a href="lesson-1a-intro-to-machine-learning.html#cb6-13" tabindex="-1"></a><span class="do">##  1          5.1         3.5          1.4</span></span>
<span id="cb6-14"><a href="lesson-1a-intro-to-machine-learning.html#cb6-14" tabindex="-1"></a><span class="do">##  2          4.9         3            1.4</span></span>
<span id="cb6-15"><a href="lesson-1a-intro-to-machine-learning.html#cb6-15" tabindex="-1"></a><span class="do">##  3          4.7         3.2          1.3</span></span>
<span id="cb6-16"><a href="lesson-1a-intro-to-machine-learning.html#cb6-16" tabindex="-1"></a><span class="do">##  4          4.6         3.1          1.5</span></span>
<span id="cb6-17"><a href="lesson-1a-intro-to-machine-learning.html#cb6-17" tabindex="-1"></a><span class="do">##  5          5           3.6          1.4</span></span>
<span id="cb6-18"><a href="lesson-1a-intro-to-machine-learning.html#cb6-18" tabindex="-1"></a><span class="do">##  6          5.4         3.9          1.7</span></span>
<span id="cb6-19"><a href="lesson-1a-intro-to-machine-learning.html#cb6-19" tabindex="-1"></a><span class="do">##  7          4.6         3.4          1.4</span></span>
<span id="cb6-20"><a href="lesson-1a-intro-to-machine-learning.html#cb6-20" tabindex="-1"></a><span class="do">##  8          5           3.4          1.5</span></span>
<span id="cb6-21"><a href="lesson-1a-intro-to-machine-learning.html#cb6-21" tabindex="-1"></a><span class="do">##  9          4.4         2.9          1.4</span></span>
<span id="cb6-22"><a href="lesson-1a-intro-to-machine-learning.html#cb6-22" tabindex="-1"></a><span class="do">## 10          4.9         3.1          1.5</span></span>
<span id="cb6-23"><a href="lesson-1a-intro-to-machine-learning.html#cb6-23" tabindex="-1"></a><span class="do">## # ℹ 140 more rows</span></span>
<span id="cb6-24"><a href="lesson-1a-intro-to-machine-learning.html#cb6-24" tabindex="-1"></a><span class="do">## # ℹ 1 more variable: Petal.Width &lt;dbl&gt;</span></span>
<span id="cb6-25"><a href="lesson-1a-intro-to-machine-learning.html#cb6-25" tabindex="-1"></a></span>
<span id="cb6-26"><a href="lesson-1a-intro-to-machine-learning.html#cb6-26" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb6-27"><a href="lesson-1a-intro-to-machine-learning.html#cb6-27" tabindex="-1"></a><span class="fu">head</span>(iris<span class="sc">$</span>Species)</span>
<span id="cb6-28"><a href="lesson-1a-intro-to-machine-learning.html#cb6-28" tabindex="-1"></a><span class="do">## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot;</span></span>
<span id="cb6-29"><a href="lesson-1a-intro-to-machine-learning.html#cb6-29" tabindex="-1"></a><span class="do">## [5] &quot;setosa&quot; &quot;setosa&quot;</span></span></code></pre></div>
</div>
<div id="ames-housing" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Ames housing<a href="lesson-1a-intro-to-machine-learning.html#ames-housing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Ames housing data set is an alternative to the Boston housing data set and provides a more comprehensive set of home features to predict sales price. More information can be found in <span class="citation">De Cock (<a href="#ref-de2011ames">2011</a>)</span> <a href="http://jse.amstat.org/v19n3/decock.pdf"><svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a>.</p>
<ul>
<li><strong>problem type</strong>: supervised regression</li>
<li><strong>response variable</strong>: <code>Sale_Price</code> (i.e., $195,000, $215,000)</li>
<li><strong>features</strong>: 80</li>
<li><strong>observations</strong>: 2,930</li>
<li><strong>objective</strong>: use property attributes to predict the sale price of a home</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="lesson-1a-intro-to-machine-learning.html#cb7-1" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb7-2"><a href="lesson-1a-intro-to-machine-learning.html#cb7-2" tabindex="-1"></a>ames <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;ames.csv&quot;</span>))</span>
<span id="cb7-3"><a href="lesson-1a-intro-to-machine-learning.html#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="lesson-1a-intro-to-machine-learning.html#cb7-4" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb7-5"><a href="lesson-1a-intro-to-machine-learning.html#cb7-5" tabindex="-1"></a><span class="fu">dim</span>(ames)</span>
<span id="cb7-6"><a href="lesson-1a-intro-to-machine-learning.html#cb7-6" tabindex="-1"></a><span class="do">## [1] 2930   81</span></span>
<span id="cb7-7"><a href="lesson-1a-intro-to-machine-learning.html#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="lesson-1a-intro-to-machine-learning.html#cb7-8" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb7-9"><a href="lesson-1a-intro-to-machine-learning.html#cb7-9" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(ames, <span class="sc">-</span>Sale_Price)</span>
<span id="cb7-10"><a href="lesson-1a-intro-to-machine-learning.html#cb7-10" tabindex="-1"></a><span class="do">## # A tibble: 2,930 × 80</span></span>
<span id="cb7-11"><a href="lesson-1a-intro-to-machine-learning.html#cb7-11" tabindex="-1"></a><span class="do">##    MS_SubClass        MS_Zoning Lot_Frontage</span></span>
<span id="cb7-12"><a href="lesson-1a-intro-to-machine-learning.html#cb7-12" tabindex="-1"></a><span class="do">##    &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;</span></span>
<span id="cb7-13"><a href="lesson-1a-intro-to-machine-learning.html#cb7-13" tabindex="-1"></a><span class="do">##  1 One_Story_1946_an… Resident…          141</span></span>
<span id="cb7-14"><a href="lesson-1a-intro-to-machine-learning.html#cb7-14" tabindex="-1"></a><span class="do">##  2 One_Story_1946_an… Resident…           80</span></span>
<span id="cb7-15"><a href="lesson-1a-intro-to-machine-learning.html#cb7-15" tabindex="-1"></a><span class="do">##  3 One_Story_1946_an… Resident…           81</span></span>
<span id="cb7-16"><a href="lesson-1a-intro-to-machine-learning.html#cb7-16" tabindex="-1"></a><span class="do">##  4 One_Story_1946_an… Resident…           93</span></span>
<span id="cb7-17"><a href="lesson-1a-intro-to-machine-learning.html#cb7-17" tabindex="-1"></a><span class="do">##  5 Two_Story_1946_an… Resident…           74</span></span>
<span id="cb7-18"><a href="lesson-1a-intro-to-machine-learning.html#cb7-18" tabindex="-1"></a><span class="do">##  6 Two_Story_1946_an… Resident…           78</span></span>
<span id="cb7-19"><a href="lesson-1a-intro-to-machine-learning.html#cb7-19" tabindex="-1"></a><span class="do">##  7 One_Story_PUD_194… Resident…           41</span></span>
<span id="cb7-20"><a href="lesson-1a-intro-to-machine-learning.html#cb7-20" tabindex="-1"></a><span class="do">##  8 One_Story_PUD_194… Resident…           43</span></span>
<span id="cb7-21"><a href="lesson-1a-intro-to-machine-learning.html#cb7-21" tabindex="-1"></a><span class="do">##  9 One_Story_PUD_194… Resident…           39</span></span>
<span id="cb7-22"><a href="lesson-1a-intro-to-machine-learning.html#cb7-22" tabindex="-1"></a><span class="do">## 10 Two_Story_1946_an… Resident…           60</span></span>
<span id="cb7-23"><a href="lesson-1a-intro-to-machine-learning.html#cb7-23" tabindex="-1"></a><span class="do">## # ℹ 2,920 more rows</span></span>
<span id="cb7-24"><a href="lesson-1a-intro-to-machine-learning.html#cb7-24" tabindex="-1"></a><span class="do">## # ℹ 77 more variables: Lot_Area &lt;dbl&gt;,</span></span>
<span id="cb7-25"><a href="lesson-1a-intro-to-machine-learning.html#cb7-25" tabindex="-1"></a><span class="do">## #   Street &lt;chr&gt;, Alley &lt;chr&gt;,</span></span>
<span id="cb7-26"><a href="lesson-1a-intro-to-machine-learning.html#cb7-26" tabindex="-1"></a><span class="do">## #   Lot_Shape &lt;chr&gt;, Land_Contour &lt;chr&gt;,</span></span>
<span id="cb7-27"><a href="lesson-1a-intro-to-machine-learning.html#cb7-27" tabindex="-1"></a><span class="do">## #   Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;,</span></span>
<span id="cb7-28"><a href="lesson-1a-intro-to-machine-learning.html#cb7-28" tabindex="-1"></a><span class="do">## #   Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;,</span></span>
<span id="cb7-29"><a href="lesson-1a-intro-to-machine-learning.html#cb7-29" tabindex="-1"></a><span class="do">## #   Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, …</span></span>
<span id="cb7-30"><a href="lesson-1a-intro-to-machine-learning.html#cb7-30" tabindex="-1"></a></span>
<span id="cb7-31"><a href="lesson-1a-intro-to-machine-learning.html#cb7-31" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb7-32"><a href="lesson-1a-intro-to-machine-learning.html#cb7-32" tabindex="-1"></a><span class="fu">head</span>(ames<span class="sc">$</span>Sale_Price)</span>
<span id="cb7-33"><a href="lesson-1a-intro-to-machine-learning.html#cb7-33" tabindex="-1"></a><span class="do">## [1] 215000 105000 172000 244000 189900</span></span>
<span id="cb7-34"><a href="lesson-1a-intro-to-machine-learning.html#cb7-34" tabindex="-1"></a><span class="do">## [6] 195500</span></span></code></pre></div>
</div>
<div id="attrition" class="section level3 hasAnchor" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Attrition<a href="lesson-1a-intro-to-machine-learning.html#attrition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The employee attrition data set was originally provided by <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/">IBM Watson Analytics Lab</a> and is a fictional data set created by IBM data scientists to explore what employee attributes influence attrition.</p>
<ul>
<li><strong>problem type</strong>: supervised binomial classification</li>
<li><strong>response variable</strong>: <code>Attrition</code> (i.e., “Yes”, “No”)</li>
<li><strong>features</strong>: 30</li>
<li><strong>observations</strong>: 1,470</li>
<li><strong>objective</strong>: use employee attributes to predict if they will attrit (leave the company)</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="lesson-1a-intro-to-machine-learning.html#cb8-1" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb8-2"><a href="lesson-1a-intro-to-machine-learning.html#cb8-2" tabindex="-1"></a>attrition <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;attrition.csv&quot;</span>))</span>
<span id="cb8-3"><a href="lesson-1a-intro-to-machine-learning.html#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="lesson-1a-intro-to-machine-learning.html#cb8-4" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb8-5"><a href="lesson-1a-intro-to-machine-learning.html#cb8-5" tabindex="-1"></a><span class="fu">dim</span>(attrition)</span>
<span id="cb8-6"><a href="lesson-1a-intro-to-machine-learning.html#cb8-6" tabindex="-1"></a><span class="do">## [1] 1470   31</span></span>
<span id="cb8-7"><a href="lesson-1a-intro-to-machine-learning.html#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="lesson-1a-intro-to-machine-learning.html#cb8-8" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb8-9"><a href="lesson-1a-intro-to-machine-learning.html#cb8-9" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(attrition, <span class="sc">-</span>Attrition)</span>
<span id="cb8-10"><a href="lesson-1a-intro-to-machine-learning.html#cb8-10" tabindex="-1"></a><span class="do">## # A tibble: 1,470 × 30</span></span>
<span id="cb8-11"><a href="lesson-1a-intro-to-machine-learning.html#cb8-11" tabindex="-1"></a><span class="do">##      Age BusinessTravel DailyRate Department</span></span>
<span id="cb8-12"><a href="lesson-1a-intro-to-machine-learning.html#cb8-12" tabindex="-1"></a><span class="do">##    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     </span></span>
<span id="cb8-13"><a href="lesson-1a-intro-to-machine-learning.html#cb8-13" tabindex="-1"></a><span class="do">##  1    41 Travel_Rarely       1102 Sales     </span></span>
<span id="cb8-14"><a href="lesson-1a-intro-to-machine-learning.html#cb8-14" tabindex="-1"></a><span class="do">##  2    49 Travel_Freque…       279 Research_…</span></span>
<span id="cb8-15"><a href="lesson-1a-intro-to-machine-learning.html#cb8-15" tabindex="-1"></a><span class="do">##  3    37 Travel_Rarely       1373 Research_…</span></span>
<span id="cb8-16"><a href="lesson-1a-intro-to-machine-learning.html#cb8-16" tabindex="-1"></a><span class="do">##  4    33 Travel_Freque…      1392 Research_…</span></span>
<span id="cb8-17"><a href="lesson-1a-intro-to-machine-learning.html#cb8-17" tabindex="-1"></a><span class="do">##  5    27 Travel_Rarely        591 Research_…</span></span>
<span id="cb8-18"><a href="lesson-1a-intro-to-machine-learning.html#cb8-18" tabindex="-1"></a><span class="do">##  6    32 Travel_Freque…      1005 Research_…</span></span>
<span id="cb8-19"><a href="lesson-1a-intro-to-machine-learning.html#cb8-19" tabindex="-1"></a><span class="do">##  7    59 Travel_Rarely       1324 Research_…</span></span>
<span id="cb8-20"><a href="lesson-1a-intro-to-machine-learning.html#cb8-20" tabindex="-1"></a><span class="do">##  8    30 Travel_Rarely       1358 Research_…</span></span>
<span id="cb8-21"><a href="lesson-1a-intro-to-machine-learning.html#cb8-21" tabindex="-1"></a><span class="do">##  9    38 Travel_Freque…       216 Research_…</span></span>
<span id="cb8-22"><a href="lesson-1a-intro-to-machine-learning.html#cb8-22" tabindex="-1"></a><span class="do">## 10    36 Travel_Rarely       1299 Research_…</span></span>
<span id="cb8-23"><a href="lesson-1a-intro-to-machine-learning.html#cb8-23" tabindex="-1"></a><span class="do">## # ℹ 1,460 more rows</span></span>
<span id="cb8-24"><a href="lesson-1a-intro-to-machine-learning.html#cb8-24" tabindex="-1"></a><span class="do">## # ℹ 26 more variables:</span></span>
<span id="cb8-25"><a href="lesson-1a-intro-to-machine-learning.html#cb8-25" tabindex="-1"></a><span class="do">## #   DistanceFromHome &lt;dbl&gt;,</span></span>
<span id="cb8-26"><a href="lesson-1a-intro-to-machine-learning.html#cb8-26" tabindex="-1"></a><span class="do">## #   Education &lt;chr&gt;, EducationField &lt;chr&gt;,</span></span>
<span id="cb8-27"><a href="lesson-1a-intro-to-machine-learning.html#cb8-27" tabindex="-1"></a><span class="do">## #   EnvironmentSatisfaction &lt;chr&gt;,</span></span>
<span id="cb8-28"><a href="lesson-1a-intro-to-machine-learning.html#cb8-28" tabindex="-1"></a><span class="do">## #   Gender &lt;chr&gt;, HourlyRate &lt;dbl&gt;,</span></span>
<span id="cb8-29"><a href="lesson-1a-intro-to-machine-learning.html#cb8-29" tabindex="-1"></a><span class="do">## #   JobInvolvement &lt;chr&gt;, JobLevel &lt;dbl&gt;, …</span></span>
<span id="cb8-30"><a href="lesson-1a-intro-to-machine-learning.html#cb8-30" tabindex="-1"></a></span>
<span id="cb8-31"><a href="lesson-1a-intro-to-machine-learning.html#cb8-31" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb8-32"><a href="lesson-1a-intro-to-machine-learning.html#cb8-32" tabindex="-1"></a><span class="fu">head</span>(attrition<span class="sc">$</span>Attrition)</span>
<span id="cb8-33"><a href="lesson-1a-intro-to-machine-learning.html#cb8-33" tabindex="-1"></a><span class="do">## [1] &quot;Yes&quot; &quot;No&quot;  &quot;Yes&quot; &quot;No&quot;  &quot;No&quot;  &quot;No&quot;</span></span></code></pre></div>
</div>
<div id="hitters" class="section level3 hasAnchor" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Hitters<a href="lesson-1a-intro-to-machine-learning.html#hitters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. The idea was to illustrate if and how major league baseball player’s batting performance could predict their salary. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. Note that the data does contain the players name but this should be removed during analysis and is not a valid feature.</p>
<ul>
<li><strong>problem type</strong>: supervised regression</li>
<li><strong>response variable</strong>: <code>Salary</code></li>
<li><strong>features</strong>: 19</li>
<li><strong>observations</strong>: 322</li>
<li><strong>objective</strong>: use baseball player’s batting attributes to predict their salary.</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="lesson-1a-intro-to-machine-learning.html#cb9-1" tabindex="-1"></a><span class="co"># access data</span></span>
<span id="cb9-2"><a href="lesson-1a-intro-to-machine-learning.html#cb9-2" tabindex="-1"></a>hitters <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="fu">here</span>(data_path, <span class="st">&quot;hitters.csv&quot;</span>))</span>
<span id="cb9-3"><a href="lesson-1a-intro-to-machine-learning.html#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="lesson-1a-intro-to-machine-learning.html#cb9-4" tabindex="-1"></a><span class="co"># initial dimension</span></span>
<span id="cb9-5"><a href="lesson-1a-intro-to-machine-learning.html#cb9-5" tabindex="-1"></a><span class="fu">dim</span>(hitters)</span>
<span id="cb9-6"><a href="lesson-1a-intro-to-machine-learning.html#cb9-6" tabindex="-1"></a><span class="do">## [1] 322  21</span></span>
<span id="cb9-7"><a href="lesson-1a-intro-to-machine-learning.html#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="lesson-1a-intro-to-machine-learning.html#cb9-8" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb9-9"><a href="lesson-1a-intro-to-machine-learning.html#cb9-9" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">select</span>(hitters, <span class="sc">-</span>Salary, <span class="sc">-</span>Player)</span>
<span id="cb9-10"><a href="lesson-1a-intro-to-machine-learning.html#cb9-10" tabindex="-1"></a><span class="do">## # A tibble: 322 × 19</span></span>
<span id="cb9-11"><a href="lesson-1a-intro-to-machine-learning.html#cb9-11" tabindex="-1"></a><span class="do">##    AtBat  Hits HmRun  Runs   RBI Walks Years</span></span>
<span id="cb9-12"><a href="lesson-1a-intro-to-machine-learning.html#cb9-12" tabindex="-1"></a><span class="do">##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb9-13"><a href="lesson-1a-intro-to-machine-learning.html#cb9-13" tabindex="-1"></a><span class="do">##  1   293    66     1    30    29    14     1</span></span>
<span id="cb9-14"><a href="lesson-1a-intro-to-machine-learning.html#cb9-14" tabindex="-1"></a><span class="do">##  2   315    81     7    24    38    39    14</span></span>
<span id="cb9-15"><a href="lesson-1a-intro-to-machine-learning.html#cb9-15" tabindex="-1"></a><span class="do">##  3   479   130    18    66    72    76     3</span></span>
<span id="cb9-16"><a href="lesson-1a-intro-to-machine-learning.html#cb9-16" tabindex="-1"></a><span class="do">##  4   496   141    20    65    78    37    11</span></span>
<span id="cb9-17"><a href="lesson-1a-intro-to-machine-learning.html#cb9-17" tabindex="-1"></a><span class="do">##  5   321    87    10    39    42    30     2</span></span>
<span id="cb9-18"><a href="lesson-1a-intro-to-machine-learning.html#cb9-18" tabindex="-1"></a><span class="do">##  6   594   169     4    74    51    35    11</span></span>
<span id="cb9-19"><a href="lesson-1a-intro-to-machine-learning.html#cb9-19" tabindex="-1"></a><span class="do">##  7   185    37     1    23     8    21     2</span></span>
<span id="cb9-20"><a href="lesson-1a-intro-to-machine-learning.html#cb9-20" tabindex="-1"></a><span class="do">##  8   298    73     0    24    24     7     3</span></span>
<span id="cb9-21"><a href="lesson-1a-intro-to-machine-learning.html#cb9-21" tabindex="-1"></a><span class="do">##  9   323    81     6    26    32     8     2</span></span>
<span id="cb9-22"><a href="lesson-1a-intro-to-machine-learning.html#cb9-22" tabindex="-1"></a><span class="do">## 10   401    92    17    49    66    65    13</span></span>
<span id="cb9-23"><a href="lesson-1a-intro-to-machine-learning.html#cb9-23" tabindex="-1"></a><span class="do">## # ℹ 312 more rows</span></span>
<span id="cb9-24"><a href="lesson-1a-intro-to-machine-learning.html#cb9-24" tabindex="-1"></a><span class="do">## # ℹ 12 more variables: CAtBat &lt;dbl&gt;,</span></span>
<span id="cb9-25"><a href="lesson-1a-intro-to-machine-learning.html#cb9-25" tabindex="-1"></a><span class="do">## #   CHits &lt;dbl&gt;, CHmRun &lt;dbl&gt;, CRuns &lt;dbl&gt;,</span></span>
<span id="cb9-26"><a href="lesson-1a-intro-to-machine-learning.html#cb9-26" tabindex="-1"></a><span class="do">## #   CRBI &lt;dbl&gt;, CWalks &lt;dbl&gt;, League &lt;chr&gt;,</span></span>
<span id="cb9-27"><a href="lesson-1a-intro-to-machine-learning.html#cb9-27" tabindex="-1"></a><span class="do">## #   Division &lt;chr&gt;, PutOuts &lt;dbl&gt;,</span></span>
<span id="cb9-28"><a href="lesson-1a-intro-to-machine-learning.html#cb9-28" tabindex="-1"></a><span class="do">## #   Assists &lt;dbl&gt;, Errors &lt;dbl&gt;,</span></span>
<span id="cb9-29"><a href="lesson-1a-intro-to-machine-learning.html#cb9-29" tabindex="-1"></a><span class="do">## #   NewLeague &lt;chr&gt;</span></span>
<span id="cb9-30"><a href="lesson-1a-intro-to-machine-learning.html#cb9-30" tabindex="-1"></a></span>
<span id="cb9-31"><a href="lesson-1a-intro-to-machine-learning.html#cb9-31" tabindex="-1"></a><span class="co"># response variable</span></span>
<span id="cb9-32"><a href="lesson-1a-intro-to-machine-learning.html#cb9-32" tabindex="-1"></a><span class="fu">head</span>(hitters<span class="sc">$</span>Salary)</span>
<span id="cb9-33"><a href="lesson-1a-intro-to-machine-learning.html#cb9-33" tabindex="-1"></a><span class="do">## [1]    NA 475.0 480.0 500.0  91.5 750.0</span></span></code></pre></div>
</div>
</div>
<div id="what-youll-learn-next" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> What You’ll Learn Next<a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The lessons that follow are designed to help you understand the individual sub-tasks of an ML project. The focus is to have an intuitive understanding of each discrete sub-task and algorithm. Once you understand when, where, and why these sub-tasks are performed you will be able to transfer this knowledge to other projects. The concepts you will learn include:</p>
<ol style="list-style-type: decimal">
<li>Provide an overview of the ML modeling process:
<ul>
<li>data splitting</li>
<li>model fitting</li>
<li>model validation and tuning</li>
<li>performance measurement</li>
<li>feature engineering</li>
</ul></li>
<li>Cover common supervised learners:
<ul>
<li>linear regression</li>
<li>regularized regression</li>
<li>K-nearest neighbors</li>
<li>decision trees</li>
<li>bagging &amp; random forests</li>
<li>gradient boosting</li>
</ul></li>
<li>Cover common unsupervised learners:
<ul>
<li>K-means clustering</li>
<li>Principal component analysis</li>
</ul></li>
<li>Along the way you’ll learn about:
<ul>
<li>each algorithm’s hyperparameters</li>
<li>model interpretation</li>
<li>feature importance</li>
<li>and more!</li>
</ul></li>
</ol>
</div>
<div id="exercises" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Exercises<a href="lesson-1a-intro-to-machine-learning.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<ol style="list-style-type: decimal">
<li>
Identify four real-life applications of supervised and unsupervised
problems.
<ul>
<li>
Explain what makes these problems supervised versus
unsupervised.
</li>
<li>
For each problem identify the target variable (if applicable) and
potential features.
</li>
</ul>
</li>
<li>
Identify and contrast a regression problem with a classification
problem.
<ul>
<li>
What is the target variable in each problem and why would being able
to accurately predict this target be beneficial to society?
</li>
<li>
What are potential features and where could you collect this
information?
</li>
<li>
What is determining if the problem is a regression or a
classification problem?
</li>
</ul>
</li>
<li>
Identify three open source data sets suitable for machine learning
(e.g., <a href="https://bit.ly/35wKu5c" class="uri">https://bit.ly/35wKu5c</a>).
<ul>
<li>
Explain the type of machine learning models that could be
constructed from the data (e.g., supervised versus unsupervised and
regression versus classification).
</li>
<li>
What are the dimensions of the data?
</li>
<li>
Is there a code book that explains who collected the data, why it
was originally collected, and what each variable represents?
</li>
<li>
If the data set is suitable for supervised learning, which
variable(s) could be considered as a useful target? Which variable(s)
could be considered as features?
</li>
</ul>
</li>
<li>
Identify examples of misuse of machine learning in society. What was
the ethical concern?
</li>
</ol>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-de2011ames" class="csl-entry">
De Cock, Dean. 2011. <span>“Ames, <span>I</span>owa: Alternative to the <span>B</span>oston Housing Data as an End of Semester Regression Project.”</span> <em>Journal of Statistics Education</em> 19 (3).
</div>
<div id="ref-fisher1936use" class="csl-entry">
Fisher, Ronald A. 1936. <span>“The Use of Multiple Measurements in Taxonomic Problems.”</span> <em>Annals of Eugenics</em> 7 (2): 179–88.
</div>
<div id="ref-harrison1978hedonic" class="csl-entry">
Harrison Jr, David, and Daniel L Rubinfeld. 1978. <span>“Hedonic Housing Prices and the Demand for Clean Air.”</span> <em>Journal of Environmental Economics and Management</em> 5 (1): 81–102.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overview.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lesson-1b-first-model-with-tidymodels.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-1/lesson-1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
