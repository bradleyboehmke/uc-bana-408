<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12 Lesson 4b: Regularized Regression | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="12 Lesson 4b: Regularized Regression | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12 Lesson 4b: Regularized Regression | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lesson-4a-logistic-regression.html"/>
<link rel="next" href="overview-4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.9/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing</a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes</a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers</a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing</a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition</a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 4</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-4b-regularized-regression" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">12</span> Lesson 4b: Regularized Regression<a href="lesson-4b-regularized-regression.html#lesson-4b-regularized-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Linear models (LMs) provide a simple, yet effective, approach to predictive modeling. Moreover, when certain assumptions required by LMs are met (e.g., constant variance), the estimated coefficients are unbiased and, of all linear unbiased estimates, have the lowest variance. However, in today’s world, data sets being analyzed typically contain a large number of features. As the number of features grow, certain assumptions typically break down and these models tend to overfit the training data, causing our generalization error to increase. <strong>Regularization</strong> methods provide a means to constrain or <em>regularize</em> the estimated coefficients, which can reduce the variance and decrease the generalization error.</p>
<div id="learning-objectives-12" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Learning objectives<a href="lesson-4b-regularized-regression.html#learning-objectives-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module you will know:</p>
<ul>
<li>Why reguralization is important.</li>
<li>How to apply ridge, lasso, and elastic net regularized models.</li>
<li>Extract and visualize the most influential features.</li>
</ul>
</div>
<div id="prerequisites-6" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Prerequisites<a href="lesson-4b-regularized-regression.html#prerequisites-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="lesson-4b-regularized-regression.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper packages</span></span>
<span id="cb123-2"><a href="lesson-4b-regularized-regression.html#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># general data munging &amp; visualization</span></span>
<span id="cb123-3"><a href="lesson-4b-regularized-regression.html#cb123-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-4"><a href="lesson-4b-regularized-regression.html#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Modeling packages</span></span>
<span id="cb123-5"><a href="lesson-4b-regularized-regression.html#cb123-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb123-6"><a href="lesson-4b-regularized-regression.html#cb123-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-7"><a href="lesson-4b-regularized-regression.html#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Model interpretability packages</span></span>
<span id="cb123-8"><a href="lesson-4b-regularized-regression.html#cb123-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)      <span class="co"># for variable importance</span></span></code></pre></div>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="lesson-4b-regularized-regression.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Stratified sampling with the rsample package</span></span>
<span id="cb124-2"><a href="lesson-4b-regularized-regression.html#cb124-2" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span>
<span id="cb124-3"><a href="lesson-4b-regularized-regression.html#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb124-4"><a href="lesson-4b-regularized-regression.html#cb124-4" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)</span>
<span id="cb124-5"><a href="lesson-4b-regularized-regression.html#cb124-5" aria-hidden="true" tabindex="-1"></a>ames_train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb124-6"><a href="lesson-4b-regularized-regression.html#cb124-6" aria-hidden="true" tabindex="-1"></a>ames_test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="why-regularize" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Why regularize?<a href="lesson-4b-regularized-regression.html#why-regularize" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_hzpzt5s2&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_bid9iuym" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Why regularize">
</iframe>
</div>
<p>The easiest way to understand regularized regression is to explain how and why it is applied to ordinary least squares (OLS). The objective in OLS regression is to find the <em>hyperplane</em> (e.g., a straight line in two dimensions) that minimizes the sum of squared errors (SSE) between the observed and predicted response values (see Figure below). This means identifying the hyperplane that minimizes the grey lines, which measure the vertical distance between the observed (red dots) and predicted (blue line) response values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane"></span>
<img src="_main_files/figure-html/hyperplane-1.png" alt="Figure: Fitted regression line using Ordinary Least Squares." width="576" />
<p class="caption">
Figure 12.1: Figure: Fitted regression line using Ordinary Least Squares.
</p>
</div>
<p>More formally, the objective function being minimized can be written as:</p>
<p><span class="math display">\[\begin{equation}
\text{minimize} \left( SSE = \sum^n_{i=1} \left(y_i - \hat{y}_i\right)^2 \right)
\end{equation}\]</span></p>
<div class="note">
<p>
Recall that we have use the following terms interchangably:
</p>
<ul>
<li>
Sum of squared errors (SSE)
</li>
<li>
Residual sum of squares (RSS)
</li>
</ul>
</div>
<p>However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model. Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results.</p>
<ul>
<li><strong>Linear relationship</strong>: Linear regression assumes a linear relationship between the predictor and the response variable.</li>
<li><strong>More observations than predictors</strong>: Although not an issue with the Ames housing data, when the number of features exceeds the number of observations (<span class="math inline">\(p &gt; n\)</span>), the OLS estimates are not obtainable.</li>
<li><strong>No or little multicollinearity</strong>: Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in OLS, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This obviously leads to an inaccurate interpretation of coefficients and makes it difficult to identify influential predictors.</li>
</ul>
<div class="note">
<p>
There are other assumptions that ordinary least squares regression
makes that are often violated with larger data sets.
</p>
</div>
<p>Many real-life data sets, like those common to <em>text mining</em> and <em>genomic studies</em> are <em>wide</em>, meaning they contain a larger number of features (<span class="math inline">\(p &gt; n\)</span>). As <em>p</em> increases, we’re more likely to violate some of the OLS assumptions, which can cause poor model performance. Consequently, alternative algorithms should be considered.</p>
<p>Having a large number of features invites additional issues in using classic regression models. For one, having a large number of features makes the model much less interpretable. Additionally, when <span class="math inline">\(p &gt; n\)</span>, there are many (in fact infinite) solutions to the OLS problem! In such cases, it is useful (and practical) to assume that a smaller subset of the features exhibit the strongest effects (something called the <em>bet on sparsity principle</em> <span class="citation">(see <a href="#ref-hastie2015statistical" role="doc-biblioref">Hastie, Tibshirani, and Wainwright 2015, 2</a>)</span>.). For this reason, we sometimes prefer estimation techniques that incorporate <em>feature selection</em>. One approach to this is called <em>hard thresholding</em> feature selection, which includes many of the traditional linear model selection approaches like <em>forward selection</em> and <em>backward elimination</em>. These procedures, however, can be computationally inefficient, do not scale well, and treat a feature as either in or out of the model (hence the name hard thresholding). In contrast, a more modern approach, called <em>soft thresholding</em>, slowly pushes the effects of irrelevant features toward zero, and in some cases, will zero out entire coefficients. As will be demonstrated, this can result in more accurate models that are also easier to interpret.</p>
<p>With wide data (or data that exhibits multicollinearity), one alternative to OLS regression is to use regularized regression (also commonly referred to as <em>penalized</em> models or <em>shrinkage</em> methods as in <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-esl" role="doc-biblioref">2001</a>)</span> and <span class="citation">Kuhn and Johnson (<a href="#ref-apm" role="doc-biblioref">2013</a>)</span>) to constrain the total size of all the coefficient estimates. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model (at the expense of no longer being unbiased—a reasonable compromise).</p>
<p>The objective function of a regularized regression model is similar to OLS, albeit with a penalty term <span class="math inline">\(P\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\text{minimize} \left( SSE + P \right)
\end{equation}\]</span></p>
<p>This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE).</p>
<p>This concept generalizes to all GLM models (e.g., logistic and Poisson regression) and even some <em>survival models</em>. So far, we have been discussing OLS and the sum of squared errors loss function. However, different models within the GLM family have different loss functions (see Chapter 4 of <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-esl" role="doc-biblioref">2001</a>)</span>). Yet we can think of the penalty parameter all the same—it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model’s loss function.</p>
<div class="note">
<p>
Regularized regression constrains the size of the predictor variable
coefficients such that the only way the coefficients can increase is if
we experience a comparable decrease in the model’s loss function.
</p>
<p>
Constraining coefficients actually creates more stable models, which
can improve model performance and generalization.
</p>
</div>
<p>There are three common penalty parameters we can implement:</p>
<ol style="list-style-type: decimal">
<li>Ridge;</li>
<li>Lasso (or LASSO);</li>
<li>Elastic net (or ENET), which is a combination of ridge and lasso.</li>
</ol>
</div>
<div id="ridge-penalty" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Ridge penalty<a href="lesson-4b-regularized-regression.html#ridge-penalty" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression <span class="citation">(<a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl and Kennard 1970</a>)</span> controls the estimated coefficients by adding <font color="red"><span class="math inline">\(\lambda \sum^p_{j=1} \beta_j^2\)</span></font> to the objective function.</p>
<p><span class="math display">\[\begin{equation}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} \beta_j^2 \right)
\end{equation}\]</span></p>
<p>The size of this penalty, referred to as <span class="math inline">\(L^2\)</span> (or Euclidean) norm, can take on a wide range of values, which is controlled by the <em>tuning parameter</em> <span class="math inline">\(\lambda\)</span>. When <span class="math inline">\(\lambda = 0\)</span> there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the penalty becomes large and forces the coefficients toward zero (but not all the way). This is illustrated below where exemplar coefficients have been regularized with <span class="math inline">\(\lambda\)</span> ranging from 0 to over 8,000.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridge-coef-example"></span>
<img src="_main_files/figure-html/ridge-coef-example-1.png" alt="Figure: Ridge regression coefficients for 15 exemplar predictor variables as $\lambda$ grows from  $0 \rightarrow \infty$. As $\lambda$ grows larger, our coefficient magnitudes are more constrained." width="672" />
<p class="caption">
Figure 12.2: Figure: Ridge regression coefficients for 15 exemplar predictor variables as <span class="math inline">\(\lambda\)</span> grows from <span class="math inline">\(0 \rightarrow \infty\)</span>. As <span class="math inline">\(\lambda\)</span> grows larger, our coefficient magnitudes are more constrained.
</p>
</div>
<p>Although these coefficients were scaled and centered prior to the analysis, you will notice that some are quite large when <span class="math inline">\(\lambda\)</span> is near zero. Furthermore, you’ll notice that feature <code>x1</code> has a large negative parameter that fluctuates until <span class="math inline">\(\lambda \approx 7\)</span> where it then continuously shrinks toward zero. This is indicative of multicollinearity and likely illustrates that constraining our coefficients with <span class="math inline">\(\lambda &gt; 7\)</span> may reduce the variance, and therefore the error, in our predictions.</p>
<p>In essence, the ridge regression model pushes many of the correlated features toward each other rather than allowing for one to be wildly positive and the other wildly negative. In addition, many of the less-important features also get pushed toward zero. This helps to provide clarity in identifying the important signals in our data.</p>
<p>However, ridge regression does not perform feature selection and will retain <strong>all</strong> available features in the final model. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity). If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable.</p>
</div>
<div id="lasso" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Lasso penalty<a href="lesson-4b-regularized-regression.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The lasso (<em>least absolute shrinkage and selection operator</em>) penalty <span class="citation">(<a href="#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani 1996</a>)</span> is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the <span class="math inline">\(L^2\)</span> norm for an <span class="math inline">\(L^1\)</span> norm: <span class="math inline">\(\lambda \sum^p_{j=1} | \beta_j|\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\text{minimize } \left( SSE + \lambda \sum^p_{j=1} | \beta_j | \right)
\end{equation}\]</span></p>
<p>Whereas the ridge penalty pushes variables to <em>approximately but not equal to zero</em>, the lasso penalty will actually push coefficients all the way to zero as illustrated in below. Switching to the lasso penalty not only improves the model but it also conducts automated feature selection.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lasso-coef-example"></span>
<img src="_main_files/figure-html/lasso-coef-example-1.png" alt="Figure: Lasso regression coefficients as $\lambda$ grows from  $0 \rightarrow \infty$." width="672" />
<p class="caption">
Figure 12.3: Figure: Lasso regression coefficients as <span class="math inline">\(\lambda\)</span> grows from <span class="math inline">\(0 \rightarrow \infty\)</span>.
</p>
</div>
<p>In the figure above we see that when <span class="math inline">\(\lambda &lt; 0.01\)</span> all 15 variables are included in the model, when <span class="math inline">\(\lambda \approx 0.5\)</span> 9 variables are retained, and when <span class="math inline">\(log\left(\lambda\right) = 1\)</span> only 5 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal.</p>
</div>
<div id="elastic" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Elastic nets<a href="lesson-4b-regularized-regression.html#elastic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A generalization of the ridge and lasso penalties, called the <em>elastic net</em> <span class="citation">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>)</span>, combines the two penalties:</p>
<p><span class="math display">\[\begin{equation}
\text{minimize } \left( SSE + \lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} | \beta_j | \right)
\end{equation}\]</span></p>
<p>Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elastic-net-coef-example"></span>
<img src="_main_files/figure-html/elastic-net-coef-example-1.png" alt="Figure: Elastic net coefficients as $\lambda$ grows from  $0 \rightarrow \infty$." width="672" />
<p class="caption">
Figure 12.4: Figure: Elastic net coefficients as <span class="math inline">\(\lambda\)</span> grows from <span class="math inline">\(0 \rightarrow \infty\)</span>.
</p>
</div>
</div>
<div id="implementation" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">12.7</span> Implementation<a href="lesson-4b-regularized-regression.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_je5mqhk7&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_z316xaw5" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Regularization implementation">
</iframe>
</div>
<p>We will start by applying a simple ridge model. In this example we simplify and focus on only four features: <code>Gr_Liv_Area</code>, <code>Year_Built</code>, <code>Garage_Cars</code>, and <code>Garage_Area</code>. In this example we will apply a <span class="math inline">\(\lambda\)</span> penalty parameter equal to 1.</p>
<div class="note">
<p>
Since regularized methods apply a penalty to the coefficients, we
need to ensure our coefficients are on a common scale. If not, then
predictors with naturally larger values (e.g., total square footage)
will be penalized more than predictors with naturally smaller values
(e.g., total number of rooms).
</p>
<p>
<strong>Takeaway - when using regularized regression models always
standardize numeric features so they have mean 0 and standard deviation
of 1.</strong>
</p>
</div>
<p>To apply a regularized model we still use <code>linear_reg()</code> but we change the engine and provide some additional arguments. There are a few engines that allow us to apply regularized models but the most popular is glmnet. In R, the <span class="math inline">\(\lambda\)</span> penalty parameter is represented by <code>penalty</code> and <code>mixture</code> controls the type of penalty (0 = ridge, 1 = lasso, and any value in between represents an elastic net).</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="lesson-4b-regularized-regression.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: create ridge model object</span></span>
<span id="cb125-2"><a href="lesson-4b-regularized-regression.html#cb125-2" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">1000</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb125-3"><a href="lesson-4b-regularized-regression.html#cb125-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb125-4"><a href="lesson-4b-regularized-regression.html#cb125-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-5"><a href="lesson-4b-regularized-regression.html#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: create model &amp; preprocessing recipe</span></span>
<span id="cb125-6"><a href="lesson-4b-regularized-regression.html#cb125-6" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_train) <span class="sc">%&gt;%</span></span>
<span id="cb125-7"><a href="lesson-4b-regularized-regression.html#cb125-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb125-8"><a href="lesson-4b-regularized-regression.html#cb125-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal_predictors</span>())</span>
<span id="cb125-9"><a href="lesson-4b-regularized-regression.html#cb125-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-10"><a href="lesson-4b-regularized-regression.html#cb125-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: fit model workflow</span></span>
<span id="cb125-11"><a href="lesson-4b-regularized-regression.html#cb125-11" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb125-12"><a href="lesson-4b-regularized-regression.html#cb125-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb125-13"><a href="lesson-4b-regularized-regression.html#cb125-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ridge_mod) <span class="sc">%&gt;%</span></span>
<span id="cb125-14"><a href="lesson-4b-regularized-regression.html#cb125-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb125-15"><a href="lesson-4b-regularized-regression.html#cb125-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-16"><a href="lesson-4b-regularized-regression.html#cb125-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: extract and tidy results</span></span>
<span id="cb125-17"><a href="lesson-4b-regularized-regression.html#cb125-17" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="sc">%&gt;%</span></span>
<span id="cb125-18"><a href="lesson-4b-regularized-regression.html#cb125-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb125-19"><a href="lesson-4b-regularized-regression.html#cb125-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>()</span>
<span id="cb125-20"><a href="lesson-4b-regularized-regression.html#cb125-20" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 309 × 3</span></span>
<span id="cb125-21"><a href="lesson-4b-regularized-regression.html#cb125-21" aria-hidden="true" tabindex="-1"></a><span class="do">##    term           estimate penalty</span></span>
<span id="cb125-22"><a href="lesson-4b-regularized-regression.html#cb125-22" aria-hidden="true" tabindex="-1"></a><span class="do">##    &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span id="cb125-23"><a href="lesson-4b-regularized-regression.html#cb125-23" aria-hidden="true" tabindex="-1"></a><span class="do">##  1 (Intercept)     154704.    1000</span></span>
<span id="cb125-24"><a href="lesson-4b-regularized-regression.html#cb125-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  2 Lot_Frontage      -841.    1000</span></span>
<span id="cb125-25"><a href="lesson-4b-regularized-regression.html#cb125-25" aria-hidden="true" tabindex="-1"></a><span class="do">##  3 Lot_Area          4067.    1000</span></span>
<span id="cb125-26"><a href="lesson-4b-regularized-regression.html#cb125-26" aria-hidden="true" tabindex="-1"></a><span class="do">##  4 Year_Built        4962.    1000</span></span>
<span id="cb125-27"><a href="lesson-4b-regularized-regression.html#cb125-27" aria-hidden="true" tabindex="-1"></a><span class="do">##  5 Year_Remod_Add    2876.    1000</span></span>
<span id="cb125-28"><a href="lesson-4b-regularized-regression.html#cb125-28" aria-hidden="true" tabindex="-1"></a><span class="do">##  6 Mas_Vnr_Area      3940.    1000</span></span>
<span id="cb125-29"><a href="lesson-4b-regularized-regression.html#cb125-29" aria-hidden="true" tabindex="-1"></a><span class="do">##  7 BsmtFin_SF_1      -957.    1000</span></span>
<span id="cb125-30"><a href="lesson-4b-regularized-regression.html#cb125-30" aria-hidden="true" tabindex="-1"></a><span class="do">##  8 BsmtFin_SF_2       597.    1000</span></span>
<span id="cb125-31"><a href="lesson-4b-regularized-regression.html#cb125-31" aria-hidden="true" tabindex="-1"></a><span class="do">##  9 Bsmt_Unf_SF      -2925.    1000</span></span>
<span id="cb125-32"><a href="lesson-4b-regularized-regression.html#cb125-32" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 Total_Bsmt_SF     5941.    1000</span></span>
<span id="cb125-33"><a href="lesson-4b-regularized-regression.html#cb125-33" aria-hidden="true" tabindex="-1"></a><span class="do">## # … with 299 more rows</span></span>
<span id="cb125-34"><a href="lesson-4b-regularized-regression.html#cb125-34" aria-hidden="true" tabindex="-1"></a><span class="do">## # ℹ Use `print(n = ...)` to see more rows</span></span></code></pre></div>
<p>Using <code>tidy()</code> on our fit workflow provides the regularized coefficients. Note that these coefficients are interpreted differently than in previous modules since we standardized our features.</p>
<div class="note">
<p>
Since our features are standardized to mean = 0 and standard
deviation = 1, we interpret the coefficients as…
</p>
<p>
For every 1 standard deviation above the mean
<code>Year_Built</code>, the average predicted <code>Sale_Price</code>
increases by $4,962.
</p>
</div>
<p>How well does our model perform, let’s use a 5-fold cross validation procedure to compute our generalization error:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="lesson-4b-regularized-regression.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb126-2"><a href="lesson-4b-regularized-regression.html#cb126-2" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(ames_train, <span class="at">v =</span> <span class="dv">5</span>, <span class="at">strata =</span> Sale_Price)</span>
<span id="cb126-3"><a href="lesson-4b-regularized-regression.html#cb126-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-4"><a href="lesson-4b-regularized-regression.html#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb126-5"><a href="lesson-4b-regularized-regression.html#cb126-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb126-6"><a href="lesson-4b-regularized-regression.html#cb126-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(ridge_mod) <span class="sc">%&gt;%</span></span>
<span id="cb126-7"><a href="lesson-4b-regularized-regression.html#cb126-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds) <span class="sc">%&gt;%</span></span>
<span id="cb126-8"><a href="lesson-4b-regularized-regression.html#cb126-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb126-9"><a href="lesson-4b-regularized-regression.html#cb126-9" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb126-10"><a href="lesson-4b-regularized-regression.html#cb126-10" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 6</span></span>
<span id="cb126-11"><a href="lesson-4b-regularized-regression.html#cb126-11" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator   mean     n std_err .config             </span></span>
<span id="cb126-12"><a href="lesson-4b-regularized-regression.html#cb126-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb126-13"><a href="lesson-4b-regularized-regression.html#cb126-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   31373.     5   3012. Preprocessor1_Model1</span></span></code></pre></div>
<div id="knowledge-check-22" class="section level3 hasAnchor" number="12.7.1">
<h3><span class="header-section-number">12.7.1</span> Knowledge check<a href="lesson-4b-regularized-regression.html#knowledge-check-22" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> data provided via Canvas fill in
the blanks to…
</p>
<ol style="list-style-type: decimal">
<li>
create train and test splits,
</li>
<li>
create ridge model object using <span class="math inline"><span class="math inline">\(\lambda = 5000\)</span></span>,
</li>
<li>
create model (our response variable <code>cmedv</code> should be a
function of all predictor variables) &amp; preprocessing recipe where
you standardize all predictor variables,
</li>
<li>
create 5-fold resampling object stratified on the response
variable,
</li>
<li>
fit model across resampling object and compute the cross validation
RMSE.
</li>
</ol>
</div>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="lesson-4b-regularized-regression.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: create train and test splits</span></span>
<span id="cb127-2"><a href="lesson-4b-regularized-regression.html#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb127-3"><a href="lesson-4b-regularized-regression.html#cb127-3" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(_______, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;cmedv&quot;</span>)</span>
<span id="cb127-4"><a href="lesson-4b-regularized-regression.html#cb127-4" aria-hidden="true" tabindex="-1"></a>boston_train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb127-5"><a href="lesson-4b-regularized-regression.html#cb127-5" aria-hidden="true" tabindex="-1"></a>boston_test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span>
<span id="cb127-6"><a href="lesson-4b-regularized-regression.html#cb127-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-7"><a href="lesson-4b-regularized-regression.html#cb127-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: create ridge model object</span></span>
<span id="cb127-8"><a href="lesson-4b-regularized-regression.html#cb127-8" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> ____, <span class="at">mixture =</span> __) <span class="sc">%&gt;%</span></span>
<span id="cb127-9"><a href="lesson-4b-regularized-regression.html#cb127-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb127-10"><a href="lesson-4b-regularized-regression.html#cb127-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-11"><a href="lesson-4b-regularized-regression.html#cb127-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: create model &amp; preprocessing recipe</span></span>
<span id="cb127-12"><a href="lesson-4b-regularized-regression.html#cb127-12" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(cmedv <span class="sc">~</span> ., <span class="at">data =</span> _______) <span class="sc">%&gt;%</span></span>
<span id="cb127-13"><a href="lesson-4b-regularized-regression.html#cb127-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(________)</span>
<span id="cb127-14"><a href="lesson-4b-regularized-regression.html#cb127-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-15"><a href="lesson-4b-regularized-regression.html#cb127-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: create 5-fold resampling object</span></span>
<span id="cb127-16"><a href="lesson-4b-regularized-regression.html#cb127-16" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb127-17"><a href="lesson-4b-regularized-regression.html#cb127-17" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(______, <span class="at">v =</span> <span class="dv">5</span>, <span class="at">strata =</span> _______)</span>
<span id="cb127-18"><a href="lesson-4b-regularized-regression.html#cb127-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-19"><a href="lesson-4b-regularized-regression.html#cb127-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: fit model across resampling object and collect results</span></span>
<span id="cb127-20"><a href="lesson-4b-regularized-regression.html#cb127-20" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb127-21"><a href="lesson-4b-regularized-regression.html#cb127-21" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(________) <span class="sc">%&gt;%</span></span>
<span id="cb127-22"><a href="lesson-4b-regularized-regression.html#cb127-22" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(________) <span class="sc">%&gt;%</span></span>
<span id="cb127-23"><a href="lesson-4b-regularized-regression.html#cb127-23" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(________) <span class="sc">%&gt;%</span></span>
<span id="cb127-24"><a href="lesson-4b-regularized-regression.html#cb127-24" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb127-25"><a href="lesson-4b-regularized-regression.html#cb127-25" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span></code></pre></div>
</div>
</div>
<div id="tuning-our-model" class="section level2 hasAnchor" number="12.8">
<h2><span class="header-section-number">12.8</span> Tuning our model<a href="lesson-4b-regularized-regression.html#tuning-our-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="video">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1492301/sp/149230100/embedIframeJs/uiconf_id/49148882/partner_id/1492301?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_blkns6dp&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_8sf9yl7y" width="640" height="610" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="BANA 4080 - Regularization tuning">
</iframe>
</div>
<p>It’s important to note that there are two main parameters that we, the data scientists, control setting the values for:</p>
<ul>
<li><code>mixture</code>: the type of regularization (ridge, lasso, elastic net) we want to apply and,</li>
<li><code>penalty</code>: the strength of the regularization parameter, which we referred to as <span class="math inline">\(\lambda\)</span> in earlier sections.</li>
</ul>
<p>These parameters can be thought of as <em>“the knobs to twiddle”</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> and we often refer to them as <strong><em>hyperparameters</em></strong>.</p>
<div class="note">
<p>
Hyperparameters (aka <em>tuning parameters</em>) are parameters that
we can use to control the complexity of machine learning algorithms. Not
all algorithms have hyperparameters (e.g., ordinary least squares);
however, more advanced algorithms have at least one or more.
</p>
</div>
<p>The proper setting of these hyperparameters is often dependent on the data and problem at hand and cannot always be estimated by the training data alone. Consequently, we often go through iterations of testing out different values to determine which hyperparameter settings provide the optimal result.</p>
<div class="note">
<p>
In the next module we’ll see how we can automate the hyperparameter
tuning process. For now, we’ll simply take a manual approach.
</p>
</div>
<div id="tuning-regularization-strength" class="section level3 hasAnchor" number="12.8.1">
<h3><span class="header-section-number">12.8.1</span> Tuning regularization strength<a href="lesson-4b-regularized-regression.html#tuning-regularization-strength" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, we’ll asses how the regularization strength impacts the performance of our model. In the two examples that follow we will use all the features in our Ames housing data; however, we’ll stick to the basic preprocessing of standardizing our numeric features and dummy encoding our categorical features.</p>
<p>For both models we still use a ridge model (<code>mixture = 0</code>) but we assess two different values for <span class="math inline">\(\lambda\)</span> (<code>penalty</code>). Note how increasing the <code>penalty</code> leads to a lower RMSE.</p>
<div class="note">
<p>
Having the larger <code>penalty</code> will not always lead to a
lower RMSE, it just happens to be the case for this data set. This is
likely because we have high collinearity across our predictors so a
larger <code>penalty</code> is helping to constrain the coefficients and
make them more stable.
</p>
</div>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="lesson-4b-regularized-regression.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb128-2"><a href="lesson-4b-regularized-regression.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge model with penalty = 10000 # </span></span>
<span id="cb128-3"><a href="lesson-4b-regularized-regression.html#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb128-4"><a href="lesson-4b-regularized-regression.html#cb128-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-5"><a href="lesson-4b-regularized-regression.html#cb128-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create linear model object</span></span>
<span id="cb128-6"><a href="lesson-4b-regularized-regression.html#cb128-6" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">10000</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb128-7"><a href="lesson-4b-regularized-regression.html#cb128-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb128-8"><a href="lesson-4b-regularized-regression.html#cb128-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-9"><a href="lesson-4b-regularized-regression.html#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb128-10"><a href="lesson-4b-regularized-regression.html#cb128-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb128-11"><a href="lesson-4b-regularized-regression.html#cb128-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(ridge_mod) <span class="sc">%&gt;%</span></span>
<span id="cb128-12"><a href="lesson-4b-regularized-regression.html#cb128-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds) <span class="sc">%&gt;%</span></span>
<span id="cb128-13"><a href="lesson-4b-regularized-regression.html#cb128-13" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb128-14"><a href="lesson-4b-regularized-regression.html#cb128-14" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb128-15"><a href="lesson-4b-regularized-regression.html#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 6</span></span>
<span id="cb128-16"><a href="lesson-4b-regularized-regression.html#cb128-16" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator   mean     n std_err .config             </span></span>
<span id="cb128-17"><a href="lesson-4b-regularized-regression.html#cb128-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb128-18"><a href="lesson-4b-regularized-regression.html#cb128-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   30999.     5   2816. Preprocessor1_Model1</span></span></code></pre></div>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="lesson-4b-regularized-regression.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb129-2"><a href="lesson-4b-regularized-regression.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge model with penalty = 20000 # </span></span>
<span id="cb129-3"><a href="lesson-4b-regularized-regression.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb129-4"><a href="lesson-4b-regularized-regression.html#cb129-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-5"><a href="lesson-4b-regularized-regression.html#cb129-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create linear model object</span></span>
<span id="cb129-6"><a href="lesson-4b-regularized-regression.html#cb129-6" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">20000</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb129-7"><a href="lesson-4b-regularized-regression.html#cb129-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb129-8"><a href="lesson-4b-regularized-regression.html#cb129-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-9"><a href="lesson-4b-regularized-regression.html#cb129-9" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb129-10"><a href="lesson-4b-regularized-regression.html#cb129-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb129-11"><a href="lesson-4b-regularized-regression.html#cb129-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(ridge_mod) <span class="sc">%&gt;%</span></span>
<span id="cb129-12"><a href="lesson-4b-regularized-regression.html#cb129-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds)</span>
<span id="cb129-13"><a href="lesson-4b-regularized-regression.html#cb129-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-14"><a href="lesson-4b-regularized-regression.html#cb129-14" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(ridge_fit) <span class="sc">%&gt;%</span></span>
<span id="cb129-15"><a href="lesson-4b-regularized-regression.html#cb129-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb129-16"><a href="lesson-4b-regularized-regression.html#cb129-16" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 6</span></span>
<span id="cb129-17"><a href="lesson-4b-regularized-regression.html#cb129-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator   mean     n std_err .config             </span></span>
<span id="cb129-18"><a href="lesson-4b-regularized-regression.html#cb129-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb129-19"><a href="lesson-4b-regularized-regression.html#cb129-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   30678.     5   2598. Preprocessor1_Model1</span></span></code></pre></div>
</div>
<div id="tuning-regularization-type" class="section level3 hasAnchor" number="12.8.2">
<h3><span class="header-section-number">12.8.2</span> Tuning regularization type<a href="lesson-4b-regularized-regression.html#tuning-regularization-type" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We should also assess the type of regularization we want to apply (i.e. Ridge, Lasso, Elastic Net). It is not always definitive which type of regularization will perform best; however, often Ridge or some combination of both <span class="math inline">\(L^1\)</span> and <span class="math inline">\(L^2\)</span> (elastic net) performs best while Lasso is more useful if we desire to eliminate noisy features altogether.</p>
<p>The following uses the same <code>penalty</code> as we did previously but changes the regularization type to Lasso (<code>mixture = 1</code>) and Elastic Net with a 50-50 mixture of <span class="math inline">\(L^1\)</span> and <span class="math inline">\(L^2\)</span> (<code>mixture = 0.5</code>).</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="lesson-4b-regularized-regression.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb130-2"><a href="lesson-4b-regularized-regression.html#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso model with penalty = 20000 # </span></span>
<span id="cb130-3"><a href="lesson-4b-regularized-regression.html#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="do">####################################</span></span>
<span id="cb130-4"><a href="lesson-4b-regularized-regression.html#cb130-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-5"><a href="lesson-4b-regularized-regression.html#cb130-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create linear model object</span></span>
<span id="cb130-6"><a href="lesson-4b-regularized-regression.html#cb130-6" aria-hidden="true" tabindex="-1"></a>lasso_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">20000</span>, <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb130-7"><a href="lesson-4b-regularized-regression.html#cb130-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb130-8"><a href="lesson-4b-regularized-regression.html#cb130-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-9"><a href="lesson-4b-regularized-regression.html#cb130-9" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb130-10"><a href="lesson-4b-regularized-regression.html#cb130-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb130-11"><a href="lesson-4b-regularized-regression.html#cb130-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(lasso_mod) <span class="sc">%&gt;%</span></span>
<span id="cb130-12"><a href="lesson-4b-regularized-regression.html#cb130-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds)</span>
<span id="cb130-13"><a href="lesson-4b-regularized-regression.html#cb130-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-14"><a href="lesson-4b-regularized-regression.html#cb130-14" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(lasso_fit) <span class="sc">%&gt;%</span></span>
<span id="cb130-15"><a href="lesson-4b-regularized-regression.html#cb130-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb130-16"><a href="lesson-4b-regularized-regression.html#cb130-16" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 6</span></span>
<span id="cb130-17"><a href="lesson-4b-regularized-regression.html#cb130-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator   mean     n std_err .config             </span></span>
<span id="cb130-18"><a href="lesson-4b-regularized-regression.html#cb130-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb130-19"><a href="lesson-4b-regularized-regression.html#cb130-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   48675.     5   2034. Preprocessor1_Model1</span></span></code></pre></div>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="lesson-4b-regularized-regression.html#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="do">##########################################</span></span>
<span id="cb131-2"><a href="lesson-4b-regularized-regression.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Elastic net model with penalty = 20000 # </span></span>
<span id="cb131-3"><a href="lesson-4b-regularized-regression.html#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="do">##########################################</span></span>
<span id="cb131-4"><a href="lesson-4b-regularized-regression.html#cb131-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-5"><a href="lesson-4b-regularized-regression.html#cb131-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create linear model object</span></span>
<span id="cb131-6"><a href="lesson-4b-regularized-regression.html#cb131-6" aria-hidden="true" tabindex="-1"></a>elastic_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">20000</span>, <span class="at">mixture =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb131-7"><a href="lesson-4b-regularized-regression.html#cb131-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb131-8"><a href="lesson-4b-regularized-regression.html#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="lesson-4b-regularized-regression.html#cb131-9" aria-hidden="true" tabindex="-1"></a>elastic_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb131-10"><a href="lesson-4b-regularized-regression.html#cb131-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb131-11"><a href="lesson-4b-regularized-regression.html#cb131-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(elastic_mod) <span class="sc">%&gt;%</span></span>
<span id="cb131-12"><a href="lesson-4b-regularized-regression.html#cb131-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds)</span>
<span id="cb131-13"><a href="lesson-4b-regularized-regression.html#cb131-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-14"><a href="lesson-4b-regularized-regression.html#cb131-14" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(elastic_fit) <span class="sc">%&gt;%</span></span>
<span id="cb131-15"><a href="lesson-4b-regularized-regression.html#cb131-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb131-16"><a href="lesson-4b-regularized-regression.html#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 6</span></span>
<span id="cb131-17"><a href="lesson-4b-regularized-regression.html#cb131-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator   mean     n std_err .config             </span></span>
<span id="cb131-18"><a href="lesson-4b-regularized-regression.html#cb131-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb131-19"><a href="lesson-4b-regularized-regression.html#cb131-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   39730.     5   2433. Preprocessor1_Model1</span></span></code></pre></div>
<p>We see that in both cases the 5-fold cross validation error is higher than the Ridge model.</p>
</div>
<div id="tuning-regularization-type-strength" class="section level3 hasAnchor" number="12.8.3">
<h3><span class="header-section-number">12.8.3</span> Tuning regularization type &amp; strength<a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Unfortunately, using the same <code>penalty</code> value across the different models rarely works to find the optimal model. Not only do we need to tune the regularization type (Ridge, Lasso, Elastic Net), but we also need to tune the <code>penalty</code> value for each regularization type.</p>
<div class="note">
<p>
This becomes far more tedious and we’d rather not manually test out
many values. In the next module we’ll discuss how we can automate the
tuning process to find optimal results.
</p>
</div>
</div>
<div id="knowledge-check-23" class="section level3 hasAnchor" number="12.8.4">
<h3><span class="header-section-number">12.8.4</span> Knowledge check<a href="lesson-4b-regularized-regression.html#knowledge-check-23" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the same <code>boston</code> training data as in the previous
knowledge check, fill in the blanks to test a variety of values for…
</p>
<ol style="list-style-type: decimal">
<li>
The type of regularization. Try a Lasso model versus an elastic
net.
</li>
<li>
The value of the penalty. Does a higher or lower penalty improve
performance?
</li>
</ol>
</div>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="lesson-4b-regularized-regression.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: create ridge model object</span></span>
<span id="cb132-2"><a href="lesson-4b-regularized-regression.html#cb132-2" aria-hidden="true" tabindex="-1"></a>glm_mod <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>(<span class="at">penalty =</span> ____, <span class="at">mixture =</span> __) <span class="sc">%&gt;%</span></span>
<span id="cb132-3"><a href="lesson-4b-regularized-regression.html#cb132-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</span>
<span id="cb132-4"><a href="lesson-4b-regularized-regression.html#cb132-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-5"><a href="lesson-4b-regularized-regression.html#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: create model &amp; preprocessing recipe</span></span>
<span id="cb132-6"><a href="lesson-4b-regularized-regression.html#cb132-6" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(cmedv <span class="sc">~</span> ., <span class="at">data =</span> boston_train) <span class="sc">%&gt;%</span></span>
<span id="cb132-7"><a href="lesson-4b-regularized-regression.html#cb132-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb132-8"><a href="lesson-4b-regularized-regression.html#cb132-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-9"><a href="lesson-4b-regularized-regression.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: create 5-fold resampling object</span></span>
<span id="cb132-10"><a href="lesson-4b-regularized-regression.html#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb132-11"><a href="lesson-4b-regularized-regression.html#cb132-11" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(boston_train, <span class="at">v =</span> <span class="dv">5</span>, <span class="at">strata =</span> cmedv)</span>
<span id="cb132-12"><a href="lesson-4b-regularized-regression.html#cb132-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-13"><a href="lesson-4b-regularized-regression.html#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: fit model across resampling object and collect results</span></span>
<span id="cb132-14"><a href="lesson-4b-regularized-regression.html#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb132-15"><a href="lesson-4b-regularized-regression.html#cb132-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb132-16"><a href="lesson-4b-regularized-regression.html#cb132-16" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(glm_mod) <span class="sc">%&gt;%</span></span>
<span id="cb132-17"><a href="lesson-4b-regularized-regression.html#cb132-17" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds) <span class="sc">%&gt;%</span></span>
<span id="cb132-18"><a href="lesson-4b-regularized-regression.html#cb132-18" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb132-19"><a href="lesson-4b-regularized-regression.html#cb132-19" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">&#39;rmse&#39;</span>)</span></code></pre></div>
</div>
</div>
<div id="feature-importance-1" class="section level2 hasAnchor" number="12.9">
<h2><span class="header-section-number">12.9</span> Feature importance<a href="lesson-4b-regularized-regression.html#feature-importance-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For regularized models, importance is determined by magnitude of the standardized coefficients. Recall that ridge, lasso, and elastic net models push non-influential features to zero (or near zero). Consequently, very small coefficient values represent features that are not very important while very large coefficient values (whether negative or positive) represent very important features.</p>
<p>We’ll check out the ridge model since that model performed best. We just need to fit a final model across all the training data, extract that final fit from the workflow object, and then use the <strong>vip</strong> package to visualize the variable importance scores for the top 20 features:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="lesson-4b-regularized-regression.html#cb133-1" aria-hidden="true" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb133-2"><a href="lesson-4b-regularized-regression.html#cb133-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb133-3"><a href="lesson-4b-regularized-regression.html#cb133-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(ridge_mod) <span class="sc">%&gt;%</span></span>
<span id="cb133-4"><a href="lesson-4b-regularized-regression.html#cb133-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb133-5"><a href="lesson-4b-regularized-regression.html#cb133-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-6"><a href="lesson-4b-regularized-regression.html#cb133-6" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb133-7"><a href="lesson-4b-regularized-regression.html#cb133-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb133-8"><a href="lesson-4b-regularized-regression.html#cb133-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">vip</span>(<span class="at">num_features =</span> <span class="dv">20</span>, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-234-1.png" width="576" style="display: block; margin: auto;" /></p>
<div id="knowledge-check-24" class="section level3 hasAnchor" number="12.9.1">
<h3><span class="header-section-number">12.9.1</span> Knowledge check<a href="lesson-4b-regularized-regression.html#knowledge-check-24" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Based on the model you found to work best for the boston data…
</p>
<ol style="list-style-type: decimal">
<li>
Fit a final model to the entire training data,
</li>
<li>
Extract the final fitt from the workflow object,
</li>
<li>
Plot the top 10 influential features. Which features are most
influential in your model?
</li>
</ol>
</div>
</div>
</div>
<div id="classification-problems-1" class="section level2 hasAnchor" number="12.10">
<h2><span class="header-section-number">12.10</span> Classification problems<a href="lesson-4b-regularized-regression.html#classification-problems-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We saw that we can apply regularization to a regression problem but we can also do the same for classification problems. The following applies the same procedures we used above but for the <code>kernlab::spam</code> data.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="lesson-4b-regularized-regression.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb134-2"><a href="lesson-4b-regularized-regression.html#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(spam)</span>
<span id="cb134-3"><a href="lesson-4b-regularized-regression.html#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="lesson-4b-regularized-regression.html#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: create train and test splits</span></span>
<span id="cb134-5"><a href="lesson-4b-regularized-regression.html#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb134-6"><a href="lesson-4b-regularized-regression.html#cb134-6" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(spam, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;type&quot;</span>)</span>
<span id="cb134-7"><a href="lesson-4b-regularized-regression.html#cb134-7" aria-hidden="true" tabindex="-1"></a>spam_train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb134-8"><a href="lesson-4b-regularized-regression.html#cb134-8" aria-hidden="true" tabindex="-1"></a>spam_test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span>
<span id="cb134-9"><a href="lesson-4b-regularized-regression.html#cb134-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-10"><a href="lesson-4b-regularized-regression.html#cb134-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: create ridge model object</span></span>
<span id="cb134-11"><a href="lesson-4b-regularized-regression.html#cb134-11" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">&lt;-</span> <span class="fu">logistic_reg</span>(<span class="at">penalty =</span> <span class="dv">100</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb134-12"><a href="lesson-4b-regularized-regression.html#cb134-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb134-13"><a href="lesson-4b-regularized-regression.html#cb134-13" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb134-14"><a href="lesson-4b-regularized-regression.html#cb134-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-15"><a href="lesson-4b-regularized-regression.html#cb134-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: create model &amp; preprocessing recipe</span></span>
<span id="cb134-16"><a href="lesson-4b-regularized-regression.html#cb134-16" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(type <span class="sc">~</span> ., <span class="at">data =</span> spam_train) <span class="sc">%&gt;%</span></span>
<span id="cb134-17"><a href="lesson-4b-regularized-regression.html#cb134-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_numeric_predictors</span>())</span>
<span id="cb134-18"><a href="lesson-4b-regularized-regression.html#cb134-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-19"><a href="lesson-4b-regularized-regression.html#cb134-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: create workflow object to combine the recipe &amp; model</span></span>
<span id="cb134-20"><a href="lesson-4b-regularized-regression.html#cb134-20" aria-hidden="true" tabindex="-1"></a>spam_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb134-21"><a href="lesson-4b-regularized-regression.html#cb134-21" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb134-22"><a href="lesson-4b-regularized-regression.html#cb134-22" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(ridge_mod)</span>
<span id="cb134-23"><a href="lesson-4b-regularized-regression.html#cb134-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-24"><a href="lesson-4b-regularized-regression.html#cb134-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: fit model across resampling object and collect results</span></span>
<span id="cb134-25"><a href="lesson-4b-regularized-regression.html#cb134-25" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb134-26"><a href="lesson-4b-regularized-regression.html#cb134-26" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(spam_train, <span class="at">v =</span> <span class="dv">5</span>, <span class="at">strata =</span> <span class="st">&quot;type&quot;</span>)</span>
<span id="cb134-27"><a href="lesson-4b-regularized-regression.html#cb134-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-28"><a href="lesson-4b-regularized-regression.html#cb134-28" aria-hidden="true" tabindex="-1"></a>spam_wf <span class="sc">%&gt;%</span></span>
<span id="cb134-29"><a href="lesson-4b-regularized-regression.html#cb134-29" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit_resamples</span>(kfolds) <span class="sc">%&gt;%</span></span>
<span id="cb134-30"><a href="lesson-4b-regularized-regression.html#cb134-30" aria-hidden="true" tabindex="-1"></a>   <span class="fu">collect_metrics</span>()</span>
<span id="cb134-31"><a href="lesson-4b-regularized-regression.html#cb134-31" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb134-32"><a href="lesson-4b-regularized-regression.html#cb134-32" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric  .estimator  mean     n  std_err .config             </span></span>
<span id="cb134-33"><a href="lesson-4b-regularized-regression.html#cb134-33" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb134-34"><a href="lesson-4b-regularized-regression.html#cb134-34" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 accuracy binary     0.606     5 0.000197 Preprocessor1_Model1</span></span>
<span id="cb134-35"><a href="lesson-4b-regularized-regression.html#cb134-35" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 roc_auc  binary     0.946     5 0.00274  Preprocessor1_Model1</span></span></code></pre></div>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="lesson-4b-regularized-regression.html#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Fit final model on all training data</span></span>
<span id="cb135-2"><a href="lesson-4b-regularized-regression.html#cb135-2" aria-hidden="true" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> spam_wf <span class="sc">%&gt;%</span></span>
<span id="cb135-3"><a href="lesson-4b-regularized-regression.html#cb135-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit</span>(<span class="at">data =</span> spam_train)</span>
<span id="cb135-4"><a href="lesson-4b-regularized-regression.html#cb135-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-5"><a href="lesson-4b-regularized-regression.html#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7: Assess top 20 most influential features</span></span>
<span id="cb135-6"><a href="lesson-4b-regularized-regression.html#cb135-6" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb135-7"><a href="lesson-4b-regularized-regression.html#cb135-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb135-8"><a href="lesson-4b-regularized-regression.html#cb135-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">vip</span>(<span class="at">num_features =</span> <span class="dv">20</span>, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-237-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="final-thoughts-1" class="section level2 hasAnchor" number="12.11">
<h2><span class="header-section-number">12.11</span> Final thoughts<a href="lesson-4b-regularized-regression.html#final-thoughts-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regularized regression provides many great benefits over traditional GLMs when applied to large data sets with lots of features. It provides a great option for handling the <span class="math inline">\(n &gt; p\)</span> problem, helps minimize the impact of multicollinearity, and can perform automated feature selection. It also has relatively few hyperparameters which makes them easy to tune, computationally efficient compared to other algorithms discussed in later modules, and memory efficient.</p>
<p>However, similar to GLMs, they are not robust to outliers in both the feature and target. Also, regularized regression models still assume a monotonic linear relationship (always increasing or decreasing in a linear fashion). It is also up to the analyst whether or not to include specific interaction effects.</p>
</div>
<div id="exercises-7" class="section level2 hasAnchor" number="12.12">
<h2><span class="header-section-number">12.12</span> Exercises<a href="lesson-4b-regularized-regression.html#exercises-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Recall the <code>Advertising.csv</code> dataset we used in the <a
href="https://bradleyboehmke.github.io/uc-bana-4080/lesson-3b-resampling.html#exercises-5">Resampling
lesson</a>. Use this same data (available via Canvas) to…
</p>
<ol style="list-style-type: decimal">
<li>
Split the data into 70-30 training-test sets.
</li>
<li>
Apply a ridge model with <code>Sales</code> being the response
variable. Perform a cross-validation procedure and test the model across
various penalty parameter values. Which penalty parameter value resulted
in the lowest RMSE?
</li>
<li>
Repeat #2 but changing the regularization type to Lasso and Elastic
net. Which regularization type results in the lowest RMSE?
</li>
<li>
Using the best model from those that you tested out in #2 and #3,
plot the feature importance. Which feature is most influential? Which is
least influential?
</li>
</ol>
</div>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-breiman2001statistical" class="csl-entry">
Breiman, Leo et al. 2001. <span>“Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).”</span> <em>Statistical Science</em> 16 (3): 199–231.
</div>
<div id="ref-esl" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer Series in Statistics New York, NY, USA:
</div>
<div id="ref-hastie2015statistical" class="csl-entry">
Hastie, T., R. Tibshirani, and M. Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E, and Robert W Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
<div id="ref-apm" class="csl-entry">
Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 26. Springer.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 267–88.
</div>
<div id="ref-zou2005regularization" class="csl-entry">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>This phrase comes from Brad Efron’s comments in <span class="citation">Breiman et al. (<a href="#ref-breiman2001statistical" role="doc-biblioref">2001</a>)</span><a href="lesson-4b-regularized-regression.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lesson-4a-logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overview-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-4/lesson-2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
