<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>19 Lesson 6c: Random Forests | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="19 Lesson 6c: Random Forests | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="19 Lesson 6c: Random Forests | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lesson-6b-bagging.html"/>
<link rel="next" href="computing-environment.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.9/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing</a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes</a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers</a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing</a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition</a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 5</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>15</b> Lesson 5b: Multivariate Adaptive Regression Splines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#learning-objectives-15"><i class="fa fa-check"></i><b>15.1</b> Learning objectives</a></li>
<li class="chapter" data-level="15.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#prerequisites-8"><i class="fa fa-check"></i><b>15.2</b> Prerequisites</a></li>
<li class="chapter" data-level="15.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#nonlinearity"><i class="fa fa-check"></i><b>15.3</b> Nonlinearity</a></li>
<li class="chapter" data-level="15.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>15.4</b> Multivariate adaptive regression splines</a></li>
<li class="chapter" data-level="15.5" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-mars-model"><i class="fa fa-check"></i><b>15.5</b> Fitting a MARS model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-basic-model"><i class="fa fa-check"></i><b>15.5.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="15.5.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model"><i class="fa fa-check"></i><b>15.5.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="15.5.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model-with-interactions"><i class="fa fa-check"></i><b>15.5.3</b> Fitting a full model with interactions</a></li>
<li class="chapter" data-level="15.5.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-25"><i class="fa fa-check"></i><b>15.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#tuning-1"><i class="fa fa-check"></i><b>15.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-26"><i class="fa fa-check"></i><b>15.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#feature-interpretation-1"><i class="fa fa-check"></i><b>15.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="15.8" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#final-thoughts-2"><i class="fa fa-check"></i><b>15.8</b> Final thoughts</a></li>
<li class="chapter" data-level="15.9" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#exercises-9"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Module 6</b></span></li>
<li class="chapter" data-level="16" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i><b>16</b> Overview</a>
<ul>
<li class="chapter" data-level="16.1" data-path="overview-5.html"><a href="overview-5.html#learning-objectives-16"><i class="fa fa-check"></i><b>16.1</b> Learning objectives</a></li>
<li class="chapter" data-level="16.2" data-path="overview-5.html"><a href="overview-5.html#estimated-time-requirement-5"><i class="fa fa-check"></i><b>16.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="16.3" data-path="overview-5.html"><a href="overview-5.html#tasks-5"><i class="fa fa-check"></i><b>16.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html"><i class="fa fa-check"></i><b>17</b> Lesson 6a: Decision Trees</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#learning-objectives-17"><i class="fa fa-check"></i><b>17.1</b> Learning objectives</a></li>
<li class="chapter" data-level="17.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#prerequisites-9"><i class="fa fa-check"></i><b>17.2</b> Prerequisites</a></li>
<li class="chapter" data-level="17.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#structure"><i class="fa fa-check"></i><b>17.3</b> Structure</a></li>
<li class="chapter" data-level="17.4" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#partitioning"><i class="fa fa-check"></i><b>17.4</b> Partitioning</a></li>
<li class="chapter" data-level="17.5" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#how-deep"><i class="fa fa-check"></i><b>17.5</b> How deep?</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#early-stopping"><i class="fa fa-check"></i><b>17.5.1</b> Early stopping</a></li>
<li class="chapter" data-level="17.5.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#pruning"><i class="fa fa-check"></i><b>17.5.2</b> Pruning</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-decision-tree"><i class="fa fa-check"></i><b>17.6</b> Fitting a decision tree</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1"><i class="fa fa-check"></i><b>17.6.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="17.6.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-full-model-1"><i class="fa fa-check"></i><b>17.6.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="17.6.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-27"><i class="fa fa-check"></i><b>17.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#tuning-2"><i class="fa fa-check"></i><b>17.7</b> Tuning</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-28"><i class="fa fa-check"></i><b>17.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#feature-interpretation-2"><i class="fa fa-check"></i><b>17.8</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-29"><i class="fa fa-check"></i><b>17.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#final-thoughts-3"><i class="fa fa-check"></i><b>17.9</b> Final thoughts</a></li>
<li class="chapter" data-level="17.10" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>17.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html"><i class="fa fa-check"></i><b>18</b> Lesson 6b: Bagging</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#learning-objectives-18"><i class="fa fa-check"></i><b>18.1</b> Learning objectives</a></li>
<li class="chapter" data-level="18.2" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#prerequisites-10"><i class="fa fa-check"></i><b>18.2</b> Prerequisites</a></li>
<li class="chapter" data-level="18.3" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#why-and-when-bagging-works"><i class="fa fa-check"></i><b>18.3</b> Why and when bagging works</a></li>
<li class="chapter" data-level="18.4" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#fitting-a-decision-tree-model"><i class="fa fa-check"></i><b>18.4</b> Fitting a decision tree model</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-30"><i class="fa fa-check"></i><b>18.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#tuning-3"><i class="fa fa-check"></i><b>18.5</b> Tuning</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-31"><i class="fa fa-check"></i><b>18.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#feature-interpretation-3"><i class="fa fa-check"></i><b>18.6</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-32"><i class="fa fa-check"></i><b>18.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#final-thoughts-4"><i class="fa fa-check"></i><b>18.7</b> Final thoughts</a></li>
<li class="chapter" data-level="18.8" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#exercises-11"><i class="fa fa-check"></i><b>18.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html"><i class="fa fa-check"></i><b>19</b> Lesson 6c: Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#learning-objectives-19"><i class="fa fa-check"></i><b>19.1</b> Learning objectives</a></li>
<li class="chapter" data-level="19.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#prerequisites-11"><i class="fa fa-check"></i><b>19.2</b> Prerequisites</a></li>
<li class="chapter" data-level="19.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#extending-bagging"><i class="fa fa-check"></i><b>19.3</b> Extending bagging</a></li>
<li class="chapter" data-level="19.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#out-of-the-box-performance"><i class="fa fa-check"></i><b>19.4</b> Out-of-the-box performance</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-33"><i class="fa fa-check"></i><b>19.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#hyperparameters"><i class="fa fa-check"></i><b>19.5</b> Hyperparameters</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#number-of-trees"><i class="fa fa-check"></i><b>19.5.1</b> Number of trees</a></li>
<li class="chapter" data-level="19.5.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#mtry"><i class="fa fa-check"></i><b>19.5.2</b> <span class="math inline">\(m_{try}\)</span></a></li>
<li class="chapter" data-level="19.5.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tree-complexity"><i class="fa fa-check"></i><b>19.5.3</b> Tree complexity</a></li>
<li class="chapter" data-level="19.5.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#others"><i class="fa fa-check"></i><b>19.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tuning-4"><i class="fa fa-check"></i><b>19.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="19.6.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-34"><i class="fa fa-check"></i><b>19.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#feature-interpretation-4"><i class="fa fa-check"></i><b>19.7</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="19.7.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-35"><i class="fa fa-check"></i><b>19.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#final-thoughts-5"><i class="fa fa-check"></i><b>19.8</b> Final thoughts</a></li>
<li class="chapter" data-level="19.9" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#exercises-12"><i class="fa fa-check"></i><b>19.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VII Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-6c-random-forests" class="section level1 hasAnchor" number="19">
<h1><span class="header-section-number">19</span> Lesson 6c: Random Forests<a href="lesson-6c-random-forests.html#lesson-6c-random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Random forests</em> are a modification of bagged decision trees that build a large collection of <em>de-correlated</em> trees to further improve predictive performance. They have become a very popular “out-of-the-box” or “off-the-shelf” learning algorithm that enjoys good predictive performance with relatively little hyperparameter tuning. Many modern implementations of random forests exist; however, Leo Breiman’s algorithm <span class="citation">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> has largely become the authoritative procedure. This module will cover the fundamentals of random forests.</p>
<div id="learning-objectives-19" class="section level2 hasAnchor" number="19.1">
<h2><span class="header-section-number">19.1</span> Learning objectives<a href="lesson-6c-random-forests.html#learning-objectives-19" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module you will know:</p>
<ul>
<li>How to implement a random forest model along with the hyperparameters that are commonly toggled in these algorithms.</li>
<li>Multiple strategies for performing a grid search.</li>
<li>How to identify influential features and their effects on the response variable.</li>
</ul>
</div>
<div id="prerequisites-11" class="section level2 hasAnchor" number="19.2">
<h2><span class="header-section-number">19.2</span> Prerequisites<a href="lesson-6c-random-forests.html#prerequisites-11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="lesson-6c-random-forests.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper packages</span></span>
<span id="cb177-2"><a href="lesson-6c-random-forests.html#cb177-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)   <span class="co"># for data wrangling &amp; plotting</span></span>
<span id="cb177-3"><a href="lesson-6c-random-forests.html#cb177-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-4"><a href="lesson-6c-random-forests.html#cb177-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Modeling packages</span></span>
<span id="cb177-5"><a href="lesson-6c-random-forests.html#cb177-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels) </span>
<span id="cb177-6"><a href="lesson-6c-random-forests.html#cb177-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-7"><a href="lesson-6c-random-forests.html#cb177-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Model interpretability packages</span></span>
<span id="cb177-8"><a href="lesson-6c-random-forests.html#cb177-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)         <span class="co"># for variable importance</span></span>
<span id="cb177-9"><a href="lesson-6c-random-forests.html#cb177-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)         <span class="co"># for variable relationships</span></span></code></pre></div>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="lesson-6c-random-forests.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb178-2"><a href="lesson-6c-random-forests.html#cb178-2" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span>
<span id="cb178-3"><a href="lesson-6c-random-forests.html#cb178-3" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)</span>
<span id="cb178-4"><a href="lesson-6c-random-forests.html#cb178-4" aria-hidden="true" tabindex="-1"></a>ames_train  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(split)</span>
<span id="cb178-5"><a href="lesson-6c-random-forests.html#cb178-5" aria-hidden="true" tabindex="-1"></a>ames_test   <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="extending-bagging" class="section level2 hasAnchor" number="19.3">
<h2><span class="header-section-number">19.3</span> Extending bagging<a href="lesson-6c-random-forests.html#extending-bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests are built using the same fundamental principles as decision trees and bagging. Bagging trees introduces a random component into the tree building process by building many trees on bootstrapped copies of the training data. Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance. However, as we saw in the last module, simply bagging trees results in tree correlation that limits the effect of variance reduction.</p>
<p>Random forests help to reduce tree correlation by injecting more randomness into the tree-growing process.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> More specifically, while growing a decision tree during the bagging process, random forests perform <em>split-variable randomization</em> where each time a split is to be performed, the search for the split variable is limited to a random subset of <span class="math inline">\(m_{try}\)</span> of the original <span class="math inline">\(p\)</span> features. Typical default values are <span class="math inline">\(m_{try} = \frac{p}{3}\)</span> (regression) and <span class="math inline">\(m_{try} = \sqrt{p}\)</span> (classification) but this should be considered a tuning parameter.</p>
<p>The basic algorithm for a regression or classification random forest can be generalized as follows:</p>
<pre><code>1.  Given a training data set
2.  Select number of trees to build (n_trees)
3.  for i = 1 to n_trees do
4.  |  Generate a bootstrap sample of the original data
5.  |  Grow a regression/classification tree to the bootstrapped data
6.  |  for each split do
7.  |  | Select m_try variables at random from all p variables
8.  |  | Pick the best variable/split-point among the m_try
9.  |  | Split the node into two child nodes
10. |  end
11. | Use typical tree model stopping criteria to determine when a 
    | tree is complete (but do not prune)
12. end
13. Output ensemble of trees </code></pre>
<div class="tip">
<p>
When <span class="math inline"><span class="math inline">\(m_{try} = p\)</span></span>, the algorithm
is equivalent to bagging decision trees.
</p>
</div>
<p>Since the algorithm randomly selects a bootstrap sample to train on <strong><em>and</em></strong> a random sample of features to use at each split, a more diverse set of trees is produced which tends to lessen tree correlation beyond bagged trees and often dramatically increase predictive power.</p>
</div>
<div id="out-of-the-box-performance" class="section level2 hasAnchor" number="19.4">
<h2><span class="header-section-number">19.4</span> Out-of-the-box performance<a href="lesson-6c-random-forests.html#out-of-the-box-performance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests have become popular because they tend to provide very good out-of-the-box performance. Although they have several hyperparameters that can be tuned, the default values tend to produce good results. Moreover, <span class="citation">Probst, Bischl, and Boulesteix (<a href="#ref-probst2018tunability" role="doc-biblioref">2018</a>)</span> illustrated that among the more popular machine learning algorithms, random forests have the least variability in their prediction accuracy when tuning.</p>
<p>For example, if we train a random forest model with all hyperparameters set to their default values, we get RMSEs comparable to some of the best model’s we’ve run thus far (without any tuning).</p>
<div class="note">
<p>
In R we will want to use the <a
href="https://github.com/imbs-hl/ranger">ranger</a> package as our
random forest engine. Similar to other examples we need to set the mode
of machine learning model to either a regression or classification
modeling objective.
</p>
</div>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="lesson-6c-random-forests.html#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model recipe with all features</span></span>
<span id="cb180-2"><a href="lesson-6c-random-forests.html#cb180-2" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(</span>
<span id="cb180-3"><a href="lesson-6c-random-forests.html#cb180-3" aria-hidden="true" tabindex="-1"></a>    Sale_Price <span class="sc">~</span> ., </span>
<span id="cb180-4"><a href="lesson-6c-random-forests.html#cb180-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ames_train</span>
<span id="cb180-5"><a href="lesson-6c-random-forests.html#cb180-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb180-6"><a href="lesson-6c-random-forests.html#cb180-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-7"><a href="lesson-6c-random-forests.html#cb180-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create bagged CART model object and</span></span>
<span id="cb180-8"><a href="lesson-6c-random-forests.html#cb180-8" aria-hidden="true" tabindex="-1"></a><span class="co"># start with 5 bagged trees</span></span>
<span id="cb180-9"><a href="lesson-6c-random-forests.html#cb180-9" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>(<span class="at">mode =</span> <span class="st">&quot;regression&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb180-10"><a href="lesson-6c-random-forests.html#cb180-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>)</span>
<span id="cb180-11"><a href="lesson-6c-random-forests.html#cb180-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-12"><a href="lesson-6c-random-forests.html#cb180-12" aria-hidden="true" tabindex="-1"></a><span class="co"># create resampling procedure</span></span>
<span id="cb180-13"><a href="lesson-6c-random-forests.html#cb180-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb180-14"><a href="lesson-6c-random-forests.html#cb180-14" aria-hidden="true" tabindex="-1"></a>kfold <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(ames_train, <span class="at">v =</span> <span class="dv">5</span>)</span>
<span id="cb180-15"><a href="lesson-6c-random-forests.html#cb180-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-16"><a href="lesson-6c-random-forests.html#cb180-16" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb180-17"><a href="lesson-6c-random-forests.html#cb180-17" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">fit_resamples</span>(rf_mod, model_recipe, kfold)</span>
<span id="cb180-18"><a href="lesson-6c-random-forests.html#cb180-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-19"><a href="lesson-6c-random-forests.html#cb180-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb180-20"><a href="lesson-6c-random-forests.html#cb180-20" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(results)</span>
<span id="cb180-21"><a href="lesson-6c-random-forests.html#cb180-21" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb180-22"><a href="lesson-6c-random-forests.html#cb180-22" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n    std_err .config     </span></span>
<span id="cb180-23"><a href="lesson-6c-random-forests.html#cb180-23" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;       </span></span>
<span id="cb180-24"><a href="lesson-6c-random-forests.html#cb180-24" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   26981.        5 1119.      Preprocesso…</span></span>
<span id="cb180-25"><a href="lesson-6c-random-forests.html#cb180-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.895     5    0.00862 Preprocesso…</span></span></code></pre></div>
<div id="knowledge-check-33" class="section level3 hasAnchor" number="19.4.1">
<h3><span class="header-section-number">19.4.1</span> Knowledge check<a href="lesson-6c-random-forests.html#knowledge-check-33" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset:
</p>
<p>
Apply a random forest model where <code>cmedv</code> is the response
variable and use all possible predictor variables. Use the default
hyperparameter settings and apply a 5-fold cross validation procedure to
assess to assess the model performance.
</p>
</div>
</div>
</div>
<div id="hyperparameters" class="section level2 hasAnchor" number="19.5">
<h2><span class="header-section-number">19.5</span> Hyperparameters<a href="lesson-6c-random-forests.html#hyperparameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although random forests perform well out-of-the-box, there are several tunable hyperparameters that we should consider when training a model. Although we briefly discuss the main hyperparameters, <span class="citation">Probst, Wright, and Boulesteix (<a href="#ref-probst2019hyperparameters" role="doc-biblioref">2019</a>)</span> provide a much more thorough discussion. The main hyperparameters to consider include:</p>
<ol style="list-style-type: decimal">
<li>The number of trees in the forest</li>
<li>The number of features to consider at any given split: <span class="math inline">\(m_{try}\)</span></li>
<li>The complexity (depth) of each tree</li>
</ol>
<div id="number-of-trees" class="section level3 hasAnchor" number="19.5.1">
<h3><span class="header-section-number">19.5.1</span> Number of trees<a href="lesson-6c-random-forests.html#number-of-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first consideration is the number of trees within your random forest. Although not technically a hyperparameter, the number of trees needs to be sufficiently large to stabilize the error rate. A good rule of thumb is to start with 10 times the number of features as illustrated below); however, as you adjust other hyperparameters such as <span class="math inline">\(m_{try}\)</span> and node size, more or fewer trees may be required. More trees provide more robust and stable error estimates and variable importance measures; however, the impact on computation time increases linearly with the number of trees.</p>
<div class="tip">
<p>
A good rule of thumb is to start with the number of predictor
variables (<span class="math inline"><span class="math inline">\(p\)</span></span>) times 10 (<span class="math inline"><span class="math inline">\(p \times 10\)</span></span>) trees and adjust as
necessary.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tuning-trees"></span>
<img src="_main_files/figure-html/tuning-trees-1.png" alt="The Ames data has 80 features and starting with 10 times the number of features typically ensures the error estimate converges." width="576" />
<p class="caption">
Figure 19.1: The Ames data has 80 features and starting with 10 times the number of features typically ensures the error estimate converges.
</p>
</div>
</div>
<div id="mtry" class="section level3 hasAnchor" number="19.5.2">
<h3><span class="header-section-number">19.5.2</span> <span class="math inline">\(m_{try}\)</span><a href="lesson-6c-random-forests.html#mtry" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The hyperparameter that controls the split-variable randomization feature of random forests is often referred to as <span class="math inline">\(m_{try}\)</span> and it helps to balance low tree correlation with reasonable predictive strength. With regression problems the default value is often <span class="math inline">\(m_{try} = \frac{p}{3}\)</span> and for classification <span class="math inline">\(m_{try} = \sqrt{p}\)</span>. However, when there are fewer relevant predictors (e.g., noisy data) a higher value of <span class="math inline">\(m_{try}\)</span> tends to perform better because it makes it more likely to select those features with the strongest signal. When there are many relevant predictors, a lower <span class="math inline">\(m_{try}\)</span> might perform better.</p>
<div class="tip">
<p>
Start with five evenly spaced values of <span class="math inline"><span class="math inline">\(m_{try}\)</span></span> across the range 2–<span class="math inline"><span class="math inline">\(p\)</span></span> centered at the recommended default as
illustrated below. For the Ames data, an mtry value slightly lower (21)
than the default (26) improves performance.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tuning-mtry"></span>
<img src="_main_files/figure-html/tuning-mtry-1.png" alt="For the Ames data, an mtry value in the low to mid 20s improves performance." width="576" />
<p class="caption">
Figure 19.2: For the Ames data, an mtry value in the low to mid 20s improves performance.
</p>
</div>
</div>
<div id="tree-complexity" class="section level3 hasAnchor" number="19.5.3">
<h3><span class="header-section-number">19.5.3</span> Tree complexity<a href="lesson-6c-random-forests.html#tree-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random forests are built on individual decision trees; consequently, most random forest implementations have one or more hyperparameters that allow us to control the depth and complexity of the individual trees. This will often include hyperparameters such as node size, max depth, max number of terminal nodes, or the required node size to allow additional splits. Node size is probably the most common hyperparameter to control tree complexity and most implementations use the default values of one for classification and five for regression as these values tend to produce good results <span class="citation">(<a href="#ref-diaz2006gene" role="doc-biblioref">Dı́az-Uriarte and De Andres 2006</a>; <a href="#ref-goldstein2011random" role="doc-biblioref">Goldstein, Polley, and Briggs 2011</a>)</span>. However, <span class="citation">Segal (<a href="#ref-segal2004machine" role="doc-biblioref">2004</a>)</span> showed that if your data has many noisy predictors and higher <span class="math inline">\(m_{try}\)</span> values are performing best, then performance may improve by increasing node size (i.e., decreasing tree depth and complexity). Moreover, if computation time is a concern then you can often decrease run time substantially by increasing the node size and have only marginal impacts to your error estimate as illustrated below.</p>
<div class="tip">
<p>
When adjusting node size start with three values between 1–10 and
adjust depending on impact to accuracy and run time. Increasing node
size to reduce tree complexity will often have a larger impact on
computation speed (right) than on your error estimate.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tuning-node-size"></span>
<img src="_main_files/figure-html/tuning-node-size-1.png" alt="Increasing node size to reduce tree complexity will often have a larger impact on computation speed (right) than on your error estimate." width="960" />
<p class="caption">
Figure 19.3: Increasing node size to reduce tree complexity will often have a larger impact on computation speed (right) than on your error estimate.
</p>
</div>
</div>
<div id="others" class="section level3 hasAnchor" number="19.5.4">
<h3><span class="header-section-number">19.5.4</span> Others<a href="lesson-6c-random-forests.html#others" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many other hyperparameters within random forest models; however, the above mentioned ones are the most common and, often, most influential in the performance of our model. For more discussion around random forest hyperparameters see <span class="citation">Probst, Wright, and Boulesteix (<a href="#ref-probst2019hyperparameters" role="doc-biblioref">2019</a>)</span>.</p>
</div>
</div>
<div id="tuning-4" class="section level2 hasAnchor" number="19.6">
<h2><span class="header-section-number">19.6</span> Tuning<a href="lesson-6c-random-forests.html#tuning-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following performs a grid search over the <code>mtry</code> (number of features to randomly use for a given tree) and <code>min_n</code> (controls tree depth) hyperparameters. Notice how we don’t actually tune the <code>trees</code> parameter. Rather, setting this to a value greater than the number of features <span class="math inline">\(\times\)</span> 10 is sufficient. Since we have 80 features we set it to at least, if not greater than <span class="math inline">\(80 \times 10 = 800\)</span>.</p>
<div class="warning">
<p>
The following grid search results in a search of 25 different
hyperparameter combinations, which results in a grid search time of
about 14 minutes!
</p>
</div>
<div class="note">
<p>
Also, note the <code>importance = “impurity”</code> code we added to
<code>set_engine()</code>. We’ll discuss why we add this shortly.
</p>
</div>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="lesson-6c-random-forests.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model recipe with all features</span></span>
<span id="cb181-2"><a href="lesson-6c-random-forests.html#cb181-2" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(</span>
<span id="cb181-3"><a href="lesson-6c-random-forests.html#cb181-3" aria-hidden="true" tabindex="-1"></a>    Sale_Price <span class="sc">~</span> ., </span>
<span id="cb181-4"><a href="lesson-6c-random-forests.html#cb181-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ames_train</span>
<span id="cb181-5"><a href="lesson-6c-random-forests.html#cb181-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb181-6"><a href="lesson-6c-random-forests.html#cb181-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-7"><a href="lesson-6c-random-forests.html#cb181-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create random forest model object with tuning option</span></span>
<span id="cb181-8"><a href="lesson-6c-random-forests.html#cb181-8" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>(</span>
<span id="cb181-9"><a href="lesson-6c-random-forests.html#cb181-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">&quot;regression&quot;</span>, </span>
<span id="cb181-10"><a href="lesson-6c-random-forests.html#cb181-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="dv">1000</span>,</span>
<span id="cb181-11"><a href="lesson-6c-random-forests.html#cb181-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb181-12"><a href="lesson-6c-random-forests.html#cb181-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb181-13"><a href="lesson-6c-random-forests.html#cb181-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb181-14"><a href="lesson-6c-random-forests.html#cb181-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>, <span class="at">importance =</span> <span class="st">&quot;permutation&quot;</span>)</span>
<span id="cb181-15"><a href="lesson-6c-random-forests.html#cb181-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-16"><a href="lesson-6c-random-forests.html#cb181-16" aria-hidden="true" tabindex="-1"></a><span class="co"># create the hyperparameter grid</span></span>
<span id="cb181-17"><a href="lesson-6c-random-forests.html#cb181-17" aria-hidden="true" tabindex="-1"></a>hyper_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(</span>
<span id="cb181-18"><a href="lesson-6c-random-forests.html#cb181-18" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mtry</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">80</span>)),</span>
<span id="cb181-19"><a href="lesson-6c-random-forests.html#cb181-19" aria-hidden="true" tabindex="-1"></a>   <span class="fu">min_n</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)),        </span>
<span id="cb181-20"><a href="lesson-6c-random-forests.html#cb181-20" aria-hidden="true" tabindex="-1"></a>   <span class="at">levels =</span> <span class="dv">5</span></span>
<span id="cb181-21"><a href="lesson-6c-random-forests.html#cb181-21" aria-hidden="true" tabindex="-1"></a>   )</span>
<span id="cb181-22"><a href="lesson-6c-random-forests.html#cb181-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-23"><a href="lesson-6c-random-forests.html#cb181-23" aria-hidden="true" tabindex="-1"></a><span class="co"># train our model across the hyper parameter grid</span></span>
<span id="cb181-24"><a href="lesson-6c-random-forests.html#cb181-24" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb181-25"><a href="lesson-6c-random-forests.html#cb181-25" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(rf_mod, model_recipe, <span class="at">resamples =</span> kfold, <span class="at">grid =</span> hyper_grid)</span>
<span id="cb181-26"><a href="lesson-6c-random-forests.html#cb181-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-27"><a href="lesson-6c-random-forests.html#cb181-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-28"><a href="lesson-6c-random-forests.html#cb181-28" aria-hidden="true" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb181-29"><a href="lesson-6c-random-forests.html#cb181-29" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(results, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb181-30"><a href="lesson-6c-random-forests.html#cb181-30" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 5 × 8</span></span>
<span id="cb181-31"><a href="lesson-6c-random-forests.html#cb181-31" aria-hidden="true" tabindex="-1"></a><span class="do">##    mtry min_n .metric .estima…¹   mean     n std_err .config</span></span>
<span id="cb181-32"><a href="lesson-6c-random-forests.html#cb181-32" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  </span></span>
<span id="cb181-33"><a href="lesson-6c-random-forests.html#cb181-33" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    41     1 rmse    standard  25988.     5   1021. Prepro…</span></span>
<span id="cb181-34"><a href="lesson-6c-random-forests.html#cb181-34" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    60     1 rmse    standard  26151.     5   1106. Prepro…</span></span>
<span id="cb181-35"><a href="lesson-6c-random-forests.html#cb181-35" aria-hidden="true" tabindex="-1"></a><span class="do">## 3    21     5 rmse    standard  26167.     5   1109. Prepro…</span></span>
<span id="cb181-36"><a href="lesson-6c-random-forests.html#cb181-36" aria-hidden="true" tabindex="-1"></a><span class="do">## 4    21     1 rmse    standard  26207.     5   1113. Prepro…</span></span>
<span id="cb181-37"><a href="lesson-6c-random-forests.html#cb181-37" aria-hidden="true" tabindex="-1"></a><span class="do">## 5    41     5 rmse    standard  26223.     5   1052. Prepro…</span></span>
<span id="cb181-38"><a href="lesson-6c-random-forests.html#cb181-38" aria-hidden="true" tabindex="-1"></a><span class="do">## # … with abbreviated variable name ¹​.estimator</span></span></code></pre></div>
<div id="knowledge-check-34" class="section level3 hasAnchor" number="19.6.1">
<h3><span class="header-section-number">19.6.1</span> Knowledge check<a href="lesson-6c-random-forests.html#knowledge-check-34" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset:
</p>
<p>
Apply a random forest model where <code>cmedv</code> is the response
variable and use all possible predictor variables. Use a 5-fold cross
validation procedure and tune <code>mtry</code>, <code>min_n</code>, and
<code>trees</code>. Assess 3 levels of each hyperparameter ranging
from:
</p>
<ul>
<li>
<code>trees</code>: use a range from 50-500
</li>
<li>
<code>mtry</code>: use a range from 2-15
</li>
<li>
<code>min_n</code>: use a range 1-10
</li>
</ul>
<p>
Which combination of hyperparameters perform best. What is the lowest
cross-validated RMSE and how does this compare to previous models on the
boston data?
</p>
</div>
</div>
</div>
<div id="feature-interpretation-4" class="section level2 hasAnchor" number="19.7">
<h2><span class="header-section-number">19.7</span> Feature interpretation<a href="lesson-6c-random-forests.html#feature-interpretation-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Computing feature importance and feature effects for random forests follow the same procedure as discussed in the bagging module. For each tree in our random forest, we compute the sum of the reduction of the loss function across all splits for a given predictor variable. We then aggregate this measure across all trees for each feature. The features with the largest average decrease in SSE (for regression) are considered most important.</p>
<div class="note">
<p>
This is called the “impurity” method for computing feature
importance. And to get this measure for our random forests we need to
add <code>importance = “impurity”</code> to <code>set_engine()</code> as
we did in the last section.
</p>
</div>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="lesson-6c-random-forests.html#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get optimal hyperparameters</span></span>
<span id="cb182-2"><a href="lesson-6c-random-forests.html#cb182-2" aria-hidden="true" tabindex="-1"></a>best_hyperparameters <span class="ot">&lt;-</span> <span class="fu">select_best</span>(results, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb182-3"><a href="lesson-6c-random-forests.html#cb182-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb182-4"><a href="lesson-6c-random-forests.html#cb182-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create final workflow object</span></span>
<span id="cb182-5"><a href="lesson-6c-random-forests.html#cb182-5" aria-hidden="true" tabindex="-1"></a>final_rf_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb182-6"><a href="lesson-6c-random-forests.html#cb182-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb182-7"><a href="lesson-6c-random-forests.html#cb182-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">add_model</span>(rf_mod) <span class="sc">%&gt;%</span></span>
<span id="cb182-8"><a href="lesson-6c-random-forests.html#cb182-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">finalize_workflow</span>(best_hyperparameters)</span>
<span id="cb182-9"><a href="lesson-6c-random-forests.html#cb182-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb182-10"><a href="lesson-6c-random-forests.html#cb182-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit final workflow object</span></span>
<span id="cb182-11"><a href="lesson-6c-random-forests.html#cb182-11" aria-hidden="true" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> final_rf_wf <span class="sc">%&gt;%</span></span>
<span id="cb182-12"><a href="lesson-6c-random-forests.html#cb182-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb182-13"><a href="lesson-6c-random-forests.html#cb182-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb182-14"><a href="lesson-6c-random-forests.html#cb182-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot feature importance</span></span>
<span id="cb182-15"><a href="lesson-6c-random-forests.html#cb182-15" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb182-16"><a href="lesson-6c-random-forests.html#cb182-16" aria-hidden="true" tabindex="-1"></a>   <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb182-17"><a href="lesson-6c-random-forests.html#cb182-17" aria-hidden="true" tabindex="-1"></a>   <span class="fu">vip</span>(<span class="at">num_features =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-325-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We can then plot the partial dependence of the most influential feature to see how it influences the predicted values. We see that as overall quality increase we see a significant increase in the predicted sale price.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="lesson-6c-random-forests.html#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction function</span></span>
<span id="cb183-2"><a href="lesson-6c-random-forests.html#cb183-2" aria-hidden="true" tabindex="-1"></a>pdp_pred_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) {</span>
<span id="cb183-3"><a href="lesson-6c-random-forests.html#cb183-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">predict</span>(object, newdata, <span class="at">type =</span> <span class="st">&quot;numeric&quot;</span>)<span class="sc">$</span>.pred</span>
<span id="cb183-4"><a href="lesson-6c-random-forests.html#cb183-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb183-5"><a href="lesson-6c-random-forests.html#cb183-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-6"><a href="lesson-6c-random-forests.html#cb183-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use the pdp package to extract partial dependence predictions</span></span>
<span id="cb183-7"><a href="lesson-6c-random-forests.html#cb183-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and then plot</span></span>
<span id="cb183-8"><a href="lesson-6c-random-forests.html#cb183-8" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb183-9"><a href="lesson-6c-random-forests.html#cb183-9" aria-hidden="true" tabindex="-1"></a>   pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb183-10"><a href="lesson-6c-random-forests.html#cb183-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">pred.var =</span> <span class="st">&quot;Overall_Qual&quot;</span>, </span>
<span id="cb183-11"><a href="lesson-6c-random-forests.html#cb183-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb183-12"><a href="lesson-6c-random-forests.html#cb183-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb183-13"><a href="lesson-6c-random-forests.html#cb183-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">train =</span> ames_train</span>
<span id="cb183-14"><a href="lesson-6c-random-forests.html#cb183-14" aria-hidden="true" tabindex="-1"></a>   ) <span class="sc">%&gt;%</span></span>
<span id="cb183-15"><a href="lesson-6c-random-forests.html#cb183-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Overall_Qual, yhat)) <span class="sc">+</span></span>
<span id="cb183-16"><a href="lesson-6c-random-forests.html#cb183-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb183-17"><a href="lesson-6c-random-forests.html#cb183-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-326-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Now let’s plot the PDP for the <code>Gr_Liv_Area</code> and see how that variable relates to the predicted sale price.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="lesson-6c-random-forests.html#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction function</span></span>
<span id="cb184-2"><a href="lesson-6c-random-forests.html#cb184-2" aria-hidden="true" tabindex="-1"></a>pdp_pred_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) {</span>
<span id="cb184-3"><a href="lesson-6c-random-forests.html#cb184-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mean</span>(<span class="fu">predict</span>(object, newdata, <span class="at">type =</span> <span class="st">&quot;numeric&quot;</span>)<span class="sc">$</span>.pred)</span>
<span id="cb184-4"><a href="lesson-6c-random-forests.html#cb184-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb184-5"><a href="lesson-6c-random-forests.html#cb184-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-6"><a href="lesson-6c-random-forests.html#cb184-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use the pdp package to extract partial dependence predictions</span></span>
<span id="cb184-7"><a href="lesson-6c-random-forests.html#cb184-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and then plot</span></span>
<span id="cb184-8"><a href="lesson-6c-random-forests.html#cb184-8" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb184-9"><a href="lesson-6c-random-forests.html#cb184-9" aria-hidden="true" tabindex="-1"></a>   pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb184-10"><a href="lesson-6c-random-forests.html#cb184-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, </span>
<span id="cb184-11"><a href="lesson-6c-random-forests.html#cb184-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb184-12"><a href="lesson-6c-random-forests.html#cb184-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb184-13"><a href="lesson-6c-random-forests.html#cb184-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">train =</span> ames_train</span>
<span id="cb184-14"><a href="lesson-6c-random-forests.html#cb184-14" aria-hidden="true" tabindex="-1"></a>   ) <span class="sc">%&gt;%</span></span>
<span id="cb184-15"><a href="lesson-6c-random-forests.html#cb184-15" aria-hidden="true" tabindex="-1"></a>   <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb184-16"><a href="lesson-6c-random-forests.html#cb184-16" aria-hidden="true" tabindex="-1"></a>   <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-327-1.png" width="864" style="display: block; margin: auto;" /></p>
<div id="knowledge-check-35" class="section level3 hasAnchor" number="19.7.1">
<h3><span class="header-section-number">19.7.1</span> Knowledge check<a href="lesson-6c-random-forests.html#knowledge-check-35" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset and the model from the
previous Knowledge check that performed best…
</p>
<ul>
<li>
Plot the top 10 most influential features.
</li>
<li>
Create and explain a PDP plot of the most influential feature.
</li>
</ul>
</div>
</div>
</div>
<div id="final-thoughts-5" class="section level2 hasAnchor" number="19.8">
<h2><span class="header-section-number">19.8</span> Final thoughts<a href="lesson-6c-random-forests.html#final-thoughts-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. They come with all the benefits of decision trees (with the exception of surrogate splits) and bagging but greatly reduce instability and between-tree correlation. And due to the added split variable selection attribute, random forests are also faster than bagging as they have a smaller feature search space at each tree split. However, random forests will still suffer from slow computational speed as your data sets get larger but, similar to bagging, the algorithm is built upon independent steps, and most modern implementations allow for parallelization to improve training time.</p>
</div>
<div id="exercises-12" class="section level2 hasAnchor" number="19.9">
<h2><span class="header-section-number">19.9</span> Exercises<a href="lesson-6c-random-forests.html#exercises-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Using the same <code>kernlab::spam</code> data we saw in the <a
href="https://bradleyboehmke.github.io/uc-bana-4080/lesson-4b-regularized-regression.html#classification-problems-1">section
12.10</a>…
</p>
<ol style="list-style-type: decimal">
<li>
Split the data into 70-30 training-test sets.
</li>
<li>
Apply a default random forest model modeling the <code>type</code>
response variable as a function of all available features.
</li>
<li>
Now tune the <code>trees</code>, <code>mtry</code>, and
<code>min_n</code> hyperparameters to find the best performing
combination of hyperparameters.
</li>
<li>
How does the model performance compare to the decision tree model
and bagged decision tree model applied in the previous two lesson
exercises?
</li>
<li>
Which 10 features are considered most influential? Are these the
same features that have been influential in previous models?
</li>
<li>
Create partial dependence plots for the top two most influential
features. Explain the relationship between the feature and the predicted
values.
</li>
</ol>
</div>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-breiman2001random" class="csl-entry">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-diaz2006gene" class="csl-entry">
Dı́az-Uriarte, Ramón, and Sara Alvarez De Andres. 2006. <span>“Gene Selection and Classification of Microarray Data Using Random Forest.”</span> <em>BMC Bioinformatics</em> 7 (1): 3.
</div>
<div id="ref-esl" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer Series in Statistics New York, NY, USA:
</div>
<div id="ref-goldstein2011random" class="csl-entry">
Goldstein, Benjamin A, Eric C Polley, and Farren BS Briggs. 2011. <span>“Random Forests for Genetic Association Studies.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 10 (1).
</div>
<div id="ref-probst2018tunability" class="csl-entry">
Probst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018. <span>“Tunability: Importance of Hyperparameters of Machine Learning Algorithms.”</span> <em>arXiv Preprint arXiv:1802.09596</em>.
</div>
<div id="ref-probst2019hyperparameters" class="csl-entry">
Probst, Philipp, Marvin N Wright, and Anne-Laure Boulesteix. 2019. <span>“Hyperparameters and Tuning Strategies for Random Forest.”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, e1301.
</div>
<div id="ref-segal2004machine" class="csl-entry">
Segal, Mark R. 2004. <span>“Machine Learning Benchmarks and Random Forest Regression.”</span> <em>UCSF: Center for Bioinformatics and Molecular Biostatistics</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>See <span class="citation">J. Friedman, Hastie, and Tibshirani (<a href="#ref-esl" role="doc-biblioref">2001</a>)</span> for a mathematical explanation of the tree correlation phenomenon.<a href="lesson-6c-random-forests.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lesson-6b-bagging.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computing-environment.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-6/lesson-3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
