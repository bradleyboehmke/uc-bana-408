<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>18 Lesson 6b: Bagging | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="18 Lesson 6b: Bagging | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="18 Lesson 6b: Bagging | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lesson-6a-decision-trees.html"/>
<link rel="next" href="lesson-6c-random-forests.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 5</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>15</b> Lesson 5b: Multivariate Adaptive Regression Splines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#learning-objectives-15"><i class="fa fa-check"></i><b>15.1</b> Learning objectives</a></li>
<li class="chapter" data-level="15.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#prerequisites-8"><i class="fa fa-check"></i><b>15.2</b> Prerequisites</a></li>
<li class="chapter" data-level="15.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#nonlinearity"><i class="fa fa-check"></i><b>15.3</b> Nonlinearity</a></li>
<li class="chapter" data-level="15.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>15.4</b> Multivariate adaptive regression splines</a></li>
<li class="chapter" data-level="15.5" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-mars-model"><i class="fa fa-check"></i><b>15.5</b> Fitting a MARS model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-basic-model"><i class="fa fa-check"></i><b>15.5.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="15.5.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model"><i class="fa fa-check"></i><b>15.5.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="15.5.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model-with-interactions"><i class="fa fa-check"></i><b>15.5.3</b> Fitting a full model with interactions</a></li>
<li class="chapter" data-level="15.5.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-25"><i class="fa fa-check"></i><b>15.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#tuning-1"><i class="fa fa-check"></i><b>15.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-26"><i class="fa fa-check"></i><b>15.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#feature-interpretation-1"><i class="fa fa-check"></i><b>15.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="15.8" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#final-thoughts-2"><i class="fa fa-check"></i><b>15.8</b> Final thoughts</a></li>
<li class="chapter" data-level="15.9" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#exercises-9"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Module 6</b></span></li>
<li class="chapter" data-level="16" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i><b>16</b> Overview</a>
<ul>
<li class="chapter" data-level="16.1" data-path="overview-5.html"><a href="overview-5.html#learning-objectives-16"><i class="fa fa-check"></i><b>16.1</b> Learning objectives</a></li>
<li class="chapter" data-level="16.2" data-path="overview-5.html"><a href="overview-5.html#estimated-time-requirement-5"><i class="fa fa-check"></i><b>16.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="16.3" data-path="overview-5.html"><a href="overview-5.html#tasks-5"><i class="fa fa-check"></i><b>16.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html"><i class="fa fa-check"></i><b>17</b> Lesson 6a: Decision Trees</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#learning-objectives-17"><i class="fa fa-check"></i><b>17.1</b> Learning objectives</a></li>
<li class="chapter" data-level="17.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#prerequisites-9"><i class="fa fa-check"></i><b>17.2</b> Prerequisites</a></li>
<li class="chapter" data-level="17.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#structure"><i class="fa fa-check"></i><b>17.3</b> Structure</a></li>
<li class="chapter" data-level="17.4" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#partitioning"><i class="fa fa-check"></i><b>17.4</b> Partitioning</a></li>
<li class="chapter" data-level="17.5" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#how-deep"><i class="fa fa-check"></i><b>17.5</b> How deep?</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#early-stopping"><i class="fa fa-check"></i><b>17.5.1</b> Early stopping</a></li>
<li class="chapter" data-level="17.5.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#pruning"><i class="fa fa-check"></i><b>17.5.2</b> Pruning</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-decision-tree"><i class="fa fa-check"></i><b>17.6</b> Fitting a decision tree</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1"><i class="fa fa-check"></i><b>17.6.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="17.6.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-full-model-1"><i class="fa fa-check"></i><b>17.6.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="17.6.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-27"><i class="fa fa-check"></i><b>17.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#tuning-2"><i class="fa fa-check"></i><b>17.7</b> Tuning</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-28"><i class="fa fa-check"></i><b>17.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#feature-interpretation-2"><i class="fa fa-check"></i><b>17.8</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-29"><i class="fa fa-check"></i><b>17.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#final-thoughts-3"><i class="fa fa-check"></i><b>17.9</b> Final thoughts</a></li>
<li class="chapter" data-level="17.10" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>17.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html"><i class="fa fa-check"></i><b>18</b> Lesson 6b: Bagging</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#learning-objectives-18"><i class="fa fa-check"></i><b>18.1</b> Learning objectives</a></li>
<li class="chapter" data-level="18.2" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#prerequisites-10"><i class="fa fa-check"></i><b>18.2</b> Prerequisites</a></li>
<li class="chapter" data-level="18.3" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#why-and-when-bagging-works"><i class="fa fa-check"></i><b>18.3</b> Why and when bagging works</a></li>
<li class="chapter" data-level="18.4" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#fitting-a-bagged-decision-tree-model"><i class="fa fa-check"></i><b>18.4</b> Fitting a bagged decision tree model</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-30"><i class="fa fa-check"></i><b>18.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#tuning-3"><i class="fa fa-check"></i><b>18.5</b> Tuning</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-31"><i class="fa fa-check"></i><b>18.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#feature-interpretation-3"><i class="fa fa-check"></i><b>18.6</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-32"><i class="fa fa-check"></i><b>18.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#final-thoughts-4"><i class="fa fa-check"></i><b>18.7</b> Final thoughts</a></li>
<li class="chapter" data-level="18.8" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#exercises-11"><i class="fa fa-check"></i><b>18.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html"><i class="fa fa-check"></i><b>19</b> Lesson 6c: Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#learning-objectives-19"><i class="fa fa-check"></i><b>19.1</b> Learning objectives</a></li>
<li class="chapter" data-level="19.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#prerequisites-11"><i class="fa fa-check"></i><b>19.2</b> Prerequisites</a></li>
<li class="chapter" data-level="19.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#extending-bagging"><i class="fa fa-check"></i><b>19.3</b> Extending bagging</a></li>
<li class="chapter" data-level="19.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#out-of-the-box-performance"><i class="fa fa-check"></i><b>19.4</b> Out-of-the-box performance</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-33"><i class="fa fa-check"></i><b>19.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#hyperparameters"><i class="fa fa-check"></i><b>19.5</b> Hyperparameters</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#number-of-trees"><i class="fa fa-check"></i><b>19.5.1</b> Number of trees</a></li>
<li class="chapter" data-level="19.5.2" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#mtry"><i class="fa fa-check"></i><b>19.5.2</b> <span class="math inline">\(m_{try}\)</span></a></li>
<li class="chapter" data-level="19.5.3" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tree-complexity"><i class="fa fa-check"></i><b>19.5.3</b> Tree complexity</a></li>
<li class="chapter" data-level="19.5.4" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#others"><i class="fa fa-check"></i><b>19.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#tuning-4"><i class="fa fa-check"></i><b>19.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="19.6.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-34"><i class="fa fa-check"></i><b>19.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#feature-interpretation-4"><i class="fa fa-check"></i><b>19.7</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="19.7.1" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#knowledge-check-35"><i class="fa fa-check"></i><b>19.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#final-thoughts-5"><i class="fa fa-check"></i><b>19.8</b> Final thoughts</a></li>
<li class="chapter" data-level="19.9" data-path="lesson-6c-random-forests.html"><a href="lesson-6c-random-forests.html#exercises-12"><i class="fa fa-check"></i><b>19.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VII Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-6b-bagging" class="section level1 hasAnchor" number="18">
<h1><span class="header-section-number">18</span> Lesson 6b: Bagging<a href="lesson-6b-bagging.html#lesson-6b-bagging" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In an earlier module we learned about bootstrapping as a resampling procedure, which creates <em>b</em> new bootstrap samples by drawing samples with replacement of the original training data. This module illustrates how we can use bootstrapping to create an <em>ensemble</em> of predictions. Bootstrap aggregating, also called <em>bagging</em>, is one of the first ensemble algorithms<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> machine learning practitioners learn and is designed to improve the stability and accuracy of regression and classification algorithms. By model averaging, bagging helps to reduce variance and minimize overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.</p>
<div id="learning-objectives-18" class="section level2 hasAnchor" number="18.1">
<h2><span class="header-section-number">18.1</span> Learning objectives<a href="lesson-6b-bagging.html#learning-objectives-18" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module you will know:</p>
<ul>
<li>How and why bagging improves decision tree performance.</li>
<li>How to implement bagging and ensure you are optimizing bagging performance.</li>
<li>How to identify influential features and their effects on the response variable.</li>
</ul>
</div>
<div id="prerequisites-10" class="section level2 hasAnchor" number="18.2">
<h2><span class="header-section-number">18.2</span> Prerequisites<a href="lesson-6b-bagging.html#prerequisites-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="lesson-6b-bagging.html#cb171-1" tabindex="-1"></a><span class="co"># Helper packages</span></span>
<span id="cb171-2"><a href="lesson-6b-bagging.html#cb171-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)   <span class="co"># for data wrangling &amp; plotting</span></span>
<span id="cb171-3"><a href="lesson-6b-bagging.html#cb171-3" tabindex="-1"></a></span>
<span id="cb171-4"><a href="lesson-6b-bagging.html#cb171-4" tabindex="-1"></a><span class="co"># Modeling packages</span></span>
<span id="cb171-5"><a href="lesson-6b-bagging.html#cb171-5" tabindex="-1"></a><span class="fu">library</span>(tidymodels) </span>
<span id="cb171-6"><a href="lesson-6b-bagging.html#cb171-6" tabindex="-1"></a><span class="fu">library</span>(baguette)    <span class="co"># you may need to install.packages this package</span></span>
<span id="cb171-7"><a href="lesson-6b-bagging.html#cb171-7" tabindex="-1"></a></span>
<span id="cb171-8"><a href="lesson-6b-bagging.html#cb171-8" tabindex="-1"></a><span class="co"># Model interpretability packages</span></span>
<span id="cb171-9"><a href="lesson-6b-bagging.html#cb171-9" tabindex="-1"></a><span class="fu">library</span>(vip)         <span class="co"># for variable importance</span></span>
<span id="cb171-10"><a href="lesson-6b-bagging.html#cb171-10" tabindex="-1"></a><span class="fu">library</span>(pdp)         <span class="co"># for variable relationships</span></span></code></pre></div>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="lesson-6b-bagging.html#cb172-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb172-2"><a href="lesson-6b-bagging.html#cb172-2" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span>
<span id="cb172-3"><a href="lesson-6b-bagging.html#cb172-3" tabindex="-1"></a>split  <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)</span>
<span id="cb172-4"><a href="lesson-6b-bagging.html#cb172-4" tabindex="-1"></a>ames_train  <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb172-5"><a href="lesson-6b-bagging.html#cb172-5" tabindex="-1"></a>ames_test   <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="why-and-when-bagging-works" class="section level2 hasAnchor" number="18.3">
<h2><span class="header-section-number">18.3</span> Why and when bagging works<a href="lesson-6b-bagging.html#why-and-when-bagging-works" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Bootstrap aggregating</em> (bagging) prediction models is a general method for fitting multiple versions of a prediction model and then combining (or ensembling) them into an aggregated prediction <span class="citation">(<a href="#ref-breiman1996bagging">Breiman 1996</a>)</span>. Bagging is a fairly straight forward algorithm in which <em>b</em> bootstrap copies of the original training data are created, the regression or classification algorithm (commonly referred to as the <em>base learner</em>) is applied to each bootstrap sample and, in the regression context, new predictions are made by averaging the predictions together from the individual base learners. When dealing with a classification problem, the base learner predictions are combined using plurality vote or by averaging the estimated class probabilities together. This is represented in the below equation where <span class="math inline">\(X\)</span> is the record for which we want to generate a prediction, <span class="math inline">\(\widehat{f_{bag}}\)</span> is the bagged prediction, and <span class="math inline">\(\widehat{f_1}\left(X\right), \widehat{f_2}\left(X\right), \dots, \widehat{f_b}\left(X\right)\)</span> are the predictions from the individual base learners.</p>
<p><span class="math display">\[\begin{equation}
  \widehat{f_{bag}} = \widehat{f_1}\left(X\right) + \widehat{f_2}\left(X\right) + \cdots + \widehat{f_b}\left(X\right)
\end{equation}\]</span></p>
<p>Because of the aggregation process, bagging effectively reduces the variance of an individual base learner (i.e., averaging reduces variance); however, bagging does not always improve upon an individual base learner. As discussed in our <a href="https://bradleyboehmke.github.io/uc-bana-4080/lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff">bias vs variance discussion</a>, some models have larger variance than others. Bagging works especially well for unstable, high variance base learners—algorithms whose predicted output undergoes major changes in response to small changes in the training data <span class="citation">(<a href="#ref-dietterich2000ensemble">Dietterich 2000b</a>, <a href="#ref-dietterich2000experimental">2000a</a>)</span>. This includes algorithms such as decision trees and KNN (when <em>k</em> is sufficiently small). However, for algorithms that are more stable or have high bias, bagging offers less improvement on predicted outputs since there is less variability (e.g., bagging a linear regression model will effectively just return the original predictions for large enough <span class="math inline">\(b\)</span>).</p>
<div class="note">
<p>
The general idea behind bagging is referred to as the “wisdom of the
crowd” effect and was popularized by <span class="citation"><span class="citation">Surowiecki (<a href="#ref-surowiecki2005wisdom">2005</a>)</span></span>. It essentially means that
the aggregation of information in large diverse groups results in
decisions that are often better than could have been made by any single
member of the group. The more diverse the group members are then the
more diverse their perspectives and predictions will be, which often
leads to better aggregated information. Think of estimating the number
of jelly beans in a jar at a carinival. While any individual guess is
likely to be way off, you’ll often find that the averaged guesses tends
to be a lot closer to the true number.
</p>
</div>
<p>This is illustrated in the below plot, which compares bagging <span class="math inline">\(b = 100\)</span> polynomial regression models, MARS models, and CART decision trees. You can see that the low variance base learner (polynomial regression) gains very little from bagging while the higher variance learner (decision trees) gains significantly more. Not only does bagging help minimize the high variability (instability) of single trees, but it also helps to smooth out the prediction surface.</p>
<p><img src="_main_files/figure-html/bagging-multiple-models-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing <em>k</em>-fold CV can become computationally burdensome.</p>
<p>A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample. The OOB sample can be used to test predictive performance and the results usually compare well compared to <em>k</em>-fold CV assuming your data set is sufficiently large (say <span class="math inline">\(n \geq 1,000\)</span>). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.</p>
<div class="note">
<p>
Think of the OOB estimate of generalization performance as an
unstructured, but free CV statistic.
</p>
</div>
</div>
<div id="fitting-a-bagged-decision-tree-model" class="section level2 hasAnchor" number="18.4">
<h2><span class="header-section-number">18.4</span> Fitting a bagged decision tree model<a href="lesson-6b-bagging.html#fitting-a-bagged-decision-tree-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall in the decision tree module that our optimally tuned single decision tree model was obtaining a mid-$30K RMSE. In this first implementation we’ll perform the same process; however, instead of a single decision tree we’ll use bagging to create and aggregate performance across <strong>5</strong> bagged trees.</p>
<p>In R we use <code>baguette::bag_tree</code> to create a bagged tree model object, which can be applied to decision tree or MARS models. Note how we do not need to perform any feature engineering as decision trees (and therefore bagged decision trees) can handle numeric and categorical features just as they are.</p>
<p>Note how aggregating just 5 decision trees improves our RMSE to around $30K.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="lesson-6b-bagging.html#cb173-1" tabindex="-1"></a><span class="co"># create model recipe with all features</span></span>
<span id="cb173-2"><a href="lesson-6b-bagging.html#cb173-2" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(</span>
<span id="cb173-3"><a href="lesson-6b-bagging.html#cb173-3" tabindex="-1"></a>    Sale_Price <span class="sc">~</span> ., </span>
<span id="cb173-4"><a href="lesson-6b-bagging.html#cb173-4" tabindex="-1"></a>    <span class="at">data =</span> ames_train</span>
<span id="cb173-5"><a href="lesson-6b-bagging.html#cb173-5" tabindex="-1"></a>  )</span>
<span id="cb173-6"><a href="lesson-6b-bagging.html#cb173-6" tabindex="-1"></a></span>
<span id="cb173-7"><a href="lesson-6b-bagging.html#cb173-7" tabindex="-1"></a><span class="co"># create bagged CART model object and</span></span>
<span id="cb173-8"><a href="lesson-6b-bagging.html#cb173-8" tabindex="-1"></a><span class="co"># start with 5 bagged trees</span></span>
<span id="cb173-9"><a href="lesson-6b-bagging.html#cb173-9" tabindex="-1"></a>tree_mod <span class="ot">&lt;-</span> <span class="fu">bag_tree</span>() <span class="sc">%&gt;%</span></span>
<span id="cb173-10"><a href="lesson-6b-bagging.html#cb173-10" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>, <span class="at">times =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb173-11"><a href="lesson-6b-bagging.html#cb173-11" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb173-12"><a href="lesson-6b-bagging.html#cb173-12" tabindex="-1"></a></span>
<span id="cb173-13"><a href="lesson-6b-bagging.html#cb173-13" tabindex="-1"></a><span class="co"># create resampling procedure</span></span>
<span id="cb173-14"><a href="lesson-6b-bagging.html#cb173-14" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb173-15"><a href="lesson-6b-bagging.html#cb173-15" tabindex="-1"></a>kfold <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(ames_train, <span class="at">v =</span> <span class="dv">5</span>)</span>
<span id="cb173-16"><a href="lesson-6b-bagging.html#cb173-16" tabindex="-1"></a></span>
<span id="cb173-17"><a href="lesson-6b-bagging.html#cb173-17" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb173-18"><a href="lesson-6b-bagging.html#cb173-18" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">fit_resamples</span>(tree_mod, model_recipe, kfold)</span>
<span id="cb173-19"><a href="lesson-6b-bagging.html#cb173-19" tabindex="-1"></a></span>
<span id="cb173-20"><a href="lesson-6b-bagging.html#cb173-20" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb173-21"><a href="lesson-6b-bagging.html#cb173-21" tabindex="-1"></a><span class="fu">collect_metrics</span>(results)</span>
<span id="cb173-22"><a href="lesson-6b-bagging.html#cb173-22" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb173-23"><a href="lesson-6b-bagging.html#cb173-23" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n   std_err .config             </span></span>
<span id="cb173-24"><a href="lesson-6b-bagging.html#cb173-24" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb173-25"><a href="lesson-6b-bagging.html#cb173-25" tabindex="-1"></a><span class="do">## 1 rmse    standard   29850.        5 1270.     Preprocessor1_Model1</span></span>
<span id="cb173-26"><a href="lesson-6b-bagging.html#cb173-26" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.861     5    0.0145 Preprocessor1_Model1</span></span></code></pre></div>
<div id="knowledge-check-30" class="section level3 hasAnchor" number="18.4.1">
<h3><span class="header-section-number">18.4.1</span> Knowledge check<a href="lesson-6b-bagging.html#knowledge-check-30" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset:
</p>
<p>
Apply a bagged decision tree model where <code>cmedv</code> is the
response variable and use all possible predictor variables. Use a 5-fold
cross validation procedure to assess…
</p>
<ul>
<li>
Bagging 5 trees
</li>
<li>
Bagging 10 trees
</li>
<li>
Bagging 25 trees
</li>
</ul>
<p>
How does the model perform in each scenario? Do adding more trees
improve performance?
</p>
</div>
</div>
</div>
<div id="tuning-3" class="section level2 hasAnchor" number="18.5">
<h2><span class="header-section-number">18.5</span> Tuning<a href="lesson-6b-bagging.html#tuning-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One thing to note is that typically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more).</p>
<p>Consequently, we can treat the number of trees as a tuning parameter where our focus is to make sure we are using enough trees for our error metric to converge at a optimal value (minimize RMSE in our case). However, rarely do we run the risk of including too many trees as we typically will not see our error metric degrade after adding more trees than necessary.</p>
<div class="tip">
<p>
The primary concern is compute efficiency, we want enough trees to
reach an optimal resampling error; however, not more trees than
necessary as it will slow down computational efficiency.
</p>
</div>
<p>In this example we use a hyperparameter grid to assess 5, 25, 50, 100, and 200 trees. We see an initial decrease but as we start using 50+ trees we only see marginal changes in our RMSE, suggesting we have likely reached enough trees to minimize the RMSE and any additional differences are likely due to the standard error that occurs during the bootstrapping process.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="lesson-6b-bagging.html#cb174-1" tabindex="-1"></a><span class="co"># create bagged CART model object with</span></span>
<span id="cb174-2"><a href="lesson-6b-bagging.html#cb174-2" tabindex="-1"></a><span class="co"># tuning option set for number of bagged trees</span></span>
<span id="cb174-3"><a href="lesson-6b-bagging.html#cb174-3" tabindex="-1"></a>tree_mod <span class="ot">&lt;-</span> <span class="fu">bag_tree</span>() <span class="sc">%&gt;%</span></span>
<span id="cb174-4"><a href="lesson-6b-bagging.html#cb174-4" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>, <span class="at">times =</span> <span class="fu">tune</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb174-5"><a href="lesson-6b-bagging.html#cb174-5" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb174-6"><a href="lesson-6b-bagging.html#cb174-6" tabindex="-1"></a></span>
<span id="cb174-7"><a href="lesson-6b-bagging.html#cb174-7" tabindex="-1"></a><span class="co"># create the hyperparameter grid</span></span>
<span id="cb174-8"><a href="lesson-6b-bagging.html#cb174-8" tabindex="-1"></a>hyper_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">times =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>))</span>
<span id="cb174-9"><a href="lesson-6b-bagging.html#cb174-9" tabindex="-1"></a></span>
<span id="cb174-10"><a href="lesson-6b-bagging.html#cb174-10" tabindex="-1"></a><span class="co"># train our model across the hyperparameter grid</span></span>
<span id="cb174-11"><a href="lesson-6b-bagging.html#cb174-11" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb174-12"><a href="lesson-6b-bagging.html#cb174-12" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(tree_mod, model_recipe, <span class="at">resamples =</span> kfold, <span class="at">grid =</span> hyper_grid)</span>
<span id="cb174-13"><a href="lesson-6b-bagging.html#cb174-13" tabindex="-1"></a></span>
<span id="cb174-14"><a href="lesson-6b-bagging.html#cb174-14" tabindex="-1"></a></span>
<span id="cb174-15"><a href="lesson-6b-bagging.html#cb174-15" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb174-16"><a href="lesson-6b-bagging.html#cb174-16" tabindex="-1"></a><span class="fu">show_best</span>(results, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb174-17"><a href="lesson-6b-bagging.html#cb174-17" tabindex="-1"></a><span class="do">## # A tibble: 5 × 7</span></span>
<span id="cb174-18"><a href="lesson-6b-bagging.html#cb174-18" tabindex="-1"></a><span class="do">##   times .metric .estimator   mean     n</span></span>
<span id="cb174-19"><a href="lesson-6b-bagging.html#cb174-19" tabindex="-1"></a><span class="do">##   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;</span></span>
<span id="cb174-20"><a href="lesson-6b-bagging.html#cb174-20" tabindex="-1"></a><span class="do">## 1   200 rmse    standard   26175.     5</span></span>
<span id="cb174-21"><a href="lesson-6b-bagging.html#cb174-21" tabindex="-1"></a><span class="do">## 2   100 rmse    standard   26430.     5</span></span>
<span id="cb174-22"><a href="lesson-6b-bagging.html#cb174-22" tabindex="-1"></a><span class="do">## 3    50 rmse    standard   26536.     5</span></span>
<span id="cb174-23"><a href="lesson-6b-bagging.html#cb174-23" tabindex="-1"></a><span class="do">## 4    25 rmse    standard   26909.     5</span></span>
<span id="cb174-24"><a href="lesson-6b-bagging.html#cb174-24" tabindex="-1"></a><span class="do">## 5     5 rmse    standard   30276.     5</span></span>
<span id="cb174-25"><a href="lesson-6b-bagging.html#cb174-25" tabindex="-1"></a><span class="do">## # ℹ 2 more variables: std_err &lt;dbl&gt;,</span></span>
<span id="cb174-26"><a href="lesson-6b-bagging.html#cb174-26" tabindex="-1"></a><span class="do">## #   .config &lt;chr&gt;</span></span></code></pre></div>
<div class="note">
<p>
When bagging trees we can also tune the same hyperparameters that we
tuned with decision trees (<code>cost_complexity</code>,
<code>tree_depth</code>, <code>min_n</code>). See <code>bag_tree?</code>
for details.
</p>
<p>
However, the default values for these hyperparameters is set to grow
our trees to the maximum depth and not prune them at all. It has been
shown that bagging many deep, overfit trees typically leads to best
performance.
</p>
</div>
<div id="knowledge-check-31" class="section level3 hasAnchor" number="18.5.1">
<h3><span class="header-section-number">18.5.1</span> Knowledge check<a href="lesson-6b-bagging.html#knowledge-check-31" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset:
</p>
<p>
Apply a bagged decision tree model where <code>cmedv</code> is the
response variable and use all possible predictor variables. Use a 5-fold
cross validation procedure and tune the number of trees to assess 5, 10,
25, 50, 100, and 150 trees.
</p>
<p>
How does the model perform in each scenario? Do adding more trees
improve performance? At what point do we experience diminishing returns
in our cross-validated RMSE?
</p>
</div>
</div>
</div>
<div id="feature-interpretation-3" class="section level2 hasAnchor" number="18.6">
<h2><span class="header-section-number">18.6</span> Feature interpretation<a href="lesson-6b-bagging.html#feature-interpretation-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Unfortunately, due to the bagging process, models that are normally perceived as interpretable are no longer so. However, we can still make inferences about how features are influencing our model. Recall in the decision tree module that we measure feature importance based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.</p>
<p>For bagged decision trees, this process is similar. For each tree, we compute the sum of the reduction of the loss function across all splits. We then aggregate this measure across all trees for each feature. The features with the largest average decrease in SSE (for regression) are considered most important.</p>
<p>The following extracts the feature importance for our best performing bagged decision trees. Unfortunately, <code>vip::vip()</code> doesn’t work naturally with models from <code>bag_tree</code> so we need to manually extract the variable importance scores and plot them with <code>ggplot</code>. We see that <code>Overall_Qual</code> and <code>Neighborhood</code> are the top two most influential features.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="lesson-6b-bagging.html#cb175-1" tabindex="-1"></a><span class="co"># identify best model</span></span>
<span id="cb175-2"><a href="lesson-6b-bagging.html#cb175-2" tabindex="-1"></a>best_hyperparameters <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb175-3"><a href="lesson-6b-bagging.html#cb175-3" tabindex="-1"></a>   <span class="fu">select_best</span>(<span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb175-4"><a href="lesson-6b-bagging.html#cb175-4" tabindex="-1"></a></span>
<span id="cb175-5"><a href="lesson-6b-bagging.html#cb175-5" tabindex="-1"></a><span class="co"># finalize workflow object</span></span>
<span id="cb175-6"><a href="lesson-6b-bagging.html#cb175-6" tabindex="-1"></a>final_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb175-7"><a href="lesson-6b-bagging.html#cb175-7" tabindex="-1"></a>   <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb175-8"><a href="lesson-6b-bagging.html#cb175-8" tabindex="-1"></a>   <span class="fu">add_model</span>(tree_mod) <span class="sc">%&gt;%</span></span>
<span id="cb175-9"><a href="lesson-6b-bagging.html#cb175-9" tabindex="-1"></a>   <span class="fu">finalize_workflow</span>(best_hyperparameters)</span>
<span id="cb175-10"><a href="lesson-6b-bagging.html#cb175-10" tabindex="-1"></a></span>
<span id="cb175-11"><a href="lesson-6b-bagging.html#cb175-11" tabindex="-1"></a><span class="co"># final fit on training data</span></span>
<span id="cb175-12"><a href="lesson-6b-bagging.html#cb175-12" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> final_wf <span class="sc">%&gt;%</span></span>
<span id="cb175-13"><a href="lesson-6b-bagging.html#cb175-13" tabindex="-1"></a>   <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb175-14"><a href="lesson-6b-bagging.html#cb175-14" tabindex="-1"></a></span>
<span id="cb175-15"><a href="lesson-6b-bagging.html#cb175-15" tabindex="-1"></a><span class="co"># plot top 20 influential variables</span></span>
<span id="cb175-16"><a href="lesson-6b-bagging.html#cb175-16" tabindex="-1"></a>vi <span class="ot">&lt;-</span> final_fit <span class="sc">%&gt;%</span></span>
<span id="cb175-17"><a href="lesson-6b-bagging.html#cb175-17" tabindex="-1"></a>   <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb175-18"><a href="lesson-6b-bagging.html#cb175-18" tabindex="-1"></a>   .[[<span class="st">&#39;fit&#39;</span>]] <span class="sc">%&gt;%</span></span>
<span id="cb175-19"><a href="lesson-6b-bagging.html#cb175-19" tabindex="-1"></a>   <span class="fu">var_imp</span>() <span class="sc">%&gt;%</span></span>
<span id="cb175-20"><a href="lesson-6b-bagging.html#cb175-20" tabindex="-1"></a>   <span class="fu">slice</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>)</span>
<span id="cb175-21"><a href="lesson-6b-bagging.html#cb175-21" tabindex="-1"></a></span>
<span id="cb175-22"><a href="lesson-6b-bagging.html#cb175-22" tabindex="-1"></a><span class="fu">ggplot</span>(vi, <span class="fu">aes</span>(value, <span class="fu">reorder</span>(term, value))) <span class="sc">+</span></span>
<span id="cb175-23"><a href="lesson-6b-bagging.html#cb175-23" tabindex="-1"></a>   <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb175-24"><a href="lesson-6b-bagging.html#cb175-24" tabindex="-1"></a>   <span class="fu">ylab</span>(<span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb175-25"><a href="lesson-6b-bagging.html#cb175-25" tabindex="-1"></a>   <span class="fu">xlab</span>(<span class="st">&quot;Feature importance&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-304-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We can then plot the partial dependence of the most influential feature to see how it influences the predicted values. We see that as overall quality increase we see a significant increase in the predicted sale price.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="lesson-6b-bagging.html#cb176-1" tabindex="-1"></a><span class="co"># prediction function</span></span>
<span id="cb176-2"><a href="lesson-6b-bagging.html#cb176-2" tabindex="-1"></a>pdp_pred_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) {</span>
<span id="cb176-3"><a href="lesson-6b-bagging.html#cb176-3" tabindex="-1"></a>   <span class="fu">predict</span>(object, newdata, <span class="at">type =</span> <span class="st">&quot;numeric&quot;</span>)<span class="sc">$</span>.pred</span>
<span id="cb176-4"><a href="lesson-6b-bagging.html#cb176-4" tabindex="-1"></a>}</span>
<span id="cb176-5"><a href="lesson-6b-bagging.html#cb176-5" tabindex="-1"></a></span>
<span id="cb176-6"><a href="lesson-6b-bagging.html#cb176-6" tabindex="-1"></a><span class="co"># use the pdp package to extract partial dependence predictions</span></span>
<span id="cb176-7"><a href="lesson-6b-bagging.html#cb176-7" tabindex="-1"></a><span class="co"># and then plot</span></span>
<span id="cb176-8"><a href="lesson-6b-bagging.html#cb176-8" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb176-9"><a href="lesson-6b-bagging.html#cb176-9" tabindex="-1"></a>   pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb176-10"><a href="lesson-6b-bagging.html#cb176-10" tabindex="-1"></a>     <span class="at">pred.var =</span> <span class="st">&quot;Overall_Qual&quot;</span>, </span>
<span id="cb176-11"><a href="lesson-6b-bagging.html#cb176-11" tabindex="-1"></a>     <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb176-12"><a href="lesson-6b-bagging.html#cb176-12" tabindex="-1"></a>     <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb176-13"><a href="lesson-6b-bagging.html#cb176-13" tabindex="-1"></a>     <span class="at">train =</span> ames_train</span>
<span id="cb176-14"><a href="lesson-6b-bagging.html#cb176-14" tabindex="-1"></a>   ) <span class="sc">%&gt;%</span></span>
<span id="cb176-15"><a href="lesson-6b-bagging.html#cb176-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Overall_Qual, yhat)) <span class="sc">+</span></span>
<span id="cb176-16"><a href="lesson-6b-bagging.html#cb176-16" tabindex="-1"></a>    <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb176-17"><a href="lesson-6b-bagging.html#cb176-17" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-305-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Now let’s plot the PDP for the <code>Gr_Liv_Area</code> and see how that variable relates to the predicted sale price.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="lesson-6b-bagging.html#cb177-1" tabindex="-1"></a><span class="co"># prediction function</span></span>
<span id="cb177-2"><a href="lesson-6b-bagging.html#cb177-2" tabindex="-1"></a>pdp_pred_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) {</span>
<span id="cb177-3"><a href="lesson-6b-bagging.html#cb177-3" tabindex="-1"></a>   <span class="fu">mean</span>(<span class="fu">predict</span>(object, newdata, <span class="at">type =</span> <span class="st">&quot;numeric&quot;</span>)<span class="sc">$</span>.pred)</span>
<span id="cb177-4"><a href="lesson-6b-bagging.html#cb177-4" tabindex="-1"></a>}</span>
<span id="cb177-5"><a href="lesson-6b-bagging.html#cb177-5" tabindex="-1"></a></span>
<span id="cb177-6"><a href="lesson-6b-bagging.html#cb177-6" tabindex="-1"></a><span class="co"># use the pdp package to extract partial dependence predictions</span></span>
<span id="cb177-7"><a href="lesson-6b-bagging.html#cb177-7" tabindex="-1"></a><span class="co"># and then plot</span></span>
<span id="cb177-8"><a href="lesson-6b-bagging.html#cb177-8" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb177-9"><a href="lesson-6b-bagging.html#cb177-9" tabindex="-1"></a>   pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb177-10"><a href="lesson-6b-bagging.html#cb177-10" tabindex="-1"></a>     <span class="at">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, </span>
<span id="cb177-11"><a href="lesson-6b-bagging.html#cb177-11" tabindex="-1"></a>     <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb177-12"><a href="lesson-6b-bagging.html#cb177-12" tabindex="-1"></a>     <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb177-13"><a href="lesson-6b-bagging.html#cb177-13" tabindex="-1"></a>     <span class="at">train =</span> ames_train</span>
<span id="cb177-14"><a href="lesson-6b-bagging.html#cb177-14" tabindex="-1"></a>   ) <span class="sc">%&gt;%</span></span>
<span id="cb177-15"><a href="lesson-6b-bagging.html#cb177-15" tabindex="-1"></a>   <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb177-16"><a href="lesson-6b-bagging.html#cb177-16" tabindex="-1"></a>   <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-306-1.png" width="864" style="display: block; margin: auto;" /></p>
<div id="knowledge-check-32" class="section level3 hasAnchor" number="18.6.1">
<h3><span class="header-section-number">18.6.1</span> Knowledge check<a href="lesson-6b-bagging.html#knowledge-check-32" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset and the model from the
previous Knowledge check that performed best…
</p>
<ul>
<li>
Plot the top 10 most influential features.
</li>
<li>
Create and explain a PDP plot of the most influential feature.
</li>
</ul>
</div>
</div>
</div>
<div id="final-thoughts-4" class="section level2 hasAnchor" number="18.7">
<h2><span class="header-section-number">18.7</span> Final thoughts<a href="lesson-6b-bagging.html#final-thoughts-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed. However, using various interpretability algorithms such as VIPs and PDPs, we can still make inferences about how our bagged model leverages feature information. Also, since bagging consists of independent processes, the algorithm is easily parallelizable.</p>
<p>However, when bagging trees, a problem still exists. Although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships.</p>
<p>For example, if we create six decision trees with different bootstrapped samples of the Boston housing data <span class="citation">(<a href="#ref-harrison1978hedonic">Harrison Jr and Rubinfeld 1978</a>)</span>, we see a similar structure as the top of the trees. Although there are 15 predictor variables to split on, all six trees have both <code>lstat</code> and <code>rm</code> variables driving the first few splits.</p>
<div class="note">
<p>
We use the Boston housing data in this example because it has fewer
features and shorter names than the Ames housing data. Consequently, it
is easier to compare multiple trees side-by-side; however, the same tree
correlation problem exists in the Ames bagged model.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tree-correlation"></span>
<img src="_main_files/figure-html/tree-correlation-1.png" alt="Six decision trees based on different bootstrap samples." width="1152" />
<p class="caption">
Figure 18.1: Six decision trees based on different bootstrap samples.
</p>
</div>
<p>In the next lesson we’ll see how to resolve this concern using a modification of bagged trees called <strong><em>random forest</em></strong>.</p>
</div>
<div id="exercises-11" class="section level2 hasAnchor" number="18.8">
<h2><span class="header-section-number">18.8</span> Exercises<a href="lesson-6b-bagging.html#exercises-11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Using the same <code>kernlab::spam</code> data we saw in the <a
href="https://bradleyboehmke.github.io/uc-bana-4080/lesson-4b-regularized-regression.html#classification-problems-1">section
12.10</a>…
</p>
<ol style="list-style-type: decimal">
<li>
Split the data into 70-30 training-test sets.
</li>
<li>
Apply a bagged decision tree model modeling the <code>type</code>
response variable as a function of all available features.
</li>
<li>
How many trees are required before the loss function
stabilizes?
</li>
<li>
How does the model performance compare to the decision tree model
applied in the previous lesson’s exercise?
</li>
<li>
Which 10 features are considered most influential? Are these the
same features that have been influential in previous model?
</li>
<li>
Create partial dependence plots for the top two most influential
features. Explain the relationship between the feature and the predicted
values.
</li>
</ol>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-breiman1996bagging" class="csl-entry">
———. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24 (2): 123–40.
</div>
<div id="ref-dietterich2000experimental" class="csl-entry">
Dietterich, Thomas G. 2000a. <span>“An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization.”</span> <em>Machine Learning</em> 40 (2): 139–57.
</div>
<div id="ref-dietterich2000ensemble" class="csl-entry">
———. 2000b. <span>“Ensemble Methods in Machine Learning.”</span> In <em>International Workshop on Multiple Classifier Systems</em>, 1–15. Springer.
</div>
<div id="ref-harrison1978hedonic" class="csl-entry">
Harrison Jr, David, and Daniel L Rubinfeld. 1978. <span>“Hedonic Housing Prices and the Demand for Clean Air.”</span> <em>Journal of Environmental Economics and Management</em> 5 (1): 81–102.
</div>
<div id="ref-surowiecki2005wisdom" class="csl-entry">
Surowiecki, James. 2005. <em>The Wisdom of Crowds</em>. Anchor.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Also commonly referred to as a meta-algorithm.<a href="lesson-6b-bagging.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lesson-6a-decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lesson-6c-random-forests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-6/lesson-2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
