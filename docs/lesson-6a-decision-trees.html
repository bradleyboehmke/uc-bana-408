<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>17 Lesson 6a: Decision Trees | Data Mining with R</title>
  <meta name="description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="17 Lesson 6a: Decision Trees | Data Mining with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  <meta name="github-repo" content="bradleyboehmke/uc-bana-7025" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="17 Lesson 6a: Decision Trees | Data Mining with R" />
  <meta name="twitter:site" content="@bradleyboehmke" />
  <meta name="twitter:description" content="Master the art of data wrangling &amp; analysis with the R programming language." />
  

<meta name="author" content="Bradley Boehmke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="overview-5.html"/>
<link rel="next" href="lesson-6b-bagging.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.9/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UC BANA 4080: Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#material"><i class="fa fa-check"></i>Material</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-structure"><i class="fa fa-check"></i>Class Structure</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I Module 1</b></span></li>
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="overview.html"><a href="overview.html#estimated-time-requirement"><i class="fa fa-check"></i><b>1.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="1.3" data-path="overview.html"><a href="overview.html#tasks"><i class="fa fa-check"></i><b>1.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Lesson 1a: Intro to machine learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#regression-problems"><i class="fa fa-check"></i><b>2.2.1</b> Regression problems</a></li>
<li class="chapter" data-level="2.2.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#classification-problems"><i class="fa fa-check"></i><b>2.2.2</b> Classification problems</a></li>
<li class="chapter" data-level="2.2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check"><i class="fa fa-check"></i><b>2.2.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-1"><i class="fa fa-check"></i><b>2.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#machine-learning-in"><i class="fa fa-check"></i><b>2.4</b> Machine Learning in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#knowledge-check-2"><i class="fa fa-check"></i><b>2.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#the-data-sets"><i class="fa fa-check"></i><b>2.5</b> The data sets</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#boston-housing"><i class="fa fa-check"></i><b>2.5.1</b> Boston housing</a></li>
<li class="chapter" data-level="2.5.2" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#pima-indians-diabetes"><i class="fa fa-check"></i><b>2.5.2</b> Pima Indians Diabetes</a></li>
<li class="chapter" data-level="2.5.3" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#iris-flowers"><i class="fa fa-check"></i><b>2.5.3</b> Iris flowers</a></li>
<li class="chapter" data-level="2.5.4" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#ames-housing"><i class="fa fa-check"></i><b>2.5.4</b> Ames housing</a></li>
<li class="chapter" data-level="2.5.5" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#attrition"><i class="fa fa-check"></i><b>2.5.5</b> Attrition</a></li>
<li class="chapter" data-level="2.5.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#hitters"><i class="fa fa-check"></i><b>2.5.6</b> Hitters</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#what-youll-learn-next"><i class="fa fa-check"></i><b>2.6</b> What You’ll Learn Next</a></li>
<li class="chapter" data-level="2.7" data-path="lesson-1a-intro-to-machine-learning.html"><a href="lesson-1a-intro-to-machine-learning.html#exercises"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html"><i class="fa fa-check"></i><b>3</b> Lesson 1b: First model with Tidymodels</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#prerequisites"><i class="fa fa-check"></i><b>3.2</b> Prerequisites</a></li>
<li class="chapter" data-level="3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#data-splitting"><i class="fa fa-check"></i><b>3.3</b> Data splitting</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#simple-random-sampling"><i class="fa fa-check"></i><b>3.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="3.3.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#stratified-sampling"><i class="fa fa-check"></i><b>3.3.2</b> Stratified sampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-3"><i class="fa fa-check"></i><b>3.3.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#building-models"><i class="fa fa-check"></i><b>3.4</b> Building models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-4"><i class="fa fa-check"></i><b>3.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#making-predictions"><i class="fa fa-check"></i><b>3.5</b> Making predictions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-5"><i class="fa fa-check"></i><b>3.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#evaluating-model-performance"><i class="fa fa-check"></i><b>3.6</b> Evaluating model performance</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#regression-models"><i class="fa fa-check"></i><b>3.6.1</b> Regression models</a></li>
<li class="chapter" data-level="3.6.2" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#classification-models"><i class="fa fa-check"></i><b>3.6.2</b> Classification models</a></li>
<li class="chapter" data-level="3.6.3" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#knowledge-check-6"><i class="fa fa-check"></i><b>3.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="lesson-1b-first-model-with-tidymodels.html"><a href="lesson-1b-first-model-with-tidymodels.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Module 2</b></span></li>
<li class="chapter" data-level="4" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i><b>4</b> Overview</a>
<ul>
<li class="chapter" data-level="4.1" data-path="overview-1.html"><a href="overview-1.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="overview-1.html"><a href="overview-1.html#estimated-time-requirement-1"><i class="fa fa-check"></i><b>4.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="4.3" data-path="overview-1.html"><a href="overview-1.html#tasks-1"><i class="fa fa-check"></i><b>4.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Lesson 2a: Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#prerequisites-1"><i class="fa fa-check"></i><b>5.2</b> Prerequisites</a></li>
<li class="chapter" data-level="5.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>5.3</b> Correlation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-7"><i class="fa fa-check"></i><b>5.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#best-fit-line"><i class="fa fa-check"></i><b>5.4.1</b> Best fit line</a></li>
<li class="chapter" data-level="5.4.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#estimating-best-fit"><i class="fa fa-check"></i><b>5.4.2</b> Estimating “best fit”</a></li>
<li class="chapter" data-level="5.4.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#inference"><i class="fa fa-check"></i><b>5.4.3</b> Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-8"><i class="fa fa-check"></i><b>5.4.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#making-predictions-1"><i class="fa fa-check"></i><b>5.5</b> Making predictions</a></li>
<li class="chapter" data-level="5.6" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#training-data-accuracy"><i class="fa fa-check"></i><b>5.6.1</b> Training data accuracy</a></li>
<li class="chapter" data-level="5.6.2" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#test-data-accuracy"><i class="fa fa-check"></i><b>5.6.2</b> Test data accuracy</a></li>
<li class="chapter" data-level="5.6.3" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#knowledge-check-9"><i class="fa fa-check"></i><b>5.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
<li class="chapter" data-level="5.8" data-path="lesson-2a-simple-linear-regression.html"><a href="lesson-2a-simple-linear-regression.html#other-resources"><i class="fa fa-check"></i><b>5.8</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Lesson 2b: Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#prerequisites-2"><i class="fa fa-check"></i><b>6.2</b> Prerequisites</a></li>
<li class="chapter" data-level="6.3" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#adding-additional-predictors"><i class="fa fa-check"></i><b>6.3</b> Adding additional predictors</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-10"><i class="fa fa-check"></i><b>6.3.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#interactions"><i class="fa fa-check"></i><b>6.4</b> Interactions</a></li>
<li class="chapter" data-level="6.5" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#qualitative-predictors"><i class="fa fa-check"></i><b>6.5</b> Qualitative predictors</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#knowledge-check-11"><i class="fa fa-check"></i><b>6.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#including-many-predictors"><i class="fa fa-check"></i><b>6.6</b> Including many predictors</a></li>
<li class="chapter" data-level="6.7" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#feature-importance"><i class="fa fa-check"></i><b>6.7</b> Feature importance</a></li>
<li class="chapter" data-level="6.8" data-path="lesson-2b-multiple-linear-regression.html"><a href="lesson-2b-multiple-linear-regression.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Module 3</b></span></li>
<li class="chapter" data-level="7" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i><b>7</b> Overview</a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-2.html"><a href="overview-2.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="overview-2.html"><a href="overview-2.html#estimated-time-requirement-2"><i class="fa fa-check"></i><b>7.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="7.3" data-path="overview-2.html"><a href="overview-2.html#tasks-2"><i class="fa fa-check"></i><b>7.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Lesson 3a: Feature engineering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#prerequisites-3"><i class="fa fa-check"></i><b>8.2</b> Prerequisites</a></li>
<li class="chapter" data-level="8.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#create-a-recipe"><i class="fa fa-check"></i><b>8.3</b> Create a recipe</a></li>
<li class="chapter" data-level="8.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#numeric-features"><i class="fa fa-check"></i><b>8.4</b> Numeric features</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#standardizing"><i class="fa fa-check"></i><b>8.4.1</b> Standardizing</a></li>
<li class="chapter" data-level="8.4.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#normalizing"><i class="fa fa-check"></i><b>8.4.2</b> Normalizing</a></li>
<li class="chapter" data-level="8.4.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-12"><i class="fa fa-check"></i><b>8.4.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#categorical-features"><i class="fa fa-check"></i><b>8.5</b> Categorical features</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#one-hot-dummy-encoding"><i class="fa fa-check"></i><b>8.5.1</b> One-hot &amp; dummy encoding</a></li>
<li class="chapter" data-level="8.5.2" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#ordinal-encoding"><i class="fa fa-check"></i><b>8.5.2</b> Ordinal encoding</a></li>
<li class="chapter" data-level="8.5.3" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#lumping"><i class="fa fa-check"></i><b>8.5.3</b> Lumping</a></li>
<li class="chapter" data-level="8.5.4" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-13"><i class="fa fa-check"></i><b>8.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#fit-a-model-with-a-recipe"><i class="fa fa-check"></i><b>8.6</b> Fit a model with a recipe</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#knowledge-check-14"><i class="fa fa-check"></i><b>8.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lesson-3a-feature-engineering.html"><a href="lesson-3a-feature-engineering.html#exercises-4"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html"><i class="fa fa-check"></i><b>9</b> Lesson 3b: Resampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#prerequisites-4"><i class="fa fa-check"></i><b>9.2</b> Prerequisites</a></li>
<li class="chapter" data-level="9.3" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#resampling-cross-validation"><i class="fa fa-check"></i><b>9.3</b> Resampling &amp; cross-validation</a></li>
<li class="chapter" data-level="9.4" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.4</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-15"><i class="fa fa-check"></i><b>9.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#bootstrap-resampling"><i class="fa fa-check"></i><b>9.5</b> Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#knowledge-check-16"><i class="fa fa-check"></i><b>9.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#alternative-methods"><i class="fa fa-check"></i><b>9.6</b> Alternative methods</a></li>
<li class="chapter" data-level="9.7" data-path="lesson-3b-resampling.html"><a href="lesson-3b-resampling.html#exercises-5"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Module 4</b></span></li>
<li class="chapter" data-level="10" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i><b>10</b> Overview</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-3.html"><a href="overview-3.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="overview-3.html"><a href="overview-3.html#estimated-time-requirement-3"><i class="fa fa-check"></i><b>10.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="10.3" data-path="overview-3.html"><a href="overview-3.html#tasks-3"><i class="fa fa-check"></i><b>10.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Lesson 4a: Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>11.2</b> Prerequisites</a></li>
<li class="chapter" data-level="11.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Why logistic regression</a></li>
<li class="chapter" data-level="11.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Simple logistic regression</a></li>
<li class="chapter" data-level="11.5" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#interpretation"><i class="fa fa-check"></i><b>11.5</b> Interpretation</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-17"><i class="fa fa-check"></i><b>11.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Multiple logistic regression</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-18"><i class="fa fa-check"></i><b>11.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>11.7</b> Assessing model accuracy</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#accuracy"><i class="fa fa-check"></i><b>11.7.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.7.2" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#confusion-matrix"><i class="fa fa-check"></i><b>11.7.2</b> Confusion matrix</a></li>
<li class="chapter" data-level="11.7.3" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#area-under-the-curve"><i class="fa fa-check"></i><b>11.7.3</b> Area under the curve</a></li>
<li class="chapter" data-level="11.7.4" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-19"><i class="fa fa-check"></i><b>11.7.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#cross-validation-performance"><i class="fa fa-check"></i><b>11.8</b> Cross-validation performance</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-20"><i class="fa fa-check"></i><b>11.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>11.9</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#knowledge-check-21"><i class="fa fa-check"></i><b>11.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#final-thoughts"><i class="fa fa-check"></i><b>11.10</b> Final thoughts</a></li>
<li class="chapter" data-level="11.11" data-path="lesson-4a-logistic-regression.html"><a href="lesson-4a-logistic-regression.html#exercises-6"><i class="fa fa-check"></i><b>11.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html"><i class="fa fa-check"></i><b>12</b> Lesson 4b: Regularized Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#learning-objectives-12"><i class="fa fa-check"></i><b>12.1</b> Learning objectives</a></li>
<li class="chapter" data-level="12.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#prerequisites-6"><i class="fa fa-check"></i><b>12.2</b> Prerequisites</a></li>
<li class="chapter" data-level="12.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#why-regularize"><i class="fa fa-check"></i><b>12.3</b> Why regularize?</a></li>
<li class="chapter" data-level="12.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#ridge-penalty"><i class="fa fa-check"></i><b>12.4</b> Ridge penalty</a></li>
<li class="chapter" data-level="12.5" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#lasso"><i class="fa fa-check"></i><b>12.5</b> Lasso penalty</a></li>
<li class="chapter" data-level="12.6" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#elastic"><i class="fa fa-check"></i><b>12.6</b> Elastic nets</a></li>
<li class="chapter" data-level="12.7" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>12.7</b> Implementation</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-22"><i class="fa fa-check"></i><b>12.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-our-model"><i class="fa fa-check"></i><b>12.8</b> Tuning our model</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-strength"><i class="fa fa-check"></i><b>12.8.1</b> Tuning regularization strength</a></li>
<li class="chapter" data-level="12.8.2" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type"><i class="fa fa-check"></i><b>12.8.2</b> Tuning regularization type</a></li>
<li class="chapter" data-level="12.8.3" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#tuning-regularization-type-strength"><i class="fa fa-check"></i><b>12.8.3</b> Tuning regularization type &amp; strength</a></li>
<li class="chapter" data-level="12.8.4" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-23"><i class="fa fa-check"></i><b>12.8.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#feature-importance-1"><i class="fa fa-check"></i><b>12.9</b> Feature importance</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#knowledge-check-24"><i class="fa fa-check"></i><b>12.9.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="12.10" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#classification-problems-1"><i class="fa fa-check"></i><b>12.10</b> Classification problems</a></li>
<li class="chapter" data-level="12.11" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>12.11</b> Final thoughts</a></li>
<li class="chapter" data-level="12.12" data-path="lesson-4b-regularized-regression.html"><a href="lesson-4b-regularized-regression.html#exercises-7"><i class="fa fa-check"></i><b>12.12</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Module 5</b></span></li>
<li class="chapter" data-level="13" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i><b>13</b> Overview</a>
<ul>
<li class="chapter" data-level="13.1" data-path="overview-4.html"><a href="overview-4.html#learning-objectives-13"><i class="fa fa-check"></i><b>13.1</b> Learning objectives</a></li>
<li class="chapter" data-level="13.2" data-path="overview-4.html"><a href="overview-4.html#estimated-time-requirement-4"><i class="fa fa-check"></i><b>13.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="13.3" data-path="overview-4.html"><a href="overview-4.html#tasks-4"><i class="fa fa-check"></i><b>13.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html"><i class="fa fa-check"></i><b>14</b> Lesson 5a: Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#learning-objectives-14"><i class="fa fa-check"></i><b>14.1</b> Learning objectives</a></li>
<li class="chapter" data-level="14.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#prerequisites-7"><i class="fa fa-check"></i><b>14.2</b> Prerequisites</a></li>
<li class="chapter" data-level="14.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>14.3</b> Bias-variance tradeoff</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#bias"><i class="fa fa-check"></i><b>14.3.1</b> Bias</a></li>
<li class="chapter" data-level="14.3.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#variance"><i class="fa fa-check"></i><b>14.3.2</b> Variance</a></li>
<li class="chapter" data-level="14.3.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#balancing-the-tradeoff"><i class="fa fa-check"></i><b>14.3.3</b> Balancing the tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>14.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="14.5" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#implementation-1"><i class="fa fa-check"></i><b>14.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#tuning"><i class="fa fa-check"></i><b>14.5.1</b> Tuning</a></li>
<li class="chapter" data-level="14.5.2" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#more-tuning"><i class="fa fa-check"></i><b>14.5.2</b> More tuning</a></li>
<li class="chapter" data-level="14.5.3" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#finalizing-our-model"><i class="fa fa-check"></i><b>14.5.3</b> Finalizing our model</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lesson-5a-hyperparameter-tuning.html"><a href="lesson-5a-hyperparameter-tuning.html#exercises-8"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>15</b> Lesson 5b: Multivariate Adaptive Regression Splines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#learning-objectives-15"><i class="fa fa-check"></i><b>15.1</b> Learning objectives</a></li>
<li class="chapter" data-level="15.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#prerequisites-8"><i class="fa fa-check"></i><b>15.2</b> Prerequisites</a></li>
<li class="chapter" data-level="15.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#nonlinearity"><i class="fa fa-check"></i><b>15.3</b> Nonlinearity</a></li>
<li class="chapter" data-level="15.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>15.4</b> Multivariate adaptive regression splines</a></li>
<li class="chapter" data-level="15.5" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-mars-model"><i class="fa fa-check"></i><b>15.5</b> Fitting a MARS model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-basic-model"><i class="fa fa-check"></i><b>15.5.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="15.5.2" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model"><i class="fa fa-check"></i><b>15.5.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="15.5.3" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#fitting-a-full-model-with-interactions"><i class="fa fa-check"></i><b>15.5.3</b> Fitting a full model with interactions</a></li>
<li class="chapter" data-level="15.5.4" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-25"><i class="fa fa-check"></i><b>15.5.4</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#tuning-1"><i class="fa fa-check"></i><b>15.6</b> Tuning</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#knowledge-check-26"><i class="fa fa-check"></i><b>15.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#feature-interpretation-1"><i class="fa fa-check"></i><b>15.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="15.8" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#final-thoughts-2"><i class="fa fa-check"></i><b>15.8</b> Final thoughts</a></li>
<li class="chapter" data-level="15.9" data-path="lesson-5b-multivariate-adaptive-regression-splines.html"><a href="lesson-5b-multivariate-adaptive-regression-splines.html#exercises-9"><i class="fa fa-check"></i><b>15.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Module 6</b></span></li>
<li class="chapter" data-level="16" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i><b>16</b> Overview</a>
<ul>
<li class="chapter" data-level="16.1" data-path="overview-5.html"><a href="overview-5.html#learning-objectives-16"><i class="fa fa-check"></i><b>16.1</b> Learning objectives</a></li>
<li class="chapter" data-level="16.2" data-path="overview-5.html"><a href="overview-5.html#estimated-time-requirement-5"><i class="fa fa-check"></i><b>16.2</b> Estimated time requirement</a></li>
<li class="chapter" data-level="16.3" data-path="overview-5.html"><a href="overview-5.html#tasks-5"><i class="fa fa-check"></i><b>16.3</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html"><i class="fa fa-check"></i><b>17</b> Lesson 6a: Decision Trees</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#learning-objectives-17"><i class="fa fa-check"></i><b>17.1</b> Learning objectives</a></li>
<li class="chapter" data-level="17.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#prerequisites-9"><i class="fa fa-check"></i><b>17.2</b> Prerequisites</a></li>
<li class="chapter" data-level="17.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#structure"><i class="fa fa-check"></i><b>17.3</b> Structure</a></li>
<li class="chapter" data-level="17.4" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#partitioning"><i class="fa fa-check"></i><b>17.4</b> Partitioning</a></li>
<li class="chapter" data-level="17.5" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#how-deep"><i class="fa fa-check"></i><b>17.5</b> How deep?</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#early-stopping"><i class="fa fa-check"></i><b>17.5.1</b> Early stopping</a></li>
<li class="chapter" data-level="17.5.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#pruning"><i class="fa fa-check"></i><b>17.5.2</b> Pruning</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-decision-tree"><i class="fa fa-check"></i><b>17.6</b> Fitting a decision tree</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1"><i class="fa fa-check"></i><b>17.6.1</b> Fitting a basic model</a></li>
<li class="chapter" data-level="17.6.2" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#fitting-a-full-model-1"><i class="fa fa-check"></i><b>17.6.2</b> Fitting a full model</a></li>
<li class="chapter" data-level="17.6.3" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-27"><i class="fa fa-check"></i><b>17.6.3</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#tuning-2"><i class="fa fa-check"></i><b>17.7</b> Tuning</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-28"><i class="fa fa-check"></i><b>17.7.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#feature-interpretation-2"><i class="fa fa-check"></i><b>17.8</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#knowledge-check-29"><i class="fa fa-check"></i><b>17.8.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#final-thoughts-3"><i class="fa fa-check"></i><b>17.9</b> Final thoughts</a></li>
<li class="chapter" data-level="17.10" data-path="lesson-6a-decision-trees.html"><a href="lesson-6a-decision-trees.html#exercises-10"><i class="fa fa-check"></i><b>17.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html"><i class="fa fa-check"></i><b>18</b> Lesson 6b: Bagging</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#learning-objectives-18"><i class="fa fa-check"></i><b>18.1</b> Learning objectives</a></li>
<li class="chapter" data-level="18.2" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#prerequisites-10"><i class="fa fa-check"></i><b>18.2</b> Prerequisites</a></li>
<li class="chapter" data-level="18.3" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#why-and-when-bagging-works"><i class="fa fa-check"></i><b>18.3</b> Why and when bagging works</a></li>
<li class="chapter" data-level="18.4" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#fitting-a-decision-tree-model"><i class="fa fa-check"></i><b>18.4</b> Fitting a decision tree model</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-30"><i class="fa fa-check"></i><b>18.4.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#tuning-3"><i class="fa fa-check"></i><b>18.5</b> Tuning</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-31"><i class="fa fa-check"></i><b>18.5.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#feature-interpretation-3"><i class="fa fa-check"></i><b>18.6</b> Feature interpretation</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#knowledge-check-32"><i class="fa fa-check"></i><b>18.6.1</b> Knowledge check</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#final-thoughts-4"><i class="fa fa-check"></i><b>18.7</b> Final thoughts</a></li>
<li class="chapter" data-level="18.8" data-path="lesson-6b-bagging.html"><a href="lesson-6b-bagging.html#exercises-11"><i class="fa fa-check"></i><b>18.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VII Additional Content</b></span></li>
<li class="chapter" data-level="" data-path="computing-environment.html"><a href="computing-environment.html"><i class="fa fa-check"></i>Computing Environment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.uc.edu/" target="blank">University of Cincinnati</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lesson-6a-decision-trees" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">17</span> Lesson 6a: Decision Trees<a href="lesson-6a-decision-trees.html#lesson-6a-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Tree-based models</em> are a class of nonparametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of <em>splitting rules</em>. Predictions are obtained by fitting a simpler model (e.g., a constant like the average response value) in each region. Such <em>divide-and-conquer</em> methods can produce simple rules that are easy to interpret and visualize with <em>tree diagrams</em>. As we’ll see, decision trees offer many benefits; however, they typically lack in predictive performance compared to more complex algorithms like neural networks and MARS. However, future modules will discuss powerful ensemble algorithms—like random forests and gradient boosting machines—which are constructed by combining together many decision trees in a clever way. This module will provide you with a strong foundation in decision trees.</p>
<div id="learning-objectives-17" class="section level2 hasAnchor" number="17.1">
<h2><span class="header-section-number">17.1</span> Learning objectives<a href="lesson-6a-decision-trees.html#learning-objectives-17" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module you will know:</p>
<ul>
<li>How decision tree models partition data and how the depth of a tree impacts performance.</li>
<li>Train, fit, tune and assess decision tree models.</li>
<li>Identify important features and visualize their influence on the response.</li>
</ul>
</div>
<div id="prerequisites-9" class="section level2 hasAnchor" number="17.2">
<h2><span class="header-section-number">17.2</span> Prerequisites<a href="lesson-6a-decision-trees.html#prerequisites-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="lesson-6a-decision-trees.html#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper packages</span></span>
<span id="cb160-2"><a href="lesson-6a-decision-trees.html#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)   <span class="co"># for data wrangling &amp; plotting</span></span>
<span id="cb160-3"><a href="lesson-6a-decision-trees.html#cb160-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-4"><a href="lesson-6a-decision-trees.html#cb160-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Modeling packages</span></span>
<span id="cb160-5"><a href="lesson-6a-decision-trees.html#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels) </span>
<span id="cb160-6"><a href="lesson-6a-decision-trees.html#cb160-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-7"><a href="lesson-6a-decision-trees.html#cb160-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Model interpretability packages</span></span>
<span id="cb160-8"><a href="lesson-6a-decision-trees.html#cb160-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)         <span class="co"># for variable importance</span></span>
<span id="cb160-9"><a href="lesson-6a-decision-trees.html#cb160-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)         <span class="co"># for variable relationships</span></span></code></pre></div>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="lesson-6a-decision-trees.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb161-2"><a href="lesson-6a-decision-trees.html#cb161-2" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> AmesHousing<span class="sc">::</span><span class="fu">make_ames</span>()</span>
<span id="cb161-3"><a href="lesson-6a-decision-trees.html#cb161-3" aria-hidden="true" tabindex="-1"></a>split  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(ames, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)</span>
<span id="cb161-4"><a href="lesson-6a-decision-trees.html#cb161-4" aria-hidden="true" tabindex="-1"></a>ames_train  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(split)</span>
<span id="cb161-5"><a href="lesson-6a-decision-trees.html#cb161-5" aria-hidden="true" tabindex="-1"></a>ames_test   <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(split)</span></code></pre></div>
</div>
<div id="structure" class="section level2 hasAnchor" number="17.3">
<h2><span class="header-section-number">17.3</span> Structure<a href="lesson-6a-decision-trees.html#structure" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are many methodologies for constructing decision trees but the most well-known is the <strong>c</strong>lassification <strong>a</strong>nd <strong>r</strong>egression <strong>t</strong>ree (CART) algorithm proposed in <span class="citation">Breiman (<a href="#ref-breiman2017classification" role="doc-biblioref">1984</a>)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> A basic decision tree partitions the training data into homogeneous subgroups (i.e., groups with similar response values) and then fits a simple <em>constant</em> in each subgroup (e.g., the mean of the within group response values for regression). The subgroups (also called nodes) are formed recursively using binary partitions formed by asking simple yes-or-no questions about each feature (e.g., is <code>age &lt; 18</code>?). This is done a number of times until a suitable stopping criteria is satisfied (e.g., a maximum depth of the tree is reached). After all the partitioning has been done, the model predicts the output based on (1) the average response values for all observations that fall in that subgroup (regression problem), or (2) the class that has majority representation (classification problem). For classification, predicted probabilities can be obtained using the proportion of each class within the subgroup.</p>
<p>What results is an inverted tree-like structure such as that in the below figure. In essence, our tree is a set of rules that allows us to make predictions by asking simple yes-or-no questions about each feature. For example, if the customer is loyal, has household income greater than $150,000, and is shopping in a store, the exemplar tree diagram below would predict that the customer will redeem a coupon.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:exemplar-decision-tree"></span>
<img src="images/exemplar-decision-tree.png" alt="Exemplar decision tree predicting whether or not a customer will redeem a coupon (yes or no) based on the customer's loyalty, household income, last month's spend, coupon placement, and shopping mode." width="100%" height="100%" />
<p class="caption">
Figure 17.1: Exemplar decision tree predicting whether or not a customer will redeem a coupon (yes or no) based on the customer’s loyalty, household income, last month’s spend, coupon placement, and shopping mode.
</p>
</div>
<p>We refer to the first subgroup at the top of the tree as the <em>root node</em> (this node contains all of the training data). The final subgroups at the bottom of the tree are called the <em>terminal nodes</em> or <em>leaves</em>. Every subgroup in between is referred to as an internal node. The connections between nodes are called <em>branches</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decision-tree-terminology"></span>
<img src="images/decision-tree-terminology.png" alt="Terminology of a decision tree." width="80%" height="80%" />
<p class="caption">
Figure 17.2: Terminology of a decision tree.
</p>
</div>
</div>
<div id="partitioning" class="section level2 hasAnchor" number="17.4">
<h2><span class="header-section-number">17.4</span> Partitioning<a href="lesson-6a-decision-trees.html#partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As illustrated above, CART uses <em>binary recursive partitioning</em> (it’s recursive because each split or rule depends on the the splits above it). The objective at each node is to find the “best” feature (<span class="math inline">\(x_i\)</span>) to partition the remaining data into one of two regions (<span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>) such that the overall error between the actual response (<span class="math inline">\(y_i\)</span>) and the predicted constant (<span class="math inline">\(c_i\)</span>) is minimized. For regression problems, the objective function to minimize is the total SSE as defined in the following equation:</p>
<p><span class="math display">\[\begin{equation}
SSE = \sum_{i \in R_1}\left(y_i - c_1\right)^2 + \sum_{i \in R_2}\left(y_i - c_2\right)^2
\end{equation}\]</span></p>
<p>For classification problems, the partitioning is usually made to maximize the reduction in cross-entropy or the Gini index.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<div class="note">
<p>
In both regression and classification trees, the objective of
partitioning is to minimize dissimilarity in the terminal nodes.
However, we suggest <span class="citation"><span class="citation">Therneau, Atkinson, et al. (<a href="#ref-therneau1997introduction" role="doc-biblioref">1997</a>)</span></span> for a more thorough
discussion regarding binary recursive partitioning.
</p>
</div>
<p>Having found the best feature/split combination, the data are partitioned into two regions and the splitting process is repeated on each of the two regions (hence the name binary recursive partitioning). This process is continued until a suitable stopping criterion is reached (e.g., a maximum depth is reached or the tree becomes “too complex”).</p>
<p>It’s important to note that a single feature can be used multiple times in a tree. For example, say we have data generated from a simple <span class="math inline">\(\sin\)</span> function with Gaussian noise: <span class="math inline">\(Y_i \stackrel{iid}{\sim} N\left(\sin\left(X_i\right), \sigma^2\right)\)</span>, for <span class="math inline">\(i = 1, 2, \dots, 500\)</span>. A regression tree built with a single root node (often referred to as a decision stump) leads to a split occurring at <span class="math inline">\(x = 3.1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decision-stump"></span>
<img src="_main_files/figure-html/decision-stump-1.png" alt="Decision tree illustrating the single split on feature x (left). The resulting decision boundary illustrates the predicted value when x &lt; 3.1 (0.64), and when x &gt; 3.1 (-0.67) (right)." width="48%" /><img src="_main_files/figure-html/decision-stump-2.png" alt="Decision tree illustrating the single split on feature x (left). The resulting decision boundary illustrates the predicted value when x &lt; 3.1 (0.64), and when x &gt; 3.1 (-0.67) (right)." width="48%" />
<p class="caption">
Figure 17.3: Decision tree illustrating the single split on feature x (left). The resulting decision boundary illustrates the predicted value when x &lt; 3.1 (0.64), and when x &gt; 3.1 (-0.67) (right).
</p>
</div>
<p>If we build a deeper tree, we’ll continue to split on the same feature (<span class="math inline">\(x\)</span>) as illustrated below. This is because <span class="math inline">\(x\)</span> is the only feature available to split on so it will continue finding the optimal splits along this feature’s values until a pre-determined stopping criteria is reached.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:depth-3-decision-tree"></span>
<img src="_main_files/figure-html/depth-3-decision-tree-1.png" alt="Decision tree illustrating with depth = 3, resulting in 7 decision splits along values of feature x and 8 prediction regions (left). The resulting decision boundary (right)." width="48%" /><img src="_main_files/figure-html/depth-3-decision-tree-2.png" alt="Decision tree illustrating with depth = 3, resulting in 7 decision splits along values of feature x and 8 prediction regions (left). The resulting decision boundary (right)." width="48%" />
<p class="caption">
Figure 17.4: Decision tree illustrating with depth = 3, resulting in 7 decision splits along values of feature x and 8 prediction regions (left). The resulting decision boundary (right).
</p>
</div>
<p>However, even when many features are available, a single feature may still dominate if it continues to provide the best split after each successive partition. For example, a decision tree applied to the iris data set <span class="citation">(<a href="#ref-fisher1936use" role="doc-biblioref">R. A. Fisher 1936</a>)</span> where the species of the flower (setosa, versicolor, and virginica) is predicted based on two features (sepal width and sepal length) results in an optimal decision tree with two splits on each feature. Also, note how the decision boundary in a classification problem results in rectangular regions enclosing the observations. The predicted value is the response class with the greatest proportion within the enclosed region.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:iris-decision-tree"></span>
<img src="_main_files/figure-html/iris-decision-tree-1.png" alt="Decision tree for the iris classification problem (left). The decision boundary results in rectangular regions that enclose the observations.  The class with the highest proportion in each region is the predicted value (right)." width="48%" /><img src="_main_files/figure-html/iris-decision-tree-2.png" alt="Decision tree for the iris classification problem (left). The decision boundary results in rectangular regions that enclose the observations.  The class with the highest proportion in each region is the predicted value (right)." width="48%" />
<p class="caption">
Figure 17.5: Decision tree for the iris classification problem (left). The decision boundary results in rectangular regions that enclose the observations. The class with the highest proportion in each region is the predicted value (right).
</p>
</div>
</div>
<div id="how-deep" class="section level2 hasAnchor" number="17.5">
<h2><span class="header-section-number">17.5</span> How deep?<a href="lesson-6a-decision-trees.html#how-deep" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This leads to an important question: how deep (i.e., complex) should we make the tree? If we grow an overly complex tree as in the below figure, we tend to overfit to our training data resulting in poor generalization performance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deep-overfit-tree"></span>
<img src="_main_files/figure-html/deep-overfit-tree-1.png" alt="Overfit decision tree with 56 splits." width="48%" /><img src="_main_files/figure-html/deep-overfit-tree-2.png" alt="Overfit decision tree with 56 splits." width="48%" />
<p class="caption">
Figure 17.6: Overfit decision tree with 56 splits.
</p>
</div>
<p>Consequently, there is a balance to be achieved in the depth and complexity of the tree to optimize predictive performance on future unseen data. To find this balance, we have two primary approaches: (1) early stopping and (2) pruning.</p>
<div id="early-stopping" class="section level3 hasAnchor" number="17.5.1">
<h3><span class="header-section-number">17.5.1</span> Early stopping<a href="lesson-6a-decision-trees.html#early-stopping" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Early stopping explicitly restricts the growth of the tree. There are several ways we can restrict tree growth but two of the most common approaches are to restrict the tree depth to a certain level or to restrict the minimum number of observations allowed in any terminal node. When limiting tree depth we stop splitting after a certain depth (e.g., only grow a tree that has a depth of 5 levels). The shallower the tree the less variance we have in our predictions; however, at some point we can start to inject too much bias as shallow trees (e.g., stumps) are not able to capture interactions and complex patterns in our data.</p>
<p>When restricting minimum terminal node size (e.g., leaf nodes must contain at least 10 observations for predictions) we are deciding to not split intermediate nodes which contain too few data points. At the far end of the spectrum, a terminal node’s size of one allows for a single observation to be captured in the leaf node and used as a prediction (in this case, we’re interpolating the training data). This results in high variance and poor generalizability. On the other hand, large values restrict further splits therefore reducing variance.</p>
<p>These two approaches can be implemented independently of one another; however, they do have interaction effects as illustrated below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dt-early-stopping"></span>
<img src="_main_files/figure-html/dt-early-stopping-1.png" alt="Illustration of how early stopping affects the decision boundary of a regression decision tree. The columns illustrate how tree depth impacts the decision boundary and the rows illustrate how the minimum number of observations in the terminal node influences the decision boundary." width="960" />
<p class="caption">
Figure 17.7: Illustration of how early stopping affects the decision boundary of a regression decision tree. The columns illustrate how tree depth impacts the decision boundary and the rows illustrate how the minimum number of observations in the terminal node influences the decision boundary.
</p>
</div>
</div>
<div id="pruning" class="section level3 hasAnchor" number="17.5.2">
<h3><span class="header-section-number">17.5.2</span> Pruning<a href="lesson-6a-decision-trees.html#pruning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to explicitly specifying the depth of a decision tree is to grow a very large, complex tree and then <em>prune</em> it back to find an optimal subtree. We find the optimal subtree by using a <em>cost complexity parameter</em> (<span class="math inline">\(\alpha\)</span>) that penalizes our objective function for the number of terminal nodes of the tree (<span class="math inline">\(T\)</span>) as in the following equation.</p>
<p><span class="math display">\[\begin{equation}
\texttt{minimize} \left\{ SSE + \alpha \vert T \vert \right\}
\end{equation}\]</span></p>
<p>For a given value of <span class="math inline">\(\alpha\)</span> we find the smallest pruned tree that has the lowest penalized error. You may recognize the close association to the lasso penalty discussed in the regularized regression lesson. As with the regularization methods, smaller penalties tend to produce more complex models, which result in larger trees. Whereas larger penalties result in much smaller trees. Consequently, as a tree grows larger, the reduction in the SSE must be greater than the cost complexity penalty. Typically, we evaluate multiple models across a spectrum of <span class="math inline">\(\alpha\)</span> and use CV to identify the optimal value and, therefore, the optimal subtree that generalizes best to unseen data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pruned-tree"></span>
<img src="_main_files/figure-html/pruned-tree-1.png" alt="To prune a tree, we grow an overly complex tree (left) and then use a cost complexity parameter to identify the optimal subtree (right)." width="960" />
<p class="caption">
Figure 17.8: To prune a tree, we grow an overly complex tree (left) and then use a cost complexity parameter to identify the optimal subtree (right).
</p>
</div>
</div>
</div>
<div id="fitting-a-decision-tree" class="section level2 hasAnchor" number="17.6">
<h2><span class="header-section-number">17.6</span> Fitting a decision tree<a href="lesson-6a-decision-trees.html#fitting-a-decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="fitting-a-basic-model-1" class="section level3 hasAnchor" number="17.6.1">
<h3><span class="header-section-number">17.6.1</span> Fitting a basic model<a href="lesson-6a-decision-trees.html#fitting-a-basic-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To illustrate some of the concepts we’ve mentioned we’ll start by implementing models using just the <code>Gr_Liv_Area</code> and <code>Year_Built</code> features in our Ames housing data.</p>
<p>In R we use the <code>decision_tree()</code> model and we’ll use the <code>rpart</code> package as our model engine. In this example we will not set a specific depth of our tree; rather, <code>rpart</code> automatically builds a fully deep tree and then prunes it to attempt to find an optimal tree depth.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="lesson-6a-decision-trees.html#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: create ridge model object</span></span>
<span id="cb162-2"><a href="lesson-6a-decision-trees.html#cb162-2" aria-hidden="true" tabindex="-1"></a>dt_mod <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>(<span class="at">mode =</span> <span class="st">&quot;regression&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb162-3"><a href="lesson-6a-decision-trees.html#cb162-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-4"><a href="lesson-6a-decision-trees.html#cb162-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: create model recipe</span></span>
<span id="cb162-5"><a href="lesson-6a-decision-trees.html#cb162-5" aria-hidden="true" tabindex="-1"></a>model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(</span>
<span id="cb162-6"><a href="lesson-6a-decision-trees.html#cb162-6" aria-hidden="true" tabindex="-1"></a>    Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> Year_Built, </span>
<span id="cb162-7"><a href="lesson-6a-decision-trees.html#cb162-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ames_train</span>
<span id="cb162-8"><a href="lesson-6a-decision-trees.html#cb162-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb162-9"><a href="lesson-6a-decision-trees.html#cb162-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb162-10"><a href="lesson-6a-decision-trees.html#cb162-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: fit model workflow</span></span>
<span id="cb162-11"><a href="lesson-6a-decision-trees.html#cb162-11" aria-hidden="true" tabindex="-1"></a>dt_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb162-12"><a href="lesson-6a-decision-trees.html#cb162-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb162-13"><a href="lesson-6a-decision-trees.html#cb162-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(dt_mod) <span class="sc">%&gt;%</span></span>
<span id="cb162-14"><a href="lesson-6a-decision-trees.html#cb162-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb162-15"><a href="lesson-6a-decision-trees.html#cb162-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-16"><a href="lesson-6a-decision-trees.html#cb162-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: results</span></span>
<span id="cb162-17"><a href="lesson-6a-decision-trees.html#cb162-17" aria-hidden="true" tabindex="-1"></a>dt_fit</span>
<span id="cb162-18"><a href="lesson-6a-decision-trees.html#cb162-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ══ Workflow [trained] ═════════════════════════════════════════════════════════════</span></span>
<span id="cb162-19"><a href="lesson-6a-decision-trees.html#cb162-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Preprocessor: Recipe</span></span>
<span id="cb162-20"><a href="lesson-6a-decision-trees.html#cb162-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Model: decision_tree()</span></span>
<span id="cb162-21"><a href="lesson-6a-decision-trees.html#cb162-21" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb162-22"><a href="lesson-6a-decision-trees.html#cb162-22" aria-hidden="true" tabindex="-1"></a><span class="do">## ── Preprocessor ───────────────────────────────────────────────────────────────────</span></span>
<span id="cb162-23"><a href="lesson-6a-decision-trees.html#cb162-23" aria-hidden="true" tabindex="-1"></a><span class="do">## 0 Recipe Steps</span></span>
<span id="cb162-24"><a href="lesson-6a-decision-trees.html#cb162-24" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb162-25"><a href="lesson-6a-decision-trees.html#cb162-25" aria-hidden="true" tabindex="-1"></a><span class="do">## ── Model ──────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb162-26"><a href="lesson-6a-decision-trees.html#cb162-26" aria-hidden="true" tabindex="-1"></a><span class="do">## n= 2049 </span></span>
<span id="cb162-27"><a href="lesson-6a-decision-trees.html#cb162-27" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb162-28"><a href="lesson-6a-decision-trees.html#cb162-28" aria-hidden="true" tabindex="-1"></a><span class="do">## node), split, n, deviance, yval</span></span>
<span id="cb162-29"><a href="lesson-6a-decision-trees.html#cb162-29" aria-hidden="true" tabindex="-1"></a><span class="do">##       * denotes terminal node</span></span>
<span id="cb162-30"><a href="lesson-6a-decision-trees.html#cb162-30" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb162-31"><a href="lesson-6a-decision-trees.html#cb162-31" aria-hidden="true" tabindex="-1"></a><span class="do">##  1) root 2049 1.321981e+13 180922.6  </span></span>
<span id="cb162-32"><a href="lesson-6a-decision-trees.html#cb162-32" aria-hidden="true" tabindex="-1"></a><span class="do">##    2) Year_Built&lt; 1985.5 1228 2.698241e+12 141467.4  </span></span>
<span id="cb162-33"><a href="lesson-6a-decision-trees.html#cb162-33" aria-hidden="true" tabindex="-1"></a><span class="do">##      4) Gr_Liv_Area&lt; 1486 840 7.763942e+11 125692.6  </span></span>
<span id="cb162-34"><a href="lesson-6a-decision-trees.html#cb162-34" aria-hidden="true" tabindex="-1"></a><span class="do">##        8) Year_Built&lt; 1952.5 317 2.687202e+11 107761.5 *</span></span>
<span id="cb162-35"><a href="lesson-6a-decision-trees.html#cb162-35" aria-hidden="true" tabindex="-1"></a><span class="do">##        9) Year_Built&gt;=1952.5 523 3.439725e+11 136561.0 *</span></span>
<span id="cb162-36"><a href="lesson-6a-decision-trees.html#cb162-36" aria-hidden="true" tabindex="-1"></a><span class="do">##      5) Gr_Liv_Area&gt;=1486 388 1.260276e+12 175619.2  </span></span>
<span id="cb162-37"><a href="lesson-6a-decision-trees.html#cb162-37" aria-hidden="true" tabindex="-1"></a><span class="do">##       10) Gr_Liv_Area&lt; 2663.5 372 8.913502e+11 170223.5 *</span></span>
<span id="cb162-38"><a href="lesson-6a-decision-trees.html#cb162-38" aria-hidden="true" tabindex="-1"></a><span class="do">##       11) Gr_Liv_Area&gt;=2663.5 16 1.062939e+11 301068.8 *</span></span>
<span id="cb162-39"><a href="lesson-6a-decision-trees.html#cb162-39" aria-hidden="true" tabindex="-1"></a><span class="do">##    3) Year_Built&gt;=1985.5 821 5.750622e+12 239937.1  </span></span>
<span id="cb162-40"><a href="lesson-6a-decision-trees.html#cb162-40" aria-hidden="true" tabindex="-1"></a><span class="do">##      6) Gr_Liv_Area&lt; 1963 622 1.813069e+12 211699.5  </span></span>
<span id="cb162-41"><a href="lesson-6a-decision-trees.html#cb162-41" aria-hidden="true" tabindex="-1"></a><span class="do">##       12) Gr_Liv_Area&lt; 1501.5 285 3.483774e+11 182098.8 *</span></span>
<span id="cb162-42"><a href="lesson-6a-decision-trees.html#cb162-42" aria-hidden="true" tabindex="-1"></a><span class="do">##       13) Gr_Liv_Area&gt;=1501.5 337 1.003788e+12 236732.8  </span></span>
<span id="cb162-43"><a href="lesson-6a-decision-trees.html#cb162-43" aria-hidden="true" tabindex="-1"></a><span class="do">##         26) Year_Built&lt; 2004.5 198 3.906393e+11 217241.1 *</span></span>
<span id="cb162-44"><a href="lesson-6a-decision-trees.html#cb162-44" aria-hidden="true" tabindex="-1"></a><span class="do">##         27) Year_Built&gt;=2004.5 139 4.307663e+11 264498.0 *</span></span>
<span id="cb162-45"><a href="lesson-6a-decision-trees.html#cb162-45" aria-hidden="true" tabindex="-1"></a><span class="do">##      7) Gr_Liv_Area&gt;=1963 199 1.891416e+12 328197.2  </span></span>
<span id="cb162-46"><a href="lesson-6a-decision-trees.html#cb162-46" aria-hidden="true" tabindex="-1"></a><span class="do">##       14) Gr_Liv_Area&lt; 2390.5 107 5.903157e+11 290924.0  </span></span>
<span id="cb162-47"><a href="lesson-6a-decision-trees.html#cb162-47" aria-hidden="true" tabindex="-1"></a><span class="do">##         28) Year_Built&lt; 2004.5 69 1.168804e+11 253975.7 *</span></span>
<span id="cb162-48"><a href="lesson-6a-decision-trees.html#cb162-48" aria-hidden="true" tabindex="-1"></a><span class="do">##         29) Year_Built&gt;=2004.5 38 2.081946e+11 358014.5 *</span></span>
<span id="cb162-49"><a href="lesson-6a-decision-trees.html#cb162-49" aria-hidden="true" tabindex="-1"></a><span class="do">##       15) Gr_Liv_Area&gt;=2390.5 92 9.795556e+11 371547.5 *</span></span></code></pre></div>
<p>We can use <code>rpart.plot()</code> to plot our tree. This is only useful if we have a relatively small tree to visualize; however, most trees we will build will be far too large to attempt to visualize. In this case, we see that the root node (first node) splits our data based on (<code>Year_Built</code>). For those observations where the home is built after 1985 we follow the right half of the decision tree and for those where the home is built in or prior to 1985 we follow the left half of the decision tree.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="lesson-6a-decision-trees.html#cb163-1" aria-hidden="true" tabindex="-1"></a>rpart.plot<span class="sc">::</span><span class="fu">rpart.plot</span>(dt_fit<span class="sc">$</span>fit<span class="sc">$</span>fit<span class="sc">$</span>fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-285-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>However, to understand how our model is performing we want to perform cross validation. We see that this single decision tree is not performing spectacularly well with the average RMSE across our 5 folds equaling just under $50K.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="lesson-6a-decision-trees.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create resampling procedure</span></span>
<span id="cb164-2"><a href="lesson-6a-decision-trees.html#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb164-3"><a href="lesson-6a-decision-trees.html#cb164-3" aria-hidden="true" tabindex="-1"></a>kfold <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(ames_train, <span class="at">v =</span> <span class="dv">5</span>)</span>
<span id="cb164-4"><a href="lesson-6a-decision-trees.html#cb164-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-5"><a href="lesson-6a-decision-trees.html#cb164-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb164-6"><a href="lesson-6a-decision-trees.html#cb164-6" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">fit_resamples</span>(dt_mod, model_recipe, kfold)</span>
<span id="cb164-7"><a href="lesson-6a-decision-trees.html#cb164-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-8"><a href="lesson-6a-decision-trees.html#cb164-8" aria-hidden="true" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb164-9"><a href="lesson-6a-decision-trees.html#cb164-9" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(results)</span>
<span id="cb164-10"><a href="lesson-6a-decision-trees.html#cb164-10" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb164-11"><a href="lesson-6a-decision-trees.html#cb164-11" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n   std_err .config             </span></span>
<span id="cb164-12"><a href="lesson-6a-decision-trees.html#cb164-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb164-13"><a href="lesson-6a-decision-trees.html#cb164-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   47178.        5 1193.     Preprocessor1_Model1</span></span>
<span id="cb164-14"><a href="lesson-6a-decision-trees.html#cb164-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.655     5    0.0129 Preprocessor1_Model1</span></span></code></pre></div>
</div>
<div id="fitting-a-full-model-1" class="section level3 hasAnchor" number="17.6.2">
<h3><span class="header-section-number">17.6.2</span> Fitting a full model<a href="lesson-6a-decision-trees.html#fitting-a-full-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Next, lets go ahead and fit a full model to include all Ames housing features. We do not need to one-hot encode our features as <strong>rpart</strong> will naturally handle categorical features. By including all features we see some improvement in our model performance as our average cross validated RMSE is now in the low $40K range.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="lesson-6a-decision-trees.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model recipe with all features</span></span>
<span id="cb165-2"><a href="lesson-6a-decision-trees.html#cb165-2" aria-hidden="true" tabindex="-1"></a>full_model_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(</span>
<span id="cb165-3"><a href="lesson-6a-decision-trees.html#cb165-3" aria-hidden="true" tabindex="-1"></a>    Sale_Price <span class="sc">~</span> ., </span>
<span id="cb165-4"><a href="lesson-6a-decision-trees.html#cb165-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ames_train</span>
<span id="cb165-5"><a href="lesson-6a-decision-trees.html#cb165-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb165-6"><a href="lesson-6a-decision-trees.html#cb165-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-7"><a href="lesson-6a-decision-trees.html#cb165-7" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb165-8"><a href="lesson-6a-decision-trees.html#cb165-8" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">fit_resamples</span>(dt_mod, full_model_recipe, kfold)</span>
<span id="cb165-9"><a href="lesson-6a-decision-trees.html#cb165-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-10"><a href="lesson-6a-decision-trees.html#cb165-10" aria-hidden="true" tabindex="-1"></a><span class="co"># model results</span></span>
<span id="cb165-11"><a href="lesson-6a-decision-trees.html#cb165-11" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(results)</span>
<span id="cb165-12"><a href="lesson-6a-decision-trees.html#cb165-12" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 2 × 6</span></span>
<span id="cb165-13"><a href="lesson-6a-decision-trees.html#cb165-13" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator      mean     n   std_err .config             </span></span>
<span id="cb165-14"><a href="lesson-6a-decision-trees.html#cb165-14" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               </span></span>
<span id="cb165-15"><a href="lesson-6a-decision-trees.html#cb165-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard   40510.        5 1694.     Preprocessor1_Model1</span></span>
<span id="cb165-16"><a href="lesson-6a-decision-trees.html#cb165-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 rsq     standard       0.745     5    0.0148 Preprocessor1_Model1</span></span></code></pre></div>
</div>
<div id="knowledge-check-27" class="section level3 hasAnchor" number="17.6.3">
<h3><span class="header-section-number">17.6.3</span> Knowledge check<a href="lesson-6a-decision-trees.html#knowledge-check-27" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset:
</p>
<ol style="list-style-type: decimal">
<li>
Apply a default decision tree model where <code>cmedv</code> is the
response variable and <code>rm</code> and <code>lstat</code> are the two
predictor variables.
<ul>
<li>
Assess the resulting tree and explain the first decision node.
</li>
<li>
Pick a branch and explain the decision nodes as you traverse down
the branch.
</li>
</ul>
</li>
<li>
Apply a decision tree model that uses all possible predictor
variables.
<ul>
<li>
Assess the resulting tree and explain the first decision node.
</li>
<li>
Pick a branch and explain the decision nodes as you traverse down
the branch.
</li>
</ul>
</li>
<li>
Use a 5-fold cross validation procedure to compare the model in #1
to the model in #2. Which model performs best?
</li>
</ol>
</div>
</div>
</div>
<div id="tuning-2" class="section level2 hasAnchor" number="17.7">
<h2><span class="header-section-number">17.7</span> Tuning<a href="lesson-6a-decision-trees.html#tuning-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As previously mentioned, the tree depth is the primary factor that impacts performance. We can control tree depth via a few different parameters:</p>
<ol style="list-style-type: decimal">
<li>Max depth: we can explicitly state the maximum depth a tree can be grown.</li>
<li>Minimum observations for a split: The minimum number of samples required to split an internal node. This limits a tree from continuing to grow as the number of observations in a give node becomes smaller.</li>
<li>Cost complexity parameter: acts as a regularization mechanism by penalizing the objective function.</li>
</ol>
<p>There is not one best approach to use and often different combinations of these parameter settings improves model performance. The following will demonstrate a small grid search across 3 different values for each of these parameters (<span class="math inline">\(3^3 = 27\)</span> total setting combinations).</p>
<p>We see the optimal model decreases our average CV RMSE into the mid $30K range.</p>
<div class="tip">
<p>
It is common to run additional grid searches after the first grid
search. These additional grid searches uses the first grid search to
find parameter values that perform well and then continue to analyze
additional ranges around these values.
</p>
</div>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="lesson-6a-decision-trees.html#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model object with tuning options</span></span>
<span id="cb166-2"><a href="lesson-6a-decision-trees.html#cb166-2" aria-hidden="true" tabindex="-1"></a>dt_mod <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>(</span>
<span id="cb166-3"><a href="lesson-6a-decision-trees.html#cb166-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">&quot;regression&quot;</span>,</span>
<span id="cb166-4"><a href="lesson-6a-decision-trees.html#cb166-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb166-5"><a href="lesson-6a-decision-trees.html#cb166-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb166-6"><a href="lesson-6a-decision-trees.html#cb166-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb166-7"><a href="lesson-6a-decision-trees.html#cb166-7" aria-hidden="true" tabindex="-1"></a> ) <span class="sc">%&gt;%</span> </span>
<span id="cb166-8"><a href="lesson-6a-decision-trees.html#cb166-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb166-9"><a href="lesson-6a-decision-trees.html#cb166-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-10"><a href="lesson-6a-decision-trees.html#cb166-10" aria-hidden="true" tabindex="-1"></a><span class="co"># create the hyperparameter grid</span></span>
<span id="cb166-11"><a href="lesson-6a-decision-trees.html#cb166-11" aria-hidden="true" tabindex="-1"></a>hyper_grid <span class="ot">&lt;-</span> <span class="fu">grid_regular</span>(</span>
<span id="cb166-12"><a href="lesson-6a-decision-trees.html#cb166-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cost_complexity</span>(), </span>
<span id="cb166-13"><a href="lesson-6a-decision-trees.html#cb166-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tree_depth</span>(),</span>
<span id="cb166-14"><a href="lesson-6a-decision-trees.html#cb166-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">min_n</span>()</span>
<span id="cb166-15"><a href="lesson-6a-decision-trees.html#cb166-15" aria-hidden="true" tabindex="-1"></a> )</span>
<span id="cb166-16"><a href="lesson-6a-decision-trees.html#cb166-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-17"><a href="lesson-6a-decision-trees.html#cb166-17" aria-hidden="true" tabindex="-1"></a><span class="co"># train our model across the hyper parameter grid</span></span>
<span id="cb166-18"><a href="lesson-6a-decision-trees.html#cb166-18" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb166-19"><a href="lesson-6a-decision-trees.html#cb166-19" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">tune_grid</span>(dt_mod, full_model_recipe, <span class="at">resamples =</span> kfold, <span class="at">grid =</span> hyper_grid)</span>
<span id="cb166-20"><a href="lesson-6a-decision-trees.html#cb166-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-21"><a href="lesson-6a-decision-trees.html#cb166-21" aria-hidden="true" tabindex="-1"></a><span class="co"># get best results</span></span>
<span id="cb166-22"><a href="lesson-6a-decision-trees.html#cb166-22" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(results, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>, <span class="at">n =</span> <span class="dv">10</span>)</span>
<span id="cb166-23"><a href="lesson-6a-decision-trees.html#cb166-23" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 10 × 9</span></span>
<span id="cb166-24"><a href="lesson-6a-decision-trees.html#cb166-24" aria-hidden="true" tabindex="-1"></a><span class="do">##    cost_complexity tree_depth min_n .metric .estimator   mean     n std_err .config</span></span>
<span id="cb166-25"><a href="lesson-6a-decision-trees.html#cb166-25" aria-hidden="true" tabindex="-1"></a><span class="do">##              &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  </span></span>
<span id="cb166-26"><a href="lesson-6a-decision-trees.html#cb166-26" aria-hidden="true" tabindex="-1"></a><span class="do">##  1    0.0000000001          8    21 rmse    standard   34883.     5    971. Prepro…</span></span>
<span id="cb166-27"><a href="lesson-6a-decision-trees.html#cb166-27" aria-hidden="true" tabindex="-1"></a><span class="do">##  2    0.00000316            8    21 rmse    standard   34883.     5    971. Prepro…</span></span>
<span id="cb166-28"><a href="lesson-6a-decision-trees.html#cb166-28" aria-hidden="true" tabindex="-1"></a><span class="do">##  3    0.0000000001         15    21 rmse    standard   34986.     5    962. Prepro…</span></span>
<span id="cb166-29"><a href="lesson-6a-decision-trees.html#cb166-29" aria-hidden="true" tabindex="-1"></a><span class="do">##  4    0.00000316           15    21 rmse    standard   34986.     5    962. Prepro…</span></span>
<span id="cb166-30"><a href="lesson-6a-decision-trees.html#cb166-30" aria-hidden="true" tabindex="-1"></a><span class="do">##  5    0.0000000001         15    40 rmse    standard   36018.     5   1028. Prepro…</span></span>
<span id="cb166-31"><a href="lesson-6a-decision-trees.html#cb166-31" aria-hidden="true" tabindex="-1"></a><span class="do">##  6    0.00000316           15    40 rmse    standard   36018.     5   1028. Prepro…</span></span>
<span id="cb166-32"><a href="lesson-6a-decision-trees.html#cb166-32" aria-hidden="true" tabindex="-1"></a><span class="do">##  7    0.0000000001          8    40 rmse    standard   36150.     5   1011. Prepro…</span></span>
<span id="cb166-33"><a href="lesson-6a-decision-trees.html#cb166-33" aria-hidden="true" tabindex="-1"></a><span class="do">##  8    0.00000316            8    40 rmse    standard   36150.     5   1011. Prepro…</span></span>
<span id="cb166-34"><a href="lesson-6a-decision-trees.html#cb166-34" aria-hidden="true" tabindex="-1"></a><span class="do">##  9    0.00000316            8     2 rmse    standard   37161.     5   1271. Prepro…</span></span>
<span id="cb166-35"><a href="lesson-6a-decision-trees.html#cb166-35" aria-hidden="true" tabindex="-1"></a><span class="do">## 10    0.0000000001          8     2 rmse    standard   37173.     5   1287. Prepro…</span></span></code></pre></div>
<div id="knowledge-check-28" class="section level3 hasAnchor" number="17.7.1">
<h3><span class="header-section-number">17.7.1</span> Knowledge check<a href="lesson-6a-decision-trees.html#knowledge-check-28" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset apply a decision tree model
that models <code>cmedv</code> as a function of all possible predictor
variables and tune the following hyperparameters with a 5-fold cross
validation procedure: - Tune the cost complexity values with the default
<code>cost_complexity()</code> values. - Tune the depth of the tree with
the default <code>tree_depth()</code> values. - Tune the minimum number
of observations in a node with the default <code>min_n()</code> values.
- Assess a total of 5 values from each parameter
(<code>levels = 5</code>).
</p>
<p>
Which model(s) provide the lowest cross validated RMSE? What
hyperparameter values provide these optimal results?
</p>
</div>
</div>
</div>
<div id="feature-interpretation-2" class="section level2 hasAnchor" number="17.8">
<h2><span class="header-section-number">17.8</span> Feature interpretation<a href="lesson-6a-decision-trees.html#feature-interpretation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.</p>
<p>We can use a similar approach as we have in the previous lessons to plot the most influential features in our decision tree models.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="lesson-6a-decision-trees.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get best hyperparameter values</span></span>
<span id="cb167-2"><a href="lesson-6a-decision-trees.html#cb167-2" aria-hidden="true" tabindex="-1"></a>best_model <span class="ot">&lt;-</span> <span class="fu">select_best</span>(results, <span class="st">&#39;rmse&#39;</span>)</span>
<span id="cb167-3"><a href="lesson-6a-decision-trees.html#cb167-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-4"><a href="lesson-6a-decision-trees.html#cb167-4" aria-hidden="true" tabindex="-1"></a><span class="co"># put together final workflow</span></span>
<span id="cb167-5"><a href="lesson-6a-decision-trees.html#cb167-5" aria-hidden="true" tabindex="-1"></a>final_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb167-6"><a href="lesson-6a-decision-trees.html#cb167-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(full_model_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb167-7"><a href="lesson-6a-decision-trees.html#cb167-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(dt_mod) <span class="sc">%&gt;%</span></span>
<span id="cb167-8"><a href="lesson-6a-decision-trees.html#cb167-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(best_model)</span>
<span id="cb167-9"><a href="lesson-6a-decision-trees.html#cb167-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-10"><a href="lesson-6a-decision-trees.html#cb167-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit final workflow across entire training data</span></span>
<span id="cb167-11"><a href="lesson-6a-decision-trees.html#cb167-11" aria-hidden="true" tabindex="-1"></a>final_fit <span class="ot">&lt;-</span> final_wf <span class="sc">%&gt;%</span></span>
<span id="cb167-12"><a href="lesson-6a-decision-trees.html#cb167-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> ames_train)</span>
<span id="cb167-13"><a href="lesson-6a-decision-trees.html#cb167-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-14"><a href="lesson-6a-decision-trees.html#cb167-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot feature importance</span></span>
<span id="cb167-15"><a href="lesson-6a-decision-trees.html#cb167-15" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb167-16"><a href="lesson-6a-decision-trees.html#cb167-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb167-17"><a href="lesson-6a-decision-trees.html#cb167-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">vip</span>(<span class="dv">20</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-292-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>And similar the MARS model, since our relationship between our response variable and the predictor variables are non-linear, it becomes helpful to visualize the relationship between the most influential feature(s) and the response variable to see how they relate. Recall that we can do that with PDP plots.</p>
<p>Here, we see that the overall quality of a home doesn’t have a big impact unless the homes are rated very good to very excellent.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="lesson-6a-decision-trees.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction function</span></span>
<span id="cb168-2"><a href="lesson-6a-decision-trees.html#cb168-2" aria-hidden="true" tabindex="-1"></a>pdp_pred_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata) {</span>
<span id="cb168-3"><a href="lesson-6a-decision-trees.html#cb168-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(<span class="fu">predict</span>(object, newdata, <span class="at">type =</span> <span class="st">&quot;numeric&quot;</span>)<span class="sc">$</span>.pred)</span>
<span id="cb168-4"><a href="lesson-6a-decision-trees.html#cb168-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb168-5"><a href="lesson-6a-decision-trees.html#cb168-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-6"><a href="lesson-6a-decision-trees.html#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use the pdp package to extract partial dependence predictions</span></span>
<span id="cb168-7"><a href="lesson-6a-decision-trees.html#cb168-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and then plot</span></span>
<span id="cb168-8"><a href="lesson-6a-decision-trees.html#cb168-8" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb168-9"><a href="lesson-6a-decision-trees.html#cb168-9" aria-hidden="true" tabindex="-1"></a>  pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb168-10"><a href="lesson-6a-decision-trees.html#cb168-10" aria-hidden="true" tabindex="-1"></a>   <span class="at">pred.var =</span> <span class="st">&quot;Overall_Qual&quot;</span>, </span>
<span id="cb168-11"><a href="lesson-6a-decision-trees.html#cb168-11" aria-hidden="true" tabindex="-1"></a>   <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb168-12"><a href="lesson-6a-decision-trees.html#cb168-12" aria-hidden="true" tabindex="-1"></a>   <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb168-13"><a href="lesson-6a-decision-trees.html#cb168-13" aria-hidden="true" tabindex="-1"></a>   <span class="at">train =</span> ames_train</span>
<span id="cb168-14"><a href="lesson-6a-decision-trees.html#cb168-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb168-15"><a href="lesson-6a-decision-trees.html#cb168-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Overall_Qual, yhat)) <span class="sc">+</span></span>
<span id="cb168-16"><a href="lesson-6a-decision-trees.html#cb168-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb168-17"><a href="lesson-6a-decision-trees.html#cb168-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-293-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>And if we do a similar plot for the <code>Gr_Liv_Area</code> variable we can see the non-linear relationship between the square footage of a home and the predicted <code>Sale_Price</code> that exists.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="lesson-6a-decision-trees.html#cb169-1" aria-hidden="true" tabindex="-1"></a>final_fit <span class="sc">%&gt;%</span></span>
<span id="cb169-2"><a href="lesson-6a-decision-trees.html#cb169-2" aria-hidden="true" tabindex="-1"></a>  pdp<span class="sc">::</span><span class="fu">partial</span>(</span>
<span id="cb169-3"><a href="lesson-6a-decision-trees.html#cb169-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, </span>
<span id="cb169-4"><a href="lesson-6a-decision-trees.html#cb169-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">pred.fun =</span> pdp_pred_fun,</span>
<span id="cb169-5"><a href="lesson-6a-decision-trees.html#cb169-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">grid.resolution =</span> <span class="dv">10</span>, </span>
<span id="cb169-6"><a href="lesson-6a-decision-trees.html#cb169-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">train =</span> ames_train</span>
<span id="cb169-7"><a href="lesson-6a-decision-trees.html#cb169-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb169-8"><a href="lesson-6a-decision-trees.html#cb169-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb169-9"><a href="lesson-6a-decision-trees.html#cb169-9" aria-hidden="true" tabindex="-1"></a>   <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>dollar)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-294-1.png" width="576" style="display: block; margin: auto;" /></p>
<div id="knowledge-check-29" class="section level3 hasAnchor" number="17.8.1">
<h3><span class="header-section-number">17.8.1</span> Knowledge check<a href="lesson-6a-decision-trees.html#knowledge-check-29" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="todo">
<p>
Using the <code>boston.csv</code> dataset and the model from the
previous Knowledge check that performed best… - Plot the top 10 most
influential features. - Create and explain a PDP plot of the most
influential feature.
</p>
</div>
</div>
</div>
<div id="final-thoughts-3" class="section level2 hasAnchor" number="17.9">
<h2><span class="header-section-number">17.9</span> Final thoughts<a href="lesson-6a-decision-trees.html#final-thoughts-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Decision trees have a number of advantages. Trees require very little pre-processing. This is not to say feature engineering may not improve upon a decision tree, but rather, that there are no pre-processing requirements. Monotonic transformations (e.g., <span class="math inline">\(\log\)</span>, <span class="math inline">\(\exp\)</span>, and <span class="math inline">\(\sqrt{}\)</span>) are not required to meet algorithm assumptions as in many parametric models; instead, they only shift the location of the optimal split points. Outliers typically do not bias the results as much since the binary partitioning simply looks for a single location to make a split within the distribution of each feature.</p>
<p>Decision trees can easily handle categorical features without preprocessing. For unordered categorical features with more than two levels, the classes are ordered based on the outcome (for regression problems, the mean of the response is used and for classification problems, the proportion of the positive outcome class is used). For more details see <span class="citation">J. Friedman, Hastie, and Tibshirani (<a href="#ref-esl" role="doc-biblioref">2001</a>)</span>, <span class="citation">Breiman and Ihaka (<a href="#ref-breiman1984nonlinear" role="doc-biblioref">1984</a>)</span>, <span class="citation">Ripley (<a href="#ref-ripley2007pattern" role="doc-biblioref">2007</a>)</span>, <span class="citation">W. D. Fisher (<a href="#ref-fisher1958grouping" role="doc-biblioref">1958</a>)</span>, and <span class="citation">Loh and Vanichsetakul (<a href="#ref-loh1988tree" role="doc-biblioref">1988</a>)</span>.</p>
<p>Missing values often cause problems with statistical models and analyses. Most procedures deal with them by refusing to deal with them—incomplete observations are tossed out. However, most decision tree implementations can easily handle missing values in the features and do not require imputation. This is handled in various ways but most commonly by creating a new “missing” class for categorical variables or using surrogate splits (see <span class="citation">Therneau, Atkinson, et al. (<a href="#ref-therneau1997introduction" role="doc-biblioref">1997</a>)</span> for details).</p>
<p>However, individual decision trees generally do not often achieve state-of-the-art predictive accuracy. In this module, we saw that the best pruned decision tree, although it performed better than linear regression, had a very poor RMSE (~$41,000) compared to some of the other models we’ve built. This is driven by the fact that decision trees are composed of simple yes-or-no rules that create rigid non-smooth decision boundaries. Furthermore, we saw that deep trees tend to have high variance (and low bias) and shallow trees tend to be overly bias (but low variance). In the modules that follow, we’ll see how we can combine multiple trees together into very powerful prediction models called <em>ensembles</em>.</p>
</div>
<div id="exercises-10" class="section level2 hasAnchor" number="17.10">
<h2><span class="header-section-number">17.10</span> Exercises<a href="lesson-6a-decision-trees.html#exercises-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="todo">
<p>
Using the same <code>kernlab::spam</code> data we saw in the <a
href="https://bradleyboehmke.github.io/uc-bana-4080/lesson-4b-regularized-regression.html#classification-problems-1">section
12.10</a>…
</p>
<ol style="list-style-type: decimal">
<li>
Split the data into 70-30 training-test sets.
</li>
<li>
Apply a decision tree classification model where <code>type</code>
is our response variable and use all possible predictor variables.
<ul>
<li>
Use a 5-fold cross-validation procedure.
</li>
<li>
Tune the cost complexity values with the default
<code>cost_complexity()</code> values.
</li>
<li>
Tune the depth of the tree with the default
<code>tree_depth()</code> values.
</li>
<li>
Tune the minimum number of observations in a node with the default
<code>min_n()</code> values.
</li>
<li>
Assess a total of 5 values from each parameter
(<code>levels = 5</code>).
</li>
</ul>
</li>
<li>
Which model(s) have the highest AUC (<code>roc_auc</code>)
scores?
</li>
<li>
What hyperparameter values provide these optimal results?
</li>
<li>
Use the hyperparameter values that provide the best results to
finalize your workflow and and identify the top 20 most influential
predictors.
</li>
<li>
<strong>Bonus</strong>: See if you can create a PDP plot for the #1
most influential variable. What does the relationship between this
feature and the response variable look like?
</li>
</ol>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-breiman2017classification" class="csl-entry">
Breiman, Leo. 1984. <em>Classification and Regression Trees</em>. Routledge.
</div>
<div id="ref-breiman1984nonlinear" class="csl-entry">
Breiman, Leo, and Ross Ihaka. 1984. <em>Nonlinear Discriminant Analysis via Scaling and ACE</em>. Department of Statistics, University of California.
</div>
<div id="ref-fisher1936use" class="csl-entry">
Fisher, Ronald A. 1936. <span>“The Use of Multiple Measurements in Taxonomic Problems.”</span> <em>Annals of Eugenics</em> 7 (2): 179–88.
</div>
<div id="ref-fisher1958grouping" class="csl-entry">
Fisher, Walter D. 1958. <span>“On Grouping for Maximum Homogeneity.”</span> <em>Journal of the American Statistical Association</em> 53 (284): 789–98.
</div>
<div id="ref-esl" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer Series in Statistics New York, NY, USA:
</div>
<div id="ref-hothorn2006unbiased" class="csl-entry">
Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. <span>“Unbiased Recursive Partitioning: A Conditional Inference Framework.”</span> <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651–74.
</div>
<div id="ref-kass1980exploratory" class="csl-entry">
Kass, Gordon V. 1980. <span>“An Exploratory Technique for Investigating Large Quantities of Categorical Data.”</span> <em>Applied Statistics</em>, 119–27.
</div>
<div id="ref-loh1988tree" class="csl-entry">
Loh, Wei-Yin, and Nunta Vanichsetakul. 1988. <span>“Tree-Structured Classification via Generalized Discriminant Analysis.”</span> <em>Journal of the American Statistical Association</em> 83 (403): 715–25.
</div>
<div id="ref-quinlan1996bagging" class="csl-entry">
Quinlan, J Ross et al. 1996. <span>“Bagging, Boosting, and C4. 5.”</span> In <em>AAAI/IAAI, Vol. 1</em>, 725–30.
</div>
<div id="ref-quinlan1986induction" class="csl-entry">
Quinlan, J. Ross. 1986. <span>“Induction of Decision Trees.”</span> <em>Machine Learning</em> 1 (1): 81–106.
</div>
<div id="ref-ripley2007pattern" class="csl-entry">
Ripley, Brian D. 2007. <em>Pattern Recognition and Neural Networks</em>. Cambridge University Press.
</div>
<div id="ref-therneau1997introduction" class="csl-entry">
Therneau, Terry M, Elizabeth J Atkinson, et al. 1997. <span>“An Introduction to Recursive Partitioning Using the <span>RPART</span> Routines.”</span> Mayo Foundation.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Other decision tree algorithms include the Iterative Dichotomiser 3 <span class="citation">(<a href="#ref-quinlan1986induction" role="doc-biblioref">J. Ross Quinlan 1986</a>)</span>, C4.5 <span class="citation">(<a href="#ref-quinlan1996bagging" role="doc-biblioref">J. Ross Quinlan et al. 1996</a>)</span>, Chi-square automatic interaction detection <span class="citation">(<a href="#ref-kass1980exploratory" role="doc-biblioref">Kass 1980</a>)</span>, Conditional inference trees <span class="citation">(<a href="#ref-hothorn2006unbiased" role="doc-biblioref">Hothorn, Hornik, and Zeileis 2006</a>)</span>, and more.<a href="lesson-6a-decision-trees.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Gini index and cross-entropy are the two most commonly applied loss functions used for decision trees. Classification error is rarely used to determine partitions as they are less sensitive to poor performing splits <span class="citation">(<a href="#ref-esl" role="doc-biblioref">J. Friedman, Hastie, and Tibshirani 2001</a>)</span>.<a href="lesson-6a-decision-trees.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overview-5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lesson-6b-bagging.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bradleyboehmke/uc-bana-4080/edit/master/module-6/lesson-1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
